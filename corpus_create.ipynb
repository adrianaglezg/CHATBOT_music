{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk \n",
    "import numpy as np\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_feature_database = 'data/all_extracted_features.json'\n",
    "with open(path_feature_database, \"r\", encoding=\"utf-8\") as file:\n",
    "    corpus_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(r'C:\\Users\\34640\\Desktop\\VSCode\\z_Cursos_Python\\chatbots\\data\\all_extracted_features.txt', 'r', errors= 'ignore')\n",
    "# raw = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"mood_1\": \"Weightlessness\",\n",
      "        \"text_1\": \"Input (Mood: Anxious, restless):\\n\\nI can't seem to sit still, my mind racing with \\\"what ifs\\\" and worst-case scenarios.  A low, throbbing bassline would capture that feeling of unease perfectly, maybe with high-pitched strings weaving in and out, mirroring the chaotic thoughts.  It needs to be fast, frantic even, but with a driving rhythm to keep the anxiety from overwhelming everything.  I need something to ground me, but something that acknowledges this feeling.\\n\",\n",
      "        \"features_1\": \"```json\\n{\\n  \\\"Tempo\\\": \\\"140 bpm\\\",\\n  \\\"Intensity/Dynamics\\\": \\\"mf - crescendo to ff during the \\\"what ifs\\\" section, then diminuendo to mp\\\",\\n  \\\"Timbre\\\": \\\"Dark, with a focus on low \n"
     ]
    }
   ],
   "source": [
    "# print(raw[:750])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuring that our raw corpus retains '\\n' and similar markers is crucial, allowing our future chatbot to correctly segment sentences and paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepropecing the corpus\n",
    "\n",
    "This stage will involve several NLP techniques, including tokenization, stop word removal, lemmatization, and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet') \n",
    "# make sur to have this libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracts the preprocessed texts from the corpus to maintain the reference to their features\n",
    "corpus_texts = []\n",
    "corpus_features = []  \n",
    "\n",
    "for entry in corpus_data:\n",
    "    for key in entry:\n",
    "        if key.startswith(\"text_\"):  \n",
    "            corpus_texts.append(entry[key]) \n",
    "\n",
    "for entry in corpus_data:\n",
    "    for key in entry:\n",
    "        if key.startswith(\"features_\"):  \n",
    "            corpus_features.append(entry[key])\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We subdivide the raw corpus to facilitate later vectorization of both the users' texts in the corpus and the new input from the person interacting with the bot. This allows us to identify the most similar text using cosine distance and retrieve its corresponding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Overwhelmed.  A cacophony of noise in my head, a frantic rhythm that won't slow down.  I need music to match ‚Äì something chaotic but ultimately resolving, maybe a crescendo into a quiet, peaceful ending.  Please, something to calm this storm.\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_texts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n  \"Tempo\": \"Allegro molto (very fast) initially, gradually slowing to Adagio (slow)\",\\n  \"Intensity/Dynamics\": \"Fortissimo (very loud) crescendo to pianissimo (very soft)\",\\n  \"Timbre\": \"Initially brassy and harsh, gradually softening to strings and woodwinds with a mellow tone\",\\n  \"Rhythm\": \"Initially irregular and complex polyrhythms, gradually resolving into a simple, regular beat\",\\n  \"Harmonic progression\": \"Cm - B‚ô≠ - A‚ô≠ - G‚ô≠ - F - Eb - D‚ô≠ - C (Descending chromatic scale resolving to tonic)\",\\n  \"Melody\": \"Initially erratic and disjointed leaps, gradually becoming smoother and descending towards the end\",\\n  \"Tonality/Mode\": \"C minor, moving towards C major at the end\",\\n  \"Articulation\": \"Initially staccato and aggressive, gradually becoming legato and flowing\",\\n  \"Silence\": \"Long pause at the very end\"\\n}\\n```\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_features[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    ''' \n",
    "    tokenizes, removes punctuation and lemmatizes\n",
    "    '''\n",
    "    tokens = nltk.word_tokenize(text.lower().translate(remove_punct_dict))\n",
    "    return \" \".join(lemmatizer.lemmatize(token) for token in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_response(user_input, features= corpus_features):\n",
    "    \"\"\" Encuentra la respuesta m√°s relevante en el corpus usando TF-IDF y similitud coseno \"\"\"\n",
    "    \n",
    "    processed_input = preprocess_text(user_input)\n",
    "\n",
    "    user_vector = vectorizer.transform([processed_input])\n",
    "\n",
    "    similarities = cosine_similarity(user_vector, tfidf_matrix)\n",
    "\n",
    "    # find best index to return the features\n",
    "    best_match_idx = np.argmax(similarities)\n",
    "    best_match_features = features[best_match_idx]\n",
    "\n",
    "    return best_match_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  c:\\Users\\34640\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  c:\\Users\\34640\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  c:\\Users\\34640\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m pip install [options] [-e] <vcs project url> ...\n",
      "  c:\\Users\\34640\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m pip install [options] [-e] <local project path> ...\n",
      "  c:\\Users\\34640\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -m\n"
     ]
    }
   ],
   "source": [
    "%pip install langdetect python.exe -m pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "import threading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeout = 30  # Tiempo l√≠mite en segundos\n",
    "stop_event = threading.Event()  # Evento para controlar el tiempo de espera\n",
    "\n",
    "def countdown_timer():\n",
    "    \"\"\"Funci√≥n que espera el tiempo l√≠mite y termina el programa si no hay input.\"\"\"\n",
    "    global stop_event\n",
    "    while not stop_event.wait(timeout):\n",
    "        print(\"\\nüïë Timeout reached! No input received. Exiting...\")\n",
    "        exit()\n",
    "\n",
    "# Iniciar el temporizador en segundo plano\n",
    "timer_thread = threading.Thread(target=countdown_timer, daemon=True)\n",
    "timer_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§üí¨ 12345 \n",
      "\n",
      "ü§ñ‚ùå Sorry, I couldn't detect the language of your text. Please try again.\n",
      "\n",
      "üë§üí¨ hola caracola \n",
      "\n",
      "ü§ñ‚ùå Sorry, I can't understand your text. It must be written in English.\n",
      "\n",
      "üë§üí¨ exit \n",
      "\n",
      "ü§ñ good bye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"\\nüë§ Share how you're feeling or what's on your mind (or type 'exit' to close this session): \")\n",
    "\n",
    "    stop_event.set()\n",
    "    stop_event.clear() #restart\n",
    "\n",
    "    print(f\"\\nüë§üí¨ {user_input} \")\n",
    "\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"\\nü§ñ good bye!\")\n",
    "        break\n",
    "\n",
    "    # elif user_input.lower().replace(\" \", \"\") == \"thakyou\":\n",
    "    #     print(\"\\nü§ñ You'r wellcome\")\n",
    "        \n",
    "\n",
    "    try:\n",
    "        if user_input.lower().replace(\" \", \"\") == \"thakyou\":\n",
    "            print(\"\\nü§ñ You'r wellcome\")\n",
    "\n",
    "        language = detect(user_input)  \n",
    "        if language != \"en\":\n",
    "            print(\"\\nü§ñ‚ùå Sorry, I can't understand your text. It must be written in English.\")\n",
    "            continue  \n",
    "        \n",
    "        print(\"\\nü§ñ We are processing your feelings to return the best musical match for you... ‚è≥\")\n",
    "        response = chatbot_response(user_input)\n",
    "        print(\"\\nü§ñ here it is!. \\nThis are the features that have been considered the most accurate for your text:\\n\")\n",
    "        print(json.dumps(response, indent=4, ensure_ascii=False))\n",
    "        print('hope it helped :)')\n",
    "\n",
    "\n",
    "    except:\n",
    "        print(\"\\nü§ñ‚ùå Sorry, I couldn't detect the language of your text. Please try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
