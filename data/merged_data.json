{
    "modes": [
        {
            "name": "Ionian (Major)",
            "pattern": "Tone - Tone - Semitone - Tone - Tone - Tone - Semitone",
            "sonority": "Bright, stable, cheerful.",
            "example": [
                "C",
                "D",
                "E",
                "F",
                "G",
                "A",
                "B",
                "C"
            ]
        },
        {
            "name": "Dorian",
            "pattern": "Tone - Semitone - Tone - Tone - Tone - Semitone - Tone",
            "sonority": "Solemn, ancient, between major and minor.",
            "example": [
                "D",
                "E",
                "F",
                "G",
                "A",
                "B",
                "C",
                "D"
            ]
        },
        {
            "name": "Phrygian",
            "pattern": "Semitone - Tone - Tone - Tone - Semitone - Tone - Tone",
            "sonority": "Exotic, mysterious, often associated with flamenco.",
            "example": [
                "E",
                "F",
                "G",
                "A",
                "B",
                "C",
                "D",
                "E"
            ]
        },
        {
            "name": "Lydian",
            "pattern": "Tone - Tone - Tone - Semitone - Tone - Tone - Semitone",
            "sonority": "Bright, dreamy, cheerful (with an augmented fourth).",
            "example": [
                "F",
                "G",
                "A",
                "B",
                "C",
                "D",
                "E",
                "F"
            ]
        },
        {
            "name": "Mixolydian",
            "pattern": "Tone - Tone - Semitone - Tone - Tone - Semitone - Tone",
            "sonority": "Cheerful, but with a relaxed or folk character (flattened seventh).",
            "example": [
                "G",
                "A",
                "B",
                "C",
                "D",
                "E",
                "F",
                "G"
            ]
        },
        {
            "name": "Aeolian (Natural Minor)",
            "pattern": "Tone - Semitone - Tone - Tone - Semitone - Tone - Tone",
            "sonority": "Melancholic, sad.",
            "example": [
                "A",
                "B",
                "C",
                "D",
                "E",
                "F",
                "G",
                "A"
            ]
        },
        {
            "name": "Locrian",
            "pattern": "Semitone - Tone - Tone - Semitone - Tone - Tone - Tone",
            "sonority": "Dark, unstable, tense (due to the diminished fifth).",
            "example": [
                "B",
                "C",
                "D",
                "E",
                "F",
                "G",
                "A",
                "B"
            ]
        }
    ],
    "mode_relations": {
        "major_modes": [
            "Ionian",
            "Lydian",
            "Mixolydian"
        ],
        "minor_modes": [
            "Aeolian",
            "Dorian",
            "Phrygian",
            "Locrian"
        ]
    },
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3779798/": "An official website of the United States government Here's how you know Search Log in Search in PMC Advanced Search Journal List User Guide As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with, the contents by NLM or the National Institutes of Health. Learn more: PMC Disclaimer PMC Copyright Notice Front Psychol. 2013 Sep 23;4:656. doi: 10.3389fpsyg.2013.00656 Preattentive processing of emotional musical tones: a multidimensional scaling and ERP study Katja N Spreckelmeyer 1, Eckart Altenmüller 2, Hans Colonius 3, Thomas F Münte 4, Author information Article notes Copyright and License information PMCID: PMC3779798 PMID: 24065950 Abstract Musical emotion can be conveyed by subtle variations in timbre. Here, we investigated whether the brain is capable to discriminate tones differing in emotional expression by recording event-related potentials ERPs in an oddball paradigm under preattentive listening conditions. First, using multidimensional Fechnerian scaling, pairs of violin tones played with a happy or sad intonation were rated same or different by a group of non-musicians. Three happy and three sad tones were selected for the ERP experiment. The Fechnerian distances between tones within an emotion were in the same range as the distances between tones of different emotions. In two conditions, either 3 happy and 1 sad or 3 sad and 1 happy tone were presented in pseudo-random order. A mismatch negativity for the emotional deviant was observed, indicating that in spite of considerable perceptual differences between the three equiprobable tones of the standard emotion, a template was formed based on timbral cues against which the emotional deviant was compared. Based on Juslin's assumption of redundant code usage, we propose that tones were grouped together, because they were identified as belonging to one emotional category based on different emotion-specific cues. These results indicate that the brain forms an emotional memory trace at a preattentive level and thus, extends previous investigations in which emotional deviance was confounded with physical dissimilarity. Differences between sad and happy tones were observed which might be due to the fact that the happy emotion is mostly communicated by suprasegmental features. Keywords: preattentive processing, musical emotion, timbre, event-related potential, mismatch negativity, multidimensional scaling Introduction Music, as well as language, can be used to transport emotional information and, from an evolutionary perspective, it does not come as a surprise that the way emotion is encoded in music is similar to the encoding of emotion in human or animal vocalizations. Interestingly, the emotional and semantic processing of speech has been shown to be supported by different brain systems by the method of double dissociation e.g., Heilman et al., 1975. While six patients with right temporoparietal lesions and left unilateral neglect were demonstrated to have a deficit in the comprehension of affective speech, six patients with left temporoparietal lesions exhibited fluent aphasia, i.e., problems with the content of speech, but no problems with affective processing. Likewise, in music processing the Montreal group around Isabelle Peretz has described a patient that is selectively impaired in the deciphering of emotions from music while being unimpaired for the processing of other aspects of music Peretz et al., 2001. Researchers have tried to identify segmental and suprasegmental features that are used to encode emotional information in human speech, animal vocalizations, and music. With regard to animals, similar acoustic features are used by different species to communicate emotions Owings and Morton, 1998. In humans, perceived emotion appears to be mainly driven by the mean level and the range of the fundamental frequency F0 Williams and Stevens, 1972; Scherer, 1988; Sloboda, 1990; Pihan et al., 2000 with low F0 being related to sadness and, conversely, high mean F0 level being related to happiness. In music, Hevner 1935, 1936, 1937 in her classical studies found that tempo and mode had the largest effects on listeners' judgments, followed by pitch level, harmony, and rhythm. According to Juslin 2001 musical features encoding sadness include slow mean tempo, legato articulation, small articulation variability, low sound level, dull timbre, large timing variations, soft duration contrasts, slow tone attacks, flat micro-intonation, slow vibrato, and final ritardando, whereas happiness is encoded by fast mean tempo, small tempo variability, staccato articulation, large articulation variability, fairly high sound level, little sound level variability, bright timbre, fast tone attacks, small timing variations, sharp duration contrasts, and rising micro-intonation. While suprasegmental features are thought to be, at least in part, the result of a lifelong sociocultural conventionalization and therefore, maybe less hardwired Sloboda, 1990, a considerable part of the emotional information is transmitted by segmental features concerning individual tones. For example, a single violin tone might be recognized as sad or happy with a rather high accuracy. Indeed, string and wind instruments which afford a high degree of control over the intonation can be used to mimic the segmental features also used by singers to convey emotional information. Segmental emotional information can be encoded into a single tone by varying its timbre, which might be defined as reflecting the different quality of sounds aside from variations in pitch, loudness, and duration. In addition to different distributions of amplitudes of the harmonic components of a complex tone in a steady state Helmholtz, 18851954, dynamic variations of the sound such as attack time and spectral flux Grey, 1977; Grey and Moorer, 1977 are also important, particularly with regard to onset characteristics. Multidimensional scaling procedures on tones differing in timbre, because they were produced by different by different musical instruments, showed that this aspect of a tone is determined by variations along three dimensions termed attack time, spectral centroid, and spectral flux McAdams et al., 1995. Likewise, in a recent study using multidimensional scaling MDS procedures to investigate the emotional information transmitted by variations in timbre, Eerola et al. 2012 found that affect dimensions could be explained in terms of three kinds of acoustic features: spectral ratio of high-frequency to low-frequency energy, temporal attack slope, and spectro-temporal spectral flux. From the discussion above, there is no question as to the importance of detection of emotional timbre in voice andby extensionin music. The question that we ask here pertains to when in the auditory processing stream emotional timbre is differentially processed. Given the high evolutionary benefit that might be afforded by the rapid decoding of emotional information from single tones or human calls, we hypothesize that such information might be processed early in the processing stream and in an automatic fashion. Indeed, there are a number of studies that have investigated rapid and preattentive classification of emotional sounds. In particular, our group presented normal non-musician participants with tone series comprising a frequent standard single violin tone played with a certain emotional connotation happy or sad and a rare deviant violin tone played with the opposite intonation Goydke et al., 2004. In parallel to the tone series, the EEG was recorded with a focus on the mismatch negativity MMN. The MMN has been shown to be an ideal tool to address the early, automatic stages of sound evaluation Näätänen, 1992; Picton et al., 2000; Näätänen et al., 2001. It is a component of the auditory event related potential ERP which is elicited during passive listening by an infrequent change in a repetitive series of sounds. In the original incarnation of the MMN paradigm, it occurs in response to any stimulus which is physically deviant in frequency, duration or intensity to the standard tone. Importantly, the standard stimulus in typical MMN experiments is the same throughout the experiment. It has been shown, however, that the MMN can also be obtained to deviations within complex series of sounds Picton et al., 2000; Näätänen et al., 2001, in which the memory trace is defined by some abstract property e.g., ascending series of tones. Thus, it appears that the notion of a standardmemory trace can be extended such that the auditory system is capable to extract systematic properties of sound series. Moreover, and important for Goydke et al. 2004 and the present study, the MMN is sensitive to changes in the spectral component of tonal timbre Tervaniemi et al., 1997. The onset latency of the MMN varies according to the nature of the stimulus deviance. Whereas simple, physically deviant stimuli show an onset latency of the MMN of about 150 ms, much later MMNs have been seen with more complex forms of deviance. Finally, it is important to stress the fact that the analysis of the incoming stimulus as well as its encoding appears to take place automatically since the MMN typically occurs when the subjects do not attend to the eliciting stimuli, for example during engagement in a different task such as reading a book Näätänen, 1992. Returning to the Goydke et al. 2004 study, deviant tones were associated with an MMN. The MMN scalp topography for the emotional deviant was similar to an MMN for a control pitch deviant tone. These results were taken to indicate that the brain can categorize tones preattentively on the basis of subtle cues related to the emotional status of the tone Goydke et al., 2004. Studies using a similar logic using both emotionally voiced words Schröder et al., 2006 or vocalizations Bostanov and Kotchoubey, 2004 have revealed analogous findings. Further, investigating different timbral dimensions attack time, spectral centroid, and spectrum fine structure and their consequences for behavioral classification latencies and ERPs in preattentive Caclin et al., 2006 and attentive Caclin et al., 2008 listening conditions, Caclin and colleagues showed that these different timbral features are separately represented in sensory auditory memory. One important aspect has been neglected by these studies, however, in the Goydke et al. 2004 study, a single e.g., happy tone was presented repeatedly as a standard and a single e.g., sad tone was presented repeatedly as the emotional deviant. Thus, it is possible, that the MMN observed for the deviants in this study might have been driven by the physical differences between the standard and deviant stimuli rather than by the postulated preattentive emotional categorization of the stimulus. Indeed, different mechanisms of deviance detection termed sensory and cognitive have been demonstrated for other types of stimulus materials Schröger and Wolff, 1996; Jääskeläinen et al., 2004; Opitz et al., 2005. Therefore, to answer this question and extend our previous findings Goydke et al., 2004, we conducted the present study. As pointed out before, segmental features encoding emotion seem to be varied. Thus, what makes the study of acoustical emotion difficult is, that the set of features encoding the same emotion does not seem to be very well defined and that there is a great variance of feature combinations found within individual emotion categories. We modified the design of our previous MMN study to see whether affective expressions are pre-attentively categorized even when their acoustical structure differs. In other words, several n 3, probability of occurrence for each tone 25 instances of sad or happy tones were defined as standards to which an equally probable deviant stimulus 25 of the other emotion had to be compared preattentively. To the extent that the MMN reflects deviance in the sense of being rare, an MMN under these circumstances would indicate that the standards have been grouped to define a single emotional entity. To test whether the brain automatically builds up categories of basic emotions across tones of different psycho-acoustical structure, it was necessary to create two sets of tones, where tones within one set could clearly be categorized as happy and sad, respectively but differed with respect to their acoustical structure. To this end, we first performed extensive studies to define the stimulus set for the MMN study using MDS methods. Two types of criteria were set for tones to be used as standards in the MMN study: first, each tone needed to be consistently categorized as happy or sad and, second, tones within one set as well as across sets needed to be perceived as different. The first point was addressed by performing affect-ratings on a set of violin tones which only differed in emotional expression but not in pitch or instrumental timbre. To tackle point 2, pairwise same-different-comparisons were collected for all tones and fed into a Fechnerian scaling procedure to assess the perceived similarity among the tones. We will first describe the scaling experiment and will then turn to the MMN experiment. For the latter, we had a straightforward expectation: If the brain categorizes tones preattentively on the basis of an automatic emotional grouping, we should observe an MMN for emotional deviant stimuli regardless of the fact that these emotional deviants were as probable as each of the three different standard stimuli. Scaling experiment Multidimensional Fechnerian scaling Dzhafarov and Colonius, 1999, 2001 is a tool for studying the perceptual relationship among stimuli. The general aim of MDS is to arrange a set of stimuli in a low-dimensional typically Euclidean space such that the distances among the stimuli represent their subjective dissimilarity as perceived by a group of judges. Judges generally perform their ratings in pairwise comparisons between all stimuli in question. Based on the dissimilarity data a MDS procedure finds the best fitting spatial constellation by use of a function minimization algorithm that evaluates different configurations with the goal of maximizing the goodness-of-fit Kruskal, 1964a,b. Though the dimensions found to span the scaling space can often be interpreted as psychologically meaningful attributes that underlie the judgment, no a priori assumptions have to be made about the nature of the dimensions. Thus, with MDS perceptual similarity can be studied without the need to introduce predefined feature concepts as labels for the dimensions which might bias people's judgments. Fechnerian scaling is a development of classical MDS which is more suitable to be used with psychophysical data. Dzhafarov and Colonius 2006 have pointed out that certain requirements for data to be used with classical MDS are usually violated in empirical data, namely the property of symmetry and the property of constant self-dissimilarity. The property of symmetry assumes that discrimination probability is independent of presentation order, and, thus, that the probability to judge a stimulus x as different from a stimulus y is the same no matter whether x or y is presented first px; y py; x. It has been known since Fechner 1860 that this is not true. The property of constant self-dissimilarity expects that any given stimulus is never perceived as different from itself, thus, that the probability to judge stimulus x as different from itself is 0 px; x py; y. However, it has been shown repeatedly that this is not the case in psychophysical data e.g., Rothkopf, 1957. The only requirement made by Fechnerian scaling is that of regular minimality, requesting that the probability to judge a stimulus as different from itself needs to be lower than any other discrimination probability. In the present experiment Fechnerian scaling is used to establish subjective distances for a set of tones where tones differ with respect to their emotional expression. Materials and methods Stimulus material To generate the stimulus material, 9 female violinists all students of the Hanover University for Music and Drama were asked to play brief melodic phrases all ending on c-sharp. Melodies were to be played several times with happy, neutral, or sad expressions. Before each musician started with a new expression, she was shown a sequence of pictures from the IAPS Lang et al., 2008 which depicted happy, neutral or sad scenes, to give her an idea of what was meant by happy, neutral, and sad. All violinists were recorded on the same day in the same room using the same recording technique: stereo 2 Neumann-microphones TLM127, 44.1 kHz sampling rate, 24 bit, distance from the instrument to the microphones was always 50 cm. Each musician filled out a form describing the changes in technique that she had applied to achieve the different expressions. From 200 melodic phrases the last tone always c-sharp was extracted using Adobe Audition. Only those tones were selected which were between 1450 and 1700 ms in length and had a pitch between 550 and 570 Hz. Tones from two violinists had to be discarded altogether because they were consistently below pitch level. The resulting pre-selection comprised 35 tones by 7 different violinists. To soften the tone onset a smooth fade-in envelope was created from 0 to 100 ms post-tone onset. The pre-selection was rated on a 5-point scale from very sad 1 to very happy 5 by 9 student subjects mean age 25.9 years, 5 males naive to the purpose of the study and different from the participants taking part in the final experiment. Each tone was rated twice by each participant to test the raters' consistency. Tones were not amplitude-normalized, because it was found that differences in affective expression could not be differentiated properly in a normalized version. Based on the affect ratings and their consistency 10 tones were selected for the final stimulus set Table 1. Table 1. Features of the stimulus material. Tone Duration ms Frequency Hz, SD Mean level dBA tone01 1676 559.69 2.41 64.5 tone02 1526 558.99 2.04 66.2 tone03 1658 559.98 4.45 72.2 tone04 1628 554.39 3.55 71.6 tone05 1506 555.86 1.13 68.8 tone06 1534 561.86 4.35 68.5 tone07 1660 563.00 4.58 66.6 tone08 1630 561.31 3.61 67.8 tone09 1570 556.96 1.25 72.4 tone10 1608 557.64 0.35 68.8 Mean SD 1599 61.5 559.3 2.75 68.74 2.66 Open in a new tab Design of the same-different forced-choice experiment Participants were 10 students mean age 25.4 years, 5 females with no musical expertise who took part in two separate sessions. In session 1 they performed a same-different forced-choice task on the violin tones to provide data for MDS. In session 2 approximately 1 week later they were asked to rate the emotional expression of the tones on a five-point-scale. For the forced-choice task, participants were tested individually while sitting in a comfortable chair 120 cm away from a 20-zoll-computer screen. All auditory stimuli were presented via closed head-phones Beyerdynamic DT 770 M with a level ranging from 64 to 73 dB. Presentation software Neurobehavioral Systems was used to present trials and to record responses. All 10 tones were combined with each other including themselves, resulting in 10 × 10 100 pairs; all 100 pairs were presented ten times, each time in a different randomized order resulting in 1000 trials altogether. The stimulus onset asynchrony SOA between the two tones of a pair was 3500 ms. Participants had to strike one of two keys to respond same or different forced choice. To make sure participants judged the psychoacoustical similarity of the tones unbiased, they were kept uninformed on the purpose of the experiment. Trial duration was about 6000 ms. The next trial was automatically started when one of the two buttons was pressed. Participants performed a short training to familiarize them with the procedure and were allowed to pause after each block of 25 trials. There were 40 blocks altogether. Participants could end the pause by pressing a button on the keyboard. The duration of the whole experiment was about 2 hours. Participants were verbally instructed to decide whether the two tones comprising a pair were same or different. For the data analysis responses were recorded as 0 same and 1 different. Mean values discrimination probabilities per pair of tones were calculated over all participants and all responses. Minimum number of responses per pair was 90. The resulting discrimination probabilities were transformed into Fechnerian distances using FSDOS Fechnerian Analysis of Discrete Object Sets by Dzhafarov and Colonius, see Affect rating In session 2 each participant from the scaling experiment performed an affect rating of each individual violin tone. All stimuli were presented twice with the order being randomized for each participant. Participants were asked to rate each tone on a 5-point-scale ranging from very sad 1 to very happy 5 by pressing one of the keys from F1 to F5 on the keyboard. Emblematic faces illustrated the sad and the happy end of the scale. Valence and arousal rating Stimulus material was also rated according to valence and arousal by two additional groups of participants. All stimuli were presented twice but the order was randomized for each participant. To give participants an idea what was meant by the terms valence and arousal they performed a short test trial on pictures taken from the IAPS. Group A valence 5 women, 5 men, mean age 27.6 was asked to rate all 10 tones on a 5-point-scale ranging from very negative 1 to very positive 5. Group B 5 women, 5 men, mean age 24.4 was asked to rate the 10 tones from very relaxed German sehr entspannt 1 to highly aroused German sehr erregt 5. Results Same-different forced-choice experiment Discrimination probabilities for each pair of tones based on participants' same-different- judgments are shown in Table 2. Fechnerian distances for each pair of tones calculated from discrimination probabilities are shown in Table 3. Given values reflect the relative distances between pairs of tones as perceived by the mean participant. For example, tone04 abbreviated t.04 in the row, is perceived about 1.5 times more distant from tone05 than from tone07. Table 2. Discrimination probabilities for the 10 tones. tone01 tone02 tone03 tone04 tone05 tone06 tone07 tone08 tone09 tone10 t.01 0.06 0.12 1 0.89 0.74 0.81 0.86 0.94 0.88 0.89 t.02 0.16 0.08 0.98 0.91 0.69 0.72 0.85 0.89 0.88 0.93 t.03 0.99 0.97 0.04 0.93 0.97 0.93 0.85 0.88 0.98 0.95 t.04 0.9 0.93 0.96 0.08 0.82 0.42 0.51 0.64 0.6 0.96 t.05 0.7 0.77 1 0.84 0.08 0.79 0.85 0.91 0.78 0.74 t.06 0.89 0.8 0.94 0.62 0.93 0.07 0.3 0.35 0.74 0.79 t.07 0.92 0.91 0.97 0.69 0.86 0.41 0.09 0.2 0.89 0.93 t.08 0.9 0.91 0.94 0.75 0.9 0.31 0.16 0.1 0.86 0.83 t.09 0.88 0.95 0.96 0.66 0.82 0.77 0.8 0.76 0.08 0.26 t.10 0.91 0.94 1 0.91 0.65 0.77 0.89 0.82 0.34 0.06 Open in a new tab Given are probabilities with which the mean perceiver judged the row tones to be different from the column tones. Table 3. Fechnerian distances. tone01 tone02 tone03 tone04 tone05 tone06 tone07 tone08 tone09 tone10 t.01 0.000 0.140 1.890 1.650 1.290 1.510 1.630 1.670 1.620 1.680 t.02 0.140 0.000 1.830 1.680 1.290 1.370 1.590 1.620 1.660 1.730 t.03 1.890 1.830 0.000 1.770 1.850 1.760 1.690 1.680 1.820 1.850 t.04 1.650 1.680 1.770 0.000 1.500 0.890 1.030 1.190 1.100 1.550 t.05 1.290 1.290 1.850 1.500 0.000 1.570 1.540 1.630 1.440 1.250 t.06 1.510 1.370 1.760 0.890 1.570 0.000 0.550 0.490 1.360 1.430 t.07 1.630 1.590 1.690 1.030 1.540 0.550 0.000 0.170 1.520 1.660 t.08 1.670 1.620 1.680 1.190 1.630 0.490 0.170 0.000 1.440 1.490 t.09 1.620 1.660 1.820 1.100 1.440 1.360 1.520 1.440 0.000 0.460 t.10 1.680 1.730 1.850 1.550 1.250 1.430 1.660 1.490 0.460 0.000 Open in a new tab Distances were calculated by FSDOS the larger the value the more distant the tones. Affect, arousal, and valence rating Results of the affect, arousal, and valence ratings are shown in Table 4 collapsed over the first and second presentation which did not differ significantly. Please note, that the affect rating was performed by the same group of participants that also took part in the same-different forced choice experiment, whereas the arousal and valence ratings were performed by two different groups of subjects. Though stemming from different groups of participants, there was a high correlation between the affect and the arousal ratings r 0.937, p 0.001. In contrast, the correlation between valence and affect ratings was rather low r 0.651, p 0.042. This is surprising for it was expected that valence and affect are closely related. It has to be noted, though, that during the testing it became apparent that participants used different concepts for the valence dimension. While some understood positivenegative in the sense of pleasantunpleasant, others linked positivenegative to the two ends of the dimension to happy and sad. This problem is paralleled by a heterogeneous use of the valence-term in the literature see Russell and Barrett, 1999, for a discussion and might serve as an explanation for the incongruous pattern. In the current experiment the valence ratings will therefore, be interpreted with caution. Table 4. Results of the affect, arousal, and valence ratings. Affect Arousal Valence Label tone01 1.90 0.61 1.75 0.42 2.80 1.40 sad01 tone02 1.95 0.61 1.90 0.66 3.20 0.98 sad02 tone03 4.40 0.94 4.55 0.44 3.55 0.90 tone04 2.90 0.39 3.15 1.00 3.35 0.67 tone05 2.20 0.71 1.80 0.54 2.70 0.63 sad03 tone06 2.70 0.59 3.00 0.62 3.25 0.49 tone07 3.45 0.98 2.95 0.55 2.95 0.44 hap01 tone08 3.60 0.77 3.20 0.71 3.30 0.63 hap02 tone09 3.35 0.71 3.40 0.81 3.25 1.03 hap03 tone10 2.55 0.55 2.80 0.63 2.70 1.01 Open in a new tab Each scale ranged from 1 to 5; last column gives the label of the tone for the MMN study. Selection of stimuli for the MMN experiment Three sad tones tone01 sad01, tone02 sad02, tone05 sad03 and 3 happy tones tone07 hap01, tone08 hap02, tone09 hap03 were chosen from the data set based on their affect ratings. The happy tones had mean affect ratings of 3.45, 3.60, and 3.35; sad tones were rated 1.90, 1.95, and 2.20, respectively. Affect ratings of happy and sad tones were significantly different F9, 90 12.9 p 0.001 and scaling procedures demonstrated that tones were perceived as different even when belonging to the same emotion category. Fechnerian distances between happy and sad tones fell between 1.44 and 1.67. Distances were 0.17, 1.52, and 1.44 among happy tones and 0.14 and 1.29 among sad tones. Event-related potential experiment Methods Participants Of a total of 19 participants three had to be excluded because of technical error two or too many blink artifacts in the ERP data one. The remaining 16 participants 8 women were aged between 21 and 29 years mean 24.9. None was a professional musician. Design Stimuli were the 6 different single violin tones chosen on the basis of the scaling experiment. Two conditions were set up in a modified oddball-design. In condition A 3 sad tones were presented in random order standards with 1 happy tone deviant randomly interspersed. In condition B 3 happy tones were presented as standards with 1 sad tone randomly interspersed as deviant tone. As deviants, the tones with the lowest and highest affect ratings were chosen. The probability of occurrence was 25 for each of the three standard tones and the deviant tone, resulting in an overall probability of 75 for the standard stimuli and 25 for the affective deviant. In both conditions each tone was presented 340 times resulting in a total of 1360 tones per condition. A randomization algorithm guaranteed that identical tones were never presented back-to-back. Both conditions were divided in two blocks of 680 tones. The order of blocks was ABAB or BABA. All four blocks were presented in one session with one pause between block 2 and 3. The total duration of the experiment was about 90 min. Tones were presented via insert ear phones used with Earlink ear-tips Aearo Comp.. Stimulus onset asynchrony between two tones was 2000 ms. Mean sound pressure level of the presentation of all tones was 70 dB. To realize a non-attentive listening paradigm, participants were instructed to pay attention to cartoons Tom and JerryThe classical collection 1 presented silently on a computer screen in front of them. To control how well participants had attended the film a difficult post-test was performed after the experiment requiring participants to recognize selected scenes. On average, 85 of the scenes were classified correctly, indicating that the participants had indeed attended the film. ERP-recording The electroencephalogram EEG was recorded from 32 tin electrodes mounted in an elastic cap according to the 1020-system. Electrode impedance was kept below 5 k. The EEG was amplified bandpass 0.140 Hz and digitized continuously at 250 Hz. Electrodes were referenced on-line to the left mastoid. Subsequently, off-line re-referencing to an electrode placed on the nose-tip was performed. Electrodes placed at the outer canthus of each eye were used to monitor horizontal eye movements. Vertical eye movements and blinks were monitored by electrodes above and below the right eye. Averages were obtained for 1024 ms epochs including a 100 ms pre-stimulus baseline period. Trials contaminated by eye movements or amplifier blocking or other artifacts within the critical time window were rejected prior to averaging. For this, different artifact rejection thresholds were defined for the eye- and EEG channels. Separate averages were calculated for each tone in both conditions. ERPs were quantified by mean amplitude measures using the mean voltage of the 100 ms period preceding the onset of the stimulus as a reference. Time windows and electrode sites are specified at the appropriate places of the result section. Effects were tested for significance in separate ANOVAs, with stimulus type standard or deviant and electrode site as factors. The Huynh-Feldt epsilon correction Huynh and Feldt, 1980 was used to correct for violations of the sphericity assumption. Reported are the original degrees of freedom and the corrected p-values. Significance level was set to p 0.05. Results The grand average waveforms to the standard and deviant tones Figure 1 are characterized by a N1-P2-complex as typically found in auditory stimulation Näätänen et al., 1988, followed by a long-duration negative component with a frontal maximum and a peak around 400500 ms. The current design allows two different ways to assess emotional deviants. Firstly, deviants and standards collected in the same experimental blocks can be compared i.e., happy standard vs. sad deviant or sad standard vs. happy deviant. These stimulus classes are emotionally as well as physically different. Secondly, the ERP to the deviant can be compared with the same tone when it was presented as standard in the other condition, such that the compared stimuli are physically identical but differ in their functional significance as standard and deviant i.e., sad standard vs. sad deviant and happy standard vs. happy deviant, see Table 5. Time windows for the statistical analysis were set as follows: 100200 ms N1, 200300 ms P2, and 380600 ms. Electrode sites included in the analysis were F3, F4, FC5, FC6, C3, C4, Fz, FCz, Cz. Figure 1. Open in a new tab Grand average ERPs for condition A top and B bottom; the respective standard-ERP bold line is depicted with the ERP to the emotionally deviating tone when it was presented as deviant dotted line or as standard in the other condition dashed line. Highlighted time windows mark significant differences in both standard-deviant comparisons. Table 5. Comparison of standard vs. deviant stimuli. Comparison Standard Deviant 100200 ms 200300 ms 380600 ms Condition A Sad standards HAP02 0.93 2.40 7.32 Condition B Happy standards SAD01 0.06 10.94 0.00 Across conditions HAP02 as std. HAP02 0.27 0.55 9.20 Across conditions SAD01 as std. SAD01 3.04 0.00 0.01 Open in a new tab Given are the F-values df 1,15. p 0.01; p 0.05. In condition A, emotional happy deviants elicited a more negative waveform in a late latency range from 380 ms, regardless of the comparison Figure 1, top; Table 5. Thus, the mismatch response cannot be explained by the fact that physically different tones elicited the different ERP waveforms. To illustrate the scalp distribution of this effect, the difference happy deviant minus sad standards was computed and the mean amplitude of the difference waveform in the time-window 500600 ms was used to create spline-interpolated isovoltage maps. The topographical distribution was typical for an MMN response. In particular, we observed a polarity inversion at temporobasal mastoid electrode sites Figure 2. In condition B Figure 1, bottom; Table 5, sad deviants, too, elicited a more negative waveform than the happy standards, though in an earlier latency range P2, 200300 ms. However, no difference was found when the ERPs to the sad tone were compared across conditions, suggesting that this effect was triggered by the structural difference of happy and sad tones rather than their functional significance as standard and deviant. To summarize the result: presenting a happy tone in a series of sad tones resulted in a late negativity that was larger in amplitude than the ERP to the same happy tone functioning as standard in the opposite condition. In contrast, no difference that could be related to its functional significance was found for the sad tone presented in a train of differing happy tones. Figure 2. Open in a new tab Spline-interpolated isovoltage maps depicting the mean amplitude of the happy deviant minus sad standard difference wave from condition A. A typical frontal maximum was observed. The polarity inversion at temporobasal electrodes suggests that this response belongs to the MMN family. Discussion The affective deviant in condition A evoked a clear mismatch reaction. Though the latency was rather long, its topographic distribution, including the typical inversion of polarity over temporal regions see Figure 2 in our nose-tip referenced data, suggests that it belongs to the MMN-family. Indeed, it is a known fact that MMN-latency increases with discrimination difficulty. In this regard, we would like to point to the predecessor study Goydke et al., 2004, in which we obtained a rather long latency of the MMN response for emotional deviants, even though the latency was still shorter than in the present study. No doubt, discrimination was particularly difficult in the present experiment, because the difference in timbre was reduced to subtle changes in the expression of same-pitch and same-instrument tones. The mismatch reaction observed for condition A suggests that a happy tone was pre-attentively categorized as different from a group of different sad tones. An MMN reflects change detection in a previously established context Näätänen, 1992. Thus, for it to occur, a context needs to be set up first. Consequently, the important question in the present experiment is not, what is so particular about the happy tone? The question is, what has led to grouping the standard sad tones into one mutual category, so that the single happy tone was perceived as standing out? For the happy tone to be categorized as deviant it was required that the sad tonesthough different in structurewere perceived as belonging to the same context, i.e., category. The question thus, arises: what has led to grouping of the sad tones? Three possibilities seem plausible: perceptual similarity emotional similarity or emotion-specific perceptual similarity Perceptual similarity From the result of the scaling-experiment it can be derived, that tones within the sad category were perceived quite as different from each other on a perceptual basis e.g., sad01 and sad03: Fechnerian distance 1.29 as was the happy deviant from the sad standards e.g., sad03 vs. happy deviant: Fechnerian distance 1.44. Relative distances are visualized in Figure 3. The arrangement of tones in a three dimensional space results from feeding Fechnerian distance values into a MDS procedure Alscal in SPSS which finds the optimal constellation of stimuli in an n-dimensional space based on dissimilarity data. Three dimensions were found to explain 99 of variance. Note that the orientation of the dimensions is arbitrary. Though the positions of SAD01 and SAD02 are relatively close, both are rather distant from SAD03. Grouping, thus, cannot be explained by perceptual similarity alone. Figure 3. Open in a new tab Arrangement of tones in a three dimensional space based on the multidimensional scaling procedure. Note that orientation of dimensions is arbitrary. Emotional similarity Affect ratings 1.90, 1.95, and 2.20 indicate that the tones were perceived as equally sad in expression. There thus, is some support for the hypothesis that the tones were grouped together based on their emotional category. However, if it was the emotional expression that has led to the automatic categorization why did it not work in condition B? No index was found for a mismatch reaction in response to a sad tone randomly interspersed in a train of different happy tones. Arguing along the same line as before, this non-finding implies that either no mutual standard memory trace was built for the happy tones or that this memory trace was considerably weaker for these tones. Since the affect ratings of the happy tones were just as homogeneous 3.35, 3.45, and 3.60 as those of the sad tones, the question arises, if the affect ratings gave a good enough representation of the emotion as it was decoded by the listeners. Against the background that decoding accuracy of acoustical emotion expressions has repeatedly been reported to be better for sadness than for happiness Johnstone and Scherer, 2000; Elfenbein and Ambady, 2002; Juslin and Laukka, 2003, it might be necessary to take a second look at the stimulus material. Banse and Scherer 1996 found that if participants had the option to choose among many different emotional labels to rate an example of vocal expression, happiness was often confused with other emotions. In the present experiment participants had given their rating on bipolar dimensions ranging from happy to sad. It cannot be ruled out that the response format biased the outcome. It is, for example, possible that in some cases participants chose to rate happy because the tone was found to be definitely not-sad, even if it was not perceived as being really happy either. In an attempt to examine the perceived similarity of the tones with respect to the expressed emotion without pre-selected response categories, a similarity rating on emotional expression was performed post-hoc. For that purpose, the same students who had participated in the first scaling-experiment were asked to perform another same-different-judgment on the same stimulus material, though this time with regard to the emotion expressed in the tone. The results are depicted in Table 6 and show that sad tones t.01, t.02, t.05 were perceived considerably more similar to each other with respect to the emotion expressed than the happy tones t.07, t.08, t.09. In fact, sad tones were judged half as dissimilar from each other than the happy tones 0.503 vs. 1.02. Figure 4 shows the relation of same and different responses given for happy and sad tone pairs, respectively. Sad tones were considerably more often considered to belong to the same emotional category than happy tones 80 vs. 57 same-responses. It can be assumed that in the MMN-experiment, too, sad tones in condition A were perceived as belonging into one emotional category while happy tones in condition B were not. The difficulty to attribute the happy tones to the same standard category can serve as explanation why the sad tone did not evoke a MMN. It was not registered as deviant against a happy context, because no such context existed. Nevertheless, the hypothesis that the MMN reflects deviance detection based on emotional categorization can at least be maintained for condition A. Table 6. Fechnerian distances as calculated from same-different-judgments of emotional expression for the 10 tones. tone01 tone02 tone03 tone04 tone05 tone06 tone07 tone08 tone09 tone10 t.01 0.000 0.012 1.763 1.003 0.491 0.943 1.103 1.003 1.072 0.983 t.02 0.012 0.000 1.751 0.991 0.503 0.931 1.091 0.991 1.072 0.971 t.03 1.763 1.751 0.000 1.390 1.700 1.040 0.880 0.990 1.420 1.560 t.04 1.003 0.991 1.390 0.000 0.820 0.580 0.630 0.620 0.600 0.750 t.05 0.491 0.503 1.700 0.820 0.000 1.020 1.170 1.080 0.730 0.650 t.06 0.943 0.931 1.040 0.580 1.020 0.000 0.160 0.060 0.860 0.850 t.07 1.103 1.091 0.880 0.630 1.170 0.160 0.000 0.110 1.020 1.010 t.08 1.003 0.991 0.990 0.620 1.080 0.060 0.110 0.000 0.920 0.910 t.09 1.072 1.072 1.420 0.600 0.730 0.860 1.020 0.920 0.000 0.150 t.10 0.983 0.971 1.560 0.750 0.650 0.850 1.010 0.910 0.150 0.000 Open in a new tab Given are perceived distances of row tones and column tones with respect to their emotional expression; sad tones were t.01, t.02, and t.05, happy tones were t.07, t.08, and t.09. Figure 4. Open in a new tab Same and different responses for tone pairs in the categories sad left and happy right, respectively. Emotion-specific perceptual similarity It was presupposed that emotion recognition in acoustical stimuli is based on certain acoustical cues coding the emotion intended to be expressed by the sender. To test whether the sad tones in the present experiment were similar with regard to prototypical cues for sadness an acoustical analysis was performed on the stimulus set. Tones were analyzed on the parameters found to be relevant in the expression of emotion on single tones Juslin, 2001. Using PRAAT Boersma, 2001 and dBSonic, tones were assessed for the following features: high frequency energy, attack, mean pitch, pitch contour, vibrato amplitude, vibrato rate, sound level. For each feature, the range of values was divided into three categories low, medium, high and each tone was classified accordingly Table 7. The acoustical analysis revealed that some though not all parameters were manipulated the way it would have been expected based on previous findings. However, Table 7 indicates that the cues were not used homogeneously. For example, mean pitch level was not a reliable cue. Moreover, vibrato was manipulated in individual ways by the musicians. Timbre, however, was well in line with expectations. All sad tones were characterized by little energy in the high frequency spectrum. In contrast, more energy in high frequencies was found in the spectrum of the deviant happy tone. Based on the findings by Tervaniemi et al. 1994 it appears that a difference in spectral structure alone can trigger the MMN. That would mean that the sad tones were grouped together as standards based on their mutual feature of attenuated higher partials. It has to be noted though that the high-frequency energy parameter is a very coarse means to describe timbre. Especially in natural tones compared to synthesized tones as used by Tervaniemi et al. 1994 the spectrum comprises a large number of frequencies with different relative intensities. As a consequence, the tones still have very individual spectra and consequently sounds, even if they all display a relatively low high-frequency energy level. This fact is also reflected in the low perceptual similarity ratings. Moreover, if the spectral structure really was the major grouping principle, it should also have applied to the happy tones in condition B. Here, all happy tones were characterized by a high amount of energy in high frequencies, while the sad deviant was not. Nevertheless, no MMN was triggered. To conclude, though the possibility cannot be completely ruled out, it is not very likely that the grouping of the sad tones was based solely on similarities of timbre structure. Instead, the heterogeneity of parameters in Table 7 provides support for Juslin's idea of redundant code usage in emotion communication Juslin, 1997b, 2001. Obviously, expressive cues were combined differently in different sad tones. Thus, though the sad tones did not display homogeneous patterns of emotion-specific cues, each tone was characterized by at least two prototypical cues for sadness expression. Based on the model assumption of redundant code usage, it seems likely that tones were grouped together because they were identified as belonging to one emotional category based on emotion specific-cues. Table 7. Results of the acoustical analysis of the sad tones. SAD01 SAD02 SAD03 Timbre high frequency energy Low Low Low Attack Medium Medium Medium Mean pitch Low Medium Medium Pitch contour Normal Down Down Vibrato amplitude Medium Medium Low Vibrato rate Slow Medium Slow Sound level Low Medium Medium Open in a new tab Tested were parameters expected to be relevant cues to express emotion on single tones. Categorization as low, medium, and high was based on comparison with the happy tones. What implication does this consideration have for the question of grouping principles in the MMN-experiment? From what is known about the principles of the MMN, the results imply that the representation of the standard in memory included invariances across several different physical features. The invariances, however, needed to be in line with a certain template on how sadness is acoustically encoded. Several researchers have suggested the existence of such hard-wired templates for the rapid processing of emotional signals Lazarus, 1991; LeDoux, 1991; Ekman, 1999; Scherer, 2001. It is assumed that to allow for quick adaptational behavior, stimulus evaluation happens fast and automatic. Incoming stimuli are expected to run through a matching process in which comparison with a number of schemes or templates takes place. Templates can be innate andor formed by social learning Ekman, 1999. The present study, while blind with respect to the origin of the template, provides some information as to how such a matching process might be performed on a pre-attentive level. Given the long latency of the MMN in the present experiment, it can be assumed that basic sensory processing has already taken place before the mismatch reaction occurs. Therefore, the MMN in the current experiment appears to reflect the mismatch between the pattern of acoustic cues identified as emotionally significant and the template for sad stimuli activated by the preceding standard tones. Our data is thus, in line with considerations that the MMN does not only occur in response to basic acoustical feature processing. Several authors have suggested that the MMN can also reflect holistic Gomes et al., 1997; Sussman et al., 1998 or gestalt-like Lattner et al., 2005 perception. They assume that the representation of the standard in the auditory memory system is not merely built up based on the just presented standard-stimuli, but that it can be influenced by prototypical representations stored in other areas of the brain Phillips et al., 2000. Evidence from a speech-specific phoneme processing task suggested that the MMN-response does not only rely on matching processes in the transient memory store but that long-term representations for prototypical stimuli were accessed already at a pre-attentive level. For phonemes, Näätänen and Winkler, 1999 assumed the existence of long-term memory traces serving as recognition patterns or templates in speech perception. He further posited that these can be activated by sounds nearly matching with the phoneme-specific invariant codes p. 14. In another contribution, Näätänen et al. 2005 point out that the mechanisms of generation of these more cognitive kinds of MMNs of course involve other, obviously higher-order, neural populations than those activated by a mere frequency change. p. 27. In the model of Schirmer and Kotz 2006 emotional-prosodic processing is conceptualized as a hierarchical process. Stage 1 comprises initial sensory processing of the auditory information before emotionally significant cues are integrated stage 2 and cognitive evaluation processes stage 3 take place. The MMN in response to emotional auditory stimuli might reflect the stage of integrating emotionally significant cues Schirmer et al., 2005. The present data is compatible with the model albeit in the area of nonverbal auditory emotion processing. The current data contributes to disentangling the processes underlying emotion recognition in the auditory domain. It has to be pointed out though that the present results can only give a first glimpse on the mechanisms underlying processing of emotionally expressive tones. More studies with a larger set of tones characterized by different cues are needed to systematically examine the nature of the stimulus evaluation process. Also, a critical issue for emotion recognition from musical sounds might be the time over which a listener can integrate the information. This might be the answer to the question as to why the happy tones were perceived less homogeneous than the sad tones. While all musicians had the intention to express happiness, it is possible that happiness can just not be expressed very well on single tones. Juslin 1997a, when looking for predictors of emotional ratings of musical performances, found that the best predictors for happiness were tempo and articulation. Both parameters are suprasegmental features and require a whole sequence of tones. In contrast, sadness ratings could be predicted by a number of cues, including segmental features such as sound level, spectrum, and attack. Conflict of interest statement The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. Acknowledgments This research was supported by the Studienstiftung des Deutschen Volkes Katja N. Spreckelmeyer and the Deutsche Forschungsgemeinschaft Katja N. Spreckelmeyer, Hans Colonius, Eckart Altenmüller, Thomas F. Münte. Hans Colonius and Thomas F. Münte were members of the SFB TR31 Active Listening during the time of the experiment. References Banse R., Scherer K. R. 1996. Acoustic profiles in vocal emotion expression. J. Pers. Soc. Psychol. 70, 614636 10.10370022-3514.70.3.614 DOI PubMed Google Scholar Boersma P. 2001. Praat, a system for doing phonetics by computer. Glot Int. 5, 341345 Google Scholar Bostanov V., Kotchoubey B. 2004. Recognition of affective prosody: continuous wavelet measures of event-related brain potentials to emotional exclamations. Psychophysiology 41, 259268 10.1111j.1469-8986.2003.00142.x DOI PubMed Google Scholar Caclin A., Brattico E., Tervaniemi M., Näätänen R., Morlet D., Giard M. H., et al. 2006. Separate neural processing of timbre dimensions in auditory sensory memory. J. Cogn. Neurosci. 18, 19591972 10.1162jocn.2006.18.12.1959 DOI PubMed Google Scholar Caclin A., McAdams S., Smith B. K., Giard M. H. 2008. Interactive processing of timbre dimensions: an exploration with event-related potentials. J. Cogn. Neurosci. 20, 4964 10.1162jocn.2008.20001 DOI PubMed Google Scholar Dzhafarov E., Colonius H. 2006. Generalized fechnerian scaling, in Measurement and Representation of Sensations, eds Colonius H., Dzhafarov E. Mahwah, NJ: Erlbaum; , 4787 Google Scholar Dzhafarov E. N., Colonius H. 1999. Fechnerian metrics in unidimensional and multidimensional stimulus spaces. Psychon. Bull. Rev. 6, 239268 10.3758BF03212329 DOI PubMed Google Scholar Dzhafarov E. N., Colonius H. 2001. Multidimensional fechnerian scaling: basics. J. Math. Psychol. 45, 670719 10.1006jmps.2000.1341 DOI PubMed Google Scholar Eerola T., Ferrer R., Alluri V. 2012. Timbre and affect dimensions: evidence from affect and similarity ratings and acoustic correlates of isolated instrument sounds. Music Percept. 30, 4970 10.1525mp.2012.30.1.49 DOI Google Scholar Ekman P. 1999. Basic emotions, in Handbook of Cognition and Emotion, eds Dalgleish T., Power M. Sussex: John Wiley and Sons Ltd. 4560 Google Scholar Elfenbein H. A., Ambady N. 2002. On the universality and cultural specificity of emotion recognition: a meta-analysis. Psychol. Bull. 128, 203235 10.10370033-2909.128.2.203 DOI PubMed Google Scholar Fechner G. 1860. Elemente der Psychophysik. Leipzig: Breitkopf und Härtel Google Scholar Gomes H., Bernstein R., Ritter W., Vaughan H. G., Miller J. 1997. Storage of feature conjunctions in transient auditory memory. Psychophysiology 34, 712716 10.1111j.1469-8986.1997.tb02146.x DOI PubMed Google Scholar Goydke K. N., Altenmüller E., Möller J., Münte T. F. 2004. Changes in emotional tone and instrumental timbre are reflected by the mismatch negativity. Brain Res. Cogn. Brain Res. 21, 351359 10.1016j.cogbrainres.2004.06.009 DOI PubMed Google Scholar Grey J. 1977. Multidimensional perceptual scaling. J. Acoust. Soc. Am. 61, 12701277 10.11211.381428 DOI PubMed Google Scholar Grey J., Moorer J. A. 1977. Perceptual evaluation of synthetic music instrument tones. J. Acoust. Soc. Am. 62, 454462 10.11211.381508 DOI Google Scholar Heilman K. M., Scholes R., Watson R. T. 1975. Auditory affective agnosia. J. Neurol. Neurosurg. Psychiatry 38, 6972 10.1136jnnp.38.1.69 DOI PMC free article PubMed Google Scholar Helmholtz H. 18851954. On the Sensations of Tone. New York, NY: Dover Publications Google Scholar Hevner K. 1935. The affective character of the major and minor modes in music. Am. J. Psychol. 47, 103118 10.2307141671023914179 DOI Google Scholar Hevner K. 1936. Experimental studies of the elements of expression in music. Am. J. Psychol. 48, 246268 10.2307141574619884142 DOI Google Scholar Hevner K. 1937. The affective value of pitch and tempo in music. Am. J. Psychol. 49, 621630 10.23071416385 DOI Google Scholar Huynh H., Feldt L. 1980. Conditions under which mean square ratios in repeated measure designs have excact f-distributions. J. Am. Stat. Assoc. 65, 15821589 10.108001621459.1970.10481187 DOI Google Scholar Jääskeläinen I. P., Ahveninen J., Bonmassar G., Dale A. M., Ilmoniemi R. J., Levanen S., et al. 2004. Human posterior auditory cortex gates novel sounds to consciousness. Proc. Natl. Acad.Sci. U.S.A. 101, 68096814 10.1073pnas.0303760101 DOI PMC free article PubMed Google Scholar Johnstone T., Scherer K. R. 2000. Vocal communication of emotion, in Handbook of Emotions, eds Lewis M., Haviland-Jones J. New York, NY: Guilford Press; , 220235 Google Scholar Juslin P. 1997a. Perceived emotional expression in synthesized performances of a short melody: capturing the listener's judgment policy. Musicae Scientiae 1, 225256 Google Scholar Juslin P. 1997b. Emotional communication in music performance: a functionalist perspective and some data. Music Percept. 14, 383418 10.230740285731 DOI Google Scholar Juslin P. 2001. Communicating emotion in music performance: a review and theoretical framework, in Music and Emotion, eds Juslin P., Sloboda J. Oxford: Oxford University Press; , 309337 Google Scholar Juslin P., Laukka P. 2003. Communication of emotions in vocal expression and music performance: different channels, same code? Psychol. Bull. 129, 770814 10.10370033-2909.129.5.770 DOI PubMed Google Scholar Kruskal J. 1964a. Multidimensional scaling by optimizing goodness of fit to a non-metric hypothesis. Psychometrika 29, 127 10.1007BF02289565 DOI Google Scholar Kruskal J. 1964b. Non-metric multidimensional scaling: a numerical method. Psychometrika 29, 115129 10.1007BF02289694 DOI Google Scholar Lang P. J., Bradley M. M., Cuthbert B. N. 2008. International affective picture system IAPS: Affective Ratings of Pictures and Instruction Manual. Technical Report A-8. University of Florida, Gainesville, FL. Google Scholar Lattner S., Meyer M. E., Friederici A. D. 2005. Voice perception: sex, pitch, and the right hemisphere. Hum. Brain Mapp. 24, 1120 10.1002hbm.20065 DOI PMC free article PubMed Google Scholar Lazarus R. 1991. Emotion and Adaptation. New York, NY: Oxford University Press Google Scholar LeDoux J. E. 1991. Emotion and the brain. J. NIH Res. 3, 4951 Google Scholar McAdams S., Winsberg S., Donnadieu S., De Soete G., Krimphoff J. 1995. Perceptual scaling of synthesized musical timbres: common dimensions, specificities, and latent subject classes. Psychol. Res. 58, 177192 10.1007BF00419633 DOI PubMed Google Scholar Näätänen R. 1992. Attention and Brain Function. Hillsdale, NJ: Erlbaum Google Scholar Näätänen R., Jacobsen T., Winkler I. 2005. Memory-based or afferent processes in mismatch negativity MMN: a review of the evidence. Psychophysiology 42, 2532 10.1111j.1469-8986.2005.00256.x DOI PubMed Google Scholar Näätänen R., Sams M., Alho K., Paavilainen P., Reinikainen K., Sokolov E. N. 1988. Frequency and location specificity of the human vertex n1 wave. Electroencephalogr. Clin. Neurophysiol. 69, 523531 10.10160013-46948890164-2 DOI PubMed Google Scholar Näätänen R., Tervaniemi M., Sussman E., Paavilainen P., Winkler I. 2001. Primitive intelligence in the auditory cortex. Trends Neurosci. 24, 283288 10.1016S0166-22360001790-2 DOI PubMed Google Scholar Näätänen R., Winkler I. 1999. The concept of auditory stimulus presentation in cognitive neuroscience. Psychol. Bull. 6, 826859 10.10370033-2909.125.6.826 DOI PubMed Google Scholar Opitz B., Schröger E., von Cramon D. Y. 2005. Sensory and cognitive mechanisms for preattentive change detection in auditory cortex. Eur. J. Neurosci. 21, 531535 10.1111j.1460-9568.2005.03839.x DOI PubMed Google Scholar Owings D., Morton E. 1998. Animal Vocal Communication: A New Approach. Cambridge: Cambridge University Press; 10.1017CBO9781139167901 DOI Google Scholar Peretz I., Blood A. J., Penhune V., Zatorre R. 2001. Cortical deafness to dissonance. Brain 124, 928940 10.1093brain124.5.928 DOI PubMed Google Scholar Phillips C., Pellathy T., Marantz A., Yellin E., Wexler K., Poeppel D., et al. 2000. Auditory cortex accesses phonological categories: an MEG mismatch study. J. Cogn. Neurosci. 12, 10381055 10.116208989290051137567 DOI PubMed Google Scholar Picton T. W., Alain C., Otten L., Ritter W., Achim A. 2000. Mismatch negativity: different water in the same river. Audiol. Neurootol. 5, 111139 10.1159000013875 DOI PubMed Google Scholar Pihan H., Altenmüller E., Hertrich I., Ackermann H. 2000. Cortical activation patterns of affective speech processing depend on concurrent demands on the subvocal rehearsal system. A DC-potential study. Brain 123, 23382349 10.1093brain123.11.2338 DOI PubMed Google Scholar Rothkopf E. Z. 1957. A measure of stimulus similarity and errors in some paired- associate learning tasks. J. Exp. Psychol. 53, 94101 10.1037h0041867 DOI PubMed Google Scholar Russell J., Barrett L. 1999. Core affect, prototypical emotional episodes, and other things called emotion: dissecting the elephant. J. Pers. Soc. Psychol. 76, 805819 10.10370022-3514.76.5.805 DOI PubMed Google Scholar Scherer K. R. 1988. On the symbolic function of vocal affect expression. J. Lang. Soc. Psychol. 7, 79100 10.11770261927X8800700201 DOI Google Scholar Scherer K. R. 2001. The nature and study of appraisal: a review of the issue, in Appraisal Processes in Emotion: Theory, Methods, Research, eds Scherer K. R., Schorr A., Johnstone T. Oxford: Oxford University Press; , 369391 Google Scholar Schirmer A., Kotz S. A. 2006. Beyond the right hemisphere: brain mechanisms mediating vocal emotional processing. Trends Cogn. Sci. 10, 2430 10.1016j.tics.2005.11.009 DOI PubMed Google Scholar Schirmer A., Striano T., Friederici A. D. 2005. Sex differences in the preattentive processing of vocal emotional expressions. Neuroreport 16, 635639 10.109700001756-200504250-00024 DOI PubMed Google Scholar Schröder C., Möbes J., Schütze M., Szymanowski F., Nager W., Bangert M., et al. 2006. Perception of emotional speech in Parkinson's disease. Mov. Disord. 21, 17741778 10.1002mds.21038 DOI PubMed Google Scholar Schröger E., Wolff C. 1996. Mismatch response of the human brain to changes in sound location. Neuroreport 7, 30053008 10.109700001756-199611250-00041 DOI PubMed Google Scholar Sloboda J. 1990. Empirical studies of the emotional response to music, in Cognitive Bases of Musical Communication, eds Jones M., Holleran S. Washington, DC: American Psychological Association; , 3346 Google Scholar Sussman E., Gomes H., Nousak J. M., Ritter W., Vaughan H. G. 1998. Feature conjunctions and auditory sensory memory. Brain Res. 793, 95102 10.1016S0006-89939800164-4 DOI PubMed Google Scholar Tervaniemi M., Maury S., Näätänen R. 1994. Neural representations of abstract stimulus features in the human brain as reflected by the mismatch negativity. Neuroreport 5, 844846 10.109700001756-199403000-00027 DOI PubMed Google Scholar Tervaniemi M., Winkler I., Näätänen R. 1997. Pre-attentive categorization of sounds by timbre as revealed by event-related potentials. Neuroreport 8, 25712574 10.109700001756-199707280-00030 DOI PubMed Google Scholar Williams C. E., Stevens K. N. 1972. Emotions and speech: some acoustical correlates. J. Acoust. Soc. Am. 52, 12381250 10.11211.1913238 DOI PubMed Google Scholar Articles from Frontiers in Psychology are provided here courtesy of Frontiers Media SA ACTIONS View on publisher site PDF 921.3 KB Cite Collections Permalink RESOURCES Similar articles Cited by other articles Links to NCBI Databases ON THIS PAGE Abstract Introduction Scaling experiment Materials and methods Results Event-related potential experiment Discussion Acknowledgments References FOLLOW NCBI Connect with NLM National Library of Medicine 8600 Rockville Pike Bethesda, MD 20894 Web Policies FOIA HHS Vulnerability Disclosure Help Accessibility Careers NLM NIH HHS USA.gov",
    "https://www.psypost.org/new-research-uncovers-atonal-musics-distinct-emotional-and-neural-effects/": "SUBSCRIBE The latest psychology and neuroscience discoveries. MY ACCOUNT MENTAL HEALTH SOCIAL PSYCHOLOGY COGNITIVE SCIENCE PSYCHOPHARMACOLOGY NEUROSCIENCE ABOUT Home Exclusive Music New research uncovers atonal musics distinct emotional and neural effects by Eric W. Dolan May 30, 2024 in Music Photo credit: Adobe Stock Stay on top of the latest psychology findings: Subscribe now! A recent study published in the journal Behavioral Neuroscience has shed light on how atonal music affects our emotions and brain activity. The researchers found that atonal music, unlike the more familiar tonal music, tends to be perceived as less pleasurable and less familiar, evoking distinct neural responses. Previous studies have mostly focused on tonal music, which follows conventional harmonic structures familiar to Western listeners. However, there is a growing interest in understanding how atonal music compositions that do not follow these traditional structures affects our emotions and brain activity. Tonal music is built on a hierarchical system of pitches centered around a tonic note, creating a sense of resolution and familiarity through chord progressions and harmonic relationships. This structure is deeply ingrained in Western music, making it easy for listeners to predict and follow the musics flow, often leading to feelings of pleasure and emotional engagement. In contrast, atonal music breaks away from these conventional harmonic structures. It does not adhere to a single key or tonal center, and instead uses a more equal treatment of all twelve pitches in the octave. This can result in music that sounds unpredictable, dissonant, or unsettling to those accustomed to tonal harmony. As a musicologist and neuroscientist, Im very interested in the study of the brain processes underlying music cognition, particularly music-induced emotions, said study author Pablo Valdés-Alemán of the Centro de Investigación Transdisciplinar en Psicología Center for Transdisciplinary Research in Psychology in Mexico. In this case, our approach was to base our study on a previous model of brain electrical activity, measured with electroencephalography EEG, where frontal asymmetries are associated with certain types of emotions. This model had been studied before with various emotional stimuli, including music. Nonetheless, we wanted to test a particular type of music atonal music, which is less familiar to the average Western listener and also less studied in this field. The study involved 25 Mexican participants, a mix of men and women aged around 38 years, who were not musicians and had no hearing impairments. Participants were all right-handed, as left-handed individuals might process emotions differently due to variations in brain hemispheric specialization. The participants listened to 16 different musical pieces eight tonal and eight atonal. These pieces were chosen to evoke either joy or sadness, aligning with Russells model of emotional dimensions that categorize emotions based on valence positive or negative and arousal high or low. Each piece lasted about 25 seconds and was played through speakers while participants were seated comfortably in a quiet, isolated room. To measure the brains electrical activity, researchers used electroencephalography EEG, which involves placing electrodes on the scalp. This method offers excellent temporal resolution, allowing researchers to track brain activity in real-time. The electrodes recorded activity in various brain regions, focusing on the frontal and parietal areas, which are crucial for processing emotions. Participants also answered questions about their emotional reactions to each piece, rating the music on scales of valence, arousal, pleasure, and familiarity. The researchers found clear differences in how tonal and atonal music were perceived and processed by the brain. Tonal music was generally rated as more positive and pleasurable compared to atonal music. Joyful pieces of tonal music were particularly highly rated in terms of valence positivity and arousal excitement. In contrast, atonal music was perceived as less familiar and less pleasurable. Interestingly, despite being perceived as negative in terms of valence and arousal, sad music whether tonal or atonal could still be rated as pleasurable by some participants. This finding aligns with previous research suggesting that people can find pleasure in sad music, highlighting the complex relationship between emotional valence and pleasure. The EEG data revealed that listening to joyful music was associated with increased activity in the left frontal brain regions, a pattern consistent with positive emotional responses. This is known as frontal alpha asymmetry, where greater left frontal activation correlates with positive emotions. Atonal music, however, was linked to increased right frontal brain activity, which is often associated with negative emotions and arousal states. Music is a powerful emotional stimulus, and it can impact and modulate our brain dynamics, as seen with EEG activity, Valdés-Alemán told PsyPost. In this case, music-induced emotions can change frontal brain activity in an asymmetrical manner. Music that induces pleasant and positive emotions is associated with increased left frontal activity. This is interesting because affective disorders, like depression, which are associated with recurrent negative affect, are linked to left frontal hypoactivation. In some cases, alternative treatments may include non-invasive brain stimulation of this area. The fact that music alone may stimulate this cortical region adds to the evidence that music listening has a positive effect on mental health. The findings highlighted a stark contrast between how participants perceived and neurologically processed tonal and atonal music Specifically, tonal music, which is classical music inspired by the European tradition, is perceived as more familiar and pleasurable than its atonal counterpart, Valdés-Alemán explained. Atonal music includes music from other cultures with different musical systems or classical music that has intentionally removed tonality to challenge the traditional tonal system. This difference is accompanied by changes in frontal EEG asymmetries, as mentioned before. In that sense, another conclusion is that familiar music tends to be more enjoyable, evoking positive emotions and possibly providing mental health benefits and modulation of underlying brain activity associated with emotional processing. Familiarity may be culture-specific, but also influenced by individual differences such as personality, music education, and past experiences. While the study provides valuable insights, it has some limitations. The sample size was relatively small and limited to non-musicians from a specific cultural background. Future research should include larger, more diverse populations to generalize the findings. Additionally, only a limited number of brain regions were monitored, and other brainwave frequencies were not explored, which could provide a more comprehensive understanding of musical emotion processing. The major caveat of this research was that EEG recordings were conducted while we were still under COVID-19 confinement, Valdés-Alemán noted. As you can imagine, we had a limited sample and, in general, everyone was under an emotional burden, which might have biased the emotional assessment of music during this period. I would say that the next step for this research is to study musics emotional effects on EEG activity for people living with some type of affective disorder, like depression. Nevertheless, the findings provide evidence that our brains response to music is influenced by both its emotional content and our familiarity with it. Remember that music can bring emotional comfort and has the power to positively influence our feelings, Valdés-Alemán added. If you are feeling down, listening to some of your favorite music might help uplift your mood. However, it is important to recognize that while music can be a helpful tool for emotional regulation, it is not a substitute for professional help. If you are experiencing persistent emotional distress or mental health issues, never hesitate to consult a mental health professional for advice and support. They can provide the necessary guidance and treatment to help you navigate your challenges effectively. The study, Brain Electrical Patterns Associated With Pleasure and Emotion Induced by Tonal and Atonal Music, was authored by Pablo Valdés-Alemán, Bernarda Téllez-Alanís, and Adriana Zamudio-Gurrola. Tweet Send Scan Share Send Pin2 Share Share Share Share Share RELATED MUSIC Can music heal emotional wounds? New research suggests it might JANUARY 21, 2025 Listening to music during memory recall can alter the emotional tone of memories, making them more positive or negative, by activating emotion- and memory-related brain regions. This discovery suggests potential therapeutic uses for reframing difficult memories. READ MORE MUSIC Your personality might determine if music helps or hinders your productivity DECEMBER 26, 2024 Whether music boosts productivity depends on your personality, task, and music type. Extroverts benefit more, simple tasks are easier with music, and calm tunes work best. READ MORE DEMENTIA Music-induced neuroplasticity: Implications for dementia treatment DECEMBER 1, 2024 Music therapy improves dementia care by reducing anxiety, agitation, and depression, enhancing mood, memory, and cognitive function. Familiar songs trigger emotional and physical responses, potentially strengthening neural connections and supporting overall wellbeing. READ MORE COGNITIVE SCIENCE Surprising precision: Nearly half of earworms match original pitch perfectly NOVEMBER 9, 2024 A recent study found that a large portion of peoples sung earworms matched the pitch of the original songs exactly, adding new evidence that our brains may retain detailed musical information in ways we never realized. READ MORE MUSIC Left-handed musicians appear to develop unique brain pathways for language processing OCTOBER 16, 2024 Left-handed musicians and non-musicians appear to develop atypical brain patterns for language through different pathways, with musicians showing enhanced interhemispheric connections and non-musicians having reduced intrahemispheric connectivity in language-related brain areas. READ MORE MUSIC Classical music enhances mood by triggering triple-time locking in the extended amygdala SEPTEMBER 15, 2024 Listening to classical music is associated with synchronized brain activity in regions involved in sound and emotion processing, with personal music preference linked to stronger neural alignment and potential improvements in depressive symptoms. READ MORE MEMORY Musical memory remains resilient in old age, even for unfamiliar tunes AUGUST 29, 2024 A new study finds that older adults retain the ability to recognize and remember new music, suggesting that musical memory remains resilient with age, even in complex, real-world settings like live concerts. READ MORE MUSIC Scientists observe a remarkable synchronization effect among classical music listeners AUGUST 24, 2024 Classical music concerts synchronize the heart rates, skin conductance, and breathing rates of audience members, creating a shared physiological experience. This synchronization varies with music type and is influenced by listeners' personality traits and focus on the music. READ MORE SUBSCRIBE Go Ad-Free! Click here to subscribe to PsyPost and support independent science journalism! STAY CONNECTED RECENT Fathers emotional awareness and testosterone linked to childrens prosocial behavior Is psychology getting race wrong? Harvard study reveals racial categories may not predict shared views on racism Parenting stress alters the link between attachment avoidance and sexual satisfaction in couples with children Gamers with lower social skills are more likely to make impulsive in-game purchases How brain connectivity differs in healthy aging and semantic dementia Large-scale neuroimaging study finds no evidence of atypical amygdala connectivity in autism Scientists uncover a new mechanical pathway linked to Alzheimers disease Loneliness skews partner perceptions, harming relationships and reinforcing isolation Contact usPrivacy policyTerms and Conditions Do not sell my information",
    "https://dl.acm.org/doi/fullHtml/10.1145/3461615.3485419": "Article Navigation When Emotions are Triggered by Single Musical Notes: Revealing the Underlying Factors of Auditory-Emotion Associations Patrick O'Toole, School of Computer Science and Information Technology, University College Cork, Ireland, patrick.otooleumail.ucc.ie Donald Glowinski, Department of Psychology and Educational Sciences, University of Geneva, Switzerland, donald.glowinskiunige.ch Ian Pitt, School of Computer Science and Information Technology, University College Cork, Ireland, ianpcs.ucc.ie Maurizio Mancini, Department of Computer Science, Sapienza University of Rome, Italy, m.mancinidi.uniroma1.it DOI: ICMI '21 Companion: Companion Publication of the 2021 International Conference on Multimodal Interaction, Montréal, QC, Canada, October 2021 Can emotion be experienced when the auditory sense is stimulated by a single musical note Q1, and do variables such as musical skills, age, and personality traits have an influence in auditory-emotion associations Q2? An experiment was conducted, in which 130 participants were asked to listen to single musical notes and rate their experienced emotional state. They also had to rate their musical proficiency, sound sensitivity, strongest learning style, and complete a reduced version of the Big-Five personality test BFI-10. Results regarding Q1 show a correlation between lower notes and sadness, and higher notes and joy, confirming previous auditory-emotion association research, while presenting new knowledge into how emotion associates with single musical notes. Results regarding Q2 show that musical proficiency low vs high, learning style aural vs physical, personality level of Conscientiousness had an effect on how participants emotionally experienced single musical notes. The results presented in this study will provide a starting point that can help develop a new auditory-visual framework that uses understandings on emotion, personality and other variables in the development of more personalised human-computer interfaces. This new framework can be used in applications that can help in learning to paint or play an instrument; promoting positive mental health, or exploring new forms of creative expression e.g., writing a song with a paint brush as the instrument or painting a picture with a piano as your brush. CCS Concepts: Human-centered computing Sound-based input output; Interaction design process and methods; Applied computingSound and music computing; Social and professional topics User characteristics; Keywords: auditory-emotion associations, multi-modal interactions, music, personality ACM Reference Format: Patrick O'Toole, Donald Glowinski, Ian Pitt, and Maurizio Mancini. 2021. When Emotions are Triggered by Single Musical Notes: Revealing the Underlying Factors of Auditory-Emotion Associations. In Companion Publication of the 2021 International Conference on Multimodal Interaction ICMI '21 Companion, October 1822, 2021, Montréal, QC, Canada. ACM, New York, NY, USA 8 Pages. 1 INTRODUCTION This paper presents an experiment to help gain a deeper understanding into associations between single musical notes and emotions, and what impact different individual variables, such as personality, musical experience and learning style can have on auditory-emotion associations. Emotion and personality have become popular areas of research in recent years, especially in relation to technology 12, 33. With a great amount of personal data being captured by people's devices, companies are using this data to understand their users and personalise tools and products to each user. Multi-modal interaction is an area that can benefit from adopting more personalised approaches to how we form associations between our senses 18, 30. Interesting insights from a study, that trained participants in sound-colour associations using an ad-hoc program, while also investigating the association between sound and emotion of participants, were found. The fore-mentioned study found a correlation between very basic emotions sadness and joy and basic auditory stimuli lower and higher pitch, but not at a significant level and with a low testing sample 31. The motivation for our paper is to explore the understandings of associations between single musical notes and emotion, and if individual variables can play an important part in the process of auditory-emotion associations. With this new understanding, an improved model of auditory-visual associations can be applied in designing human-computer interfaces, one that takes account of emotion as well as personality, age, gender, and other variables. Figure 1 highlights the research of single musical notes-emotion associations, that is presented in this paper, as part of a larger framework of auditory-emotion-colour associations. This paper will help contribute to this larger framework of auditory-emotion-visual associations and contribute to future designs of intelligent and personalised human-computer interfaces, that can harness the power of multi-modal associations in creative and artistic digital environments. Figure 1: The work presented in the paper is highlighted by the dotted line: while cross-modal melody-emotion-colour and melody-colour associations are widely studied 20, 29, 32, 36, 37, 38, 39, 40, 42, we focus on single note-emotion associations by also looking at the individual variables in Section 2.2, in the larger framework of studying sound-emotion-colour associations. 2 BACKGROUND 2.1 Auditory-Emotion Research Previous research into associations between auditory stimuli and emotion has been mostly situated in auditory-visual cross-modal association research, that mainly explores the role of emotion in this relationship, mostly using musical excerpts instead of single musical notes 4, 6, 21, 28, 37, 46. Palmer et al. present the emotional meditation hypothesis, which is understood to be, when people listen to music, they have emotional responses and pick colors with similar emotional content 32. The study from Palmer et al., and other studies have provided interesting insights into how auditory stimuli is perceived emotionally in relation to colors, with music in a major scale and at a faster tempo to be perceived lighter in colour and happier, with the opposite being true for a minor scale 32, 39, 43. Other studies have focused solely on the auditory-emotion associations, however, these studies like the ones mentioned above focus on musical excerpts and not basic auditory stimuli 27, 40. Very few studies have been conducted on the association between basic auditory stimuli, like single musical notes, and emotion 2, 26, 37. Like Palmer et al., Spence also supports emotion mediation as one of the key factors in cross-modal correspondences, not just for complex auditory stimuli but also more simple stimuli 37. Comparing simple auditory stimuli to more complex stimuli shows that emotion mediation counts for less variance in the empirical matching data, and therefore elicits less pronounced perceived emotion 37. If we can understand auditory-emotion associations in a similar way to auditory-visual associations, that are mediated by emotion, lower notes darker colours should associate with negative emotions and higher notes lighter colours with positive emotions 29, 36. Studies on the impact of different emotion models has been carried out in research around auditory-emotion associations, with three emotion models highlighted Basicdiscrete, dimensional and musically-induced 40, 45. While the musically-induced emotion model would seem appropriate for a study on single musical note-emotion associations, the comparative test of these models and also information on personality bias help formulate the best approach for the experiment presented in this paper. The study by Zentner et al., showed that, compared to the other two models, the basic emotion model performed rather well when rating perceived emotions, even if the musically-induced model was more consistent 45. In another study, the dimensional model performed better than both the basic and musically-induced emotion one in the discrimination of musical excerpts. However, Personality-related differences were the most pronounced in the case of the discrete emotion model 40. Three different studies on emotion words expressed when listening to music stimuli, show that happiness, sadness, anger and fear were among the top choices across all three studies 23. Taking into account the studies presented above, and bearing in mind that our study focuses on single musical notes rather than musical excerpts, and includes personality traits as an independent variable to be analysed, a basic emotion model joy, sadness, anger and fear is proposed. 2.2 Individual Variables In this paper, we also seek to understand how twelve individual variables Age, Gender, Musical Experience, Sound Sensitivity, Auditory-to-Other-Sense, Other-Sense-to-Auditory, Learning Style and the Big Five personality traits BFI of Openness, Conscientiousness, Extraversion, Agreeableness and Neuroticism, discussed in this section, can impact on the associations between single musical notes and emotion. 2.2.1 Personality traits. The relationship between personality traits and emotional responses to music is well researched. Studies have found significant correlations between the traits NeuroticismExtraversion and positivenegative emotions 3, 8, 22. ChamorroPremuzic Furnham found that personality and intelligence can be a factor on how people use music. A person with a high IQ and high Extraversion might use music in a rational cognitive way, while someone with high Neuroticism and low Conscientiousness might use music for emotional regulation 8. In another study, Neuroticism was found to significantly map to all negative emotions, while Extraversion significantly mapped to the positive emotions, interest and enjoyment as well as the negative emotion shyness 22. From a review on present literature on the correlation between basic emotions and personality traits, limited findings regarding the traits of Openness to Experience, Conscientiousness and Agreeableness were found 15. The Big Five personality test is a widely adopted test used in studies relating to personality traits. It is simple and easy to interpret, and is also an effective way to understand the personality traits of an individual. Rammstedt and John conducted research into the accuracy of the BFI-10 test using only 10 questions and found it sufficient to use in a research settings with limited time constraints, compared to the BFI-44 test using 44 questions 34. 2.2.2 Musical vs Non-Musical Experience. A study by Manno et al., investigated how musicians and non-musicians identify emotion in music. Music uses Temporal Fine Structures TFS and Envelope ENV modulation to resolve emotion in music, however the exact contributions of TFS and ENV is not known. The study showed that TFS is essential in identifying emotion in music and that there is a difference in how much TFS is used by musicians and non-musicians. Non-musicians use less fine structure information and have reduced emotional resolvability curves compared to musicians 27. While the study above shows that musical experience can have a difference in how much TFS is used in identifying emotion in music, it is unclear if the same logic can be adopted when identifying emotion from single musical notes. A study on expressive intentions with single piano notes, shows that musical expertise has no mean effect on performers when tested on the four acoustical parameters of pitch, intensity, articulation and rhythmic density, with both musicians and non-musicians using these acoustic parameters in very similar ways when listening to a single piano note 2. 2.2.3 Age and Gender. Studies show that age can have an impact on how we associate between our senses and how we use emotion in response to stimuli, and that it can change over time 21, 35, 44. Hunter et al., using two variants of an emotional Stroop task, found that older adults can match congruent cross-modal stimuli just as well as younger adults however, with incongruent stimuli, the older adults performed worse than their younger counterparts 21. Older adults tend to weaken the ability to perceive negative emotions the older they get 35, this can be caused by the fact they tend to remove problematic relationships and avoid interpersonal conflict, leading to a more positive social environment 1, 5. As well as age, studies into the role of gender in auditory-emotion associations research has shown some interesting insights. Studies between men and women with regards to both uni and multi sensory emotional stimuli have shown that women are more accurate than men in recognition of emotional prosody 10, 24, 41. 2.2.4 Other Variables. Sound sensitivity, in general and when sound is triggered as the primary and secondary source, are also investigated in this paper. Misophonia is a condition characterized by heightened emotional reactivity to common repetitive sounds, accompanied by difficulties responding to these sounds and associated impairment in functioning 7. A study on the relationship between the Neuroticism trait and Misophonia, with regard to the role of emotion regulation found that difficulties with emotion regulation and Neuroticism were significantly positively correlated with symptoms of misophonia 7. Past research in auditory-visual cross-modal associations and chromaesthesia have also informed our understanding of the way in which perception operates between auditoryother sense and other senseauditory with regard to sound sensitivity 11, 26, 36. Seven learning-style terms from Gardner's Theory of Multiple Intelligences Aural, Visual, Logical, Verbal, Physical, Social Interpersonal and Solitary Intrapersonal have been used to understand the different ways people learn and take in information 16. While the method used in our study to understand learning styles relies on the participants perception of the strongest learning style, and not assessed with a well-known method, like the BFI test for personality traits, some interesting insights that can inform future work is expected. 3 EXPERIMENT This experiment looks to understand associations between single musical notes and basic emotions, and how other independent variables impact these associations, with two specific questions in mind: Q1 - Can emotion be experienced from a single musical note and if so, how basic a level is the emotional perception that is experienced? Q2 - Is there a significant correlation between auditory-emotion choices when factoring in the twelve independent variables mentioned in Section 2.2? 3.1 Participants 130 people female 95, male 34, other 1 participated in the online survey, with participants identifying with the following age groups: 35-44 38, 24-34 32, 45-55 29, 55-64 14, 18-24 9, over 65 6 and under 18 2. Information was also gathered on participants musical experience 82 with musical experience and 18 with no experience and creativeness 61, regularly partaking in a creative hobby; 24, not so regularly; and 15, with no creative hobby. While musical experience was used as one of the individual variables that could impact the emotion response to auditory stimuli, the questions regarding musical instruments played by participants and their level of creative expression was used to gather an extra layer of understanding to the results of the experiment. 3.2 Sound and Emotion Stimuli In this experiment, the auditory stimuli were recorded on the Logic Pro X software using a Fender Telecaster electric guitar as the instrument. Each note was recorded on a mono track with each note lasting 6 seconds with a smooth short fade at the end of each clip. A basic EQ, compressor and room reverb are used to obtain a more natural sound, with each sound file rendered to an mp3 file with the normalised function turned on. The envelope of the recorded notes consist of a sharp attack to the maximum amplitude, with a steady decay as the note rings out. As this was an online survey, we had no control of the absolute volume participants listened to the stimuli, but asked them to set it to a comfortable listening level. The first octave of the guitar was chosen to elicit the auditory stimuli, from note E2 82.41Hz to note D3 155.6Hz. Final Cut Pro was used to create short video files with a white background with black text counting down from three, with audio stimuli playing on zero. These video files provided better functionality in Limesurvey 1, than audio only files and also added a visual instruction to participants. For the emotion stimuli, the four emotion words, sadness, anger, fear and joy were used. These emotions are taken from the discrete emotion model mentioned in Section 2.1. 3.3 Procedure The online survey was distributed through the University College Cork survey mailing list, as well as being shared on LinkedIn and Twitter. Once participants clicked on the survey link, they landed on the welcome page and were presented with a detailed explanation of the survey, as well as contact information of the researchers, if they had any questions regarding the survey and could click Next to agree to start the survey. The survey was broken up into four sections: General Information, Personality Test, Cross-modal Association Questions and Auditory-emotion Questions. The survey is described in following four sections. 3.3.1 General Questions Section. Each section was displayed on a separate page with participants instructed to click the Next button to move to the next section. The first section contained five questions regarding age, gender, musical ability, instruments played this question only appeared if the participant had stated they had musical experience in the previous question and creative hobbies. 3.3.2 Personality Test Section. This section involved the participants taking the short Big Five personality test BFI-10. Participants responded to ten questions on a five point Likert scale ranging from Strongly Disagree to Strongly Agree. These questions were randomised to avoid any bias in the survey. After the ten questions were answered, participants could consent to share their email address to get their BFI-10 scores sent to them at a later time. If consent was given, the participant could input their email address on the next page. 3.3.3 Cross-modal Association Section. The third section presented four questions, with the first three designed to gain an understanding of the participants relationship with sound, and how other senses impact and are impacted by auditory stimuli. The fourth question looked to find what learning style participants associated with their style of learning. The participants rated the first three questions, between 1 weak and 5 strong in sensitivitysensation. The first question asked participants to rate their sensitivity to noisesound; the next two questions asked participants to rate the sensation strength in other senses when the auditory sense is the primary trigger i.e., when a bell is rung and triggers sensation, and in the auditory sense when the other sense is the primary trigger i.e., when a flash of light triggers sensation. For the last question participants were given the options of Visual, Logical, Aural, Verbal, Physical, Social Interpersonal and Solitary Interpersonal learning styles from Section 2.2.4, and asked to choose their top three, in order of perceived strongest to weakest. 3.3.4 Auditory-emotion Questions Section. The final section contained twelve questions presented in randomised order, to avoid selection bias. Each question contained a nine second video, which instructed participants to press play when ready. The video presented a count down from three with the auditory stimuli playing on zero, giving the participant time to fully listen to the note played. As mentioned in Section 3.2, the first octave of the guitar was used for the auditory stimuli. They were informed to listen once and to choose an emotion word from the options presented Sadness, Joy, Anger, Fear. Figure 2: This figure shows the percentage between the four emotion words that participants chose for each musical note. Sadness significantly associated with the lower notes, with Joy significantly associating with the higher notes. Once the twelve sound-emotion questions were answered, they were directed to submit the survey and a conclusion page was displayed thanking participants for taking part. 3.4 Data Handling and Analysis The survey data was stored within the Limesurvey application running from the researcher's hosting site. All data was anonymised and information on IP addresses, date stamp and referrer URLs were not collected. Email addresses were collected only from the participants who consented to leave it for the purpose of sharing the results of the personality test. The email data was deleted once the results were obtained and emailed to the participants. To answer Q1, a chi-square test was conducted on the cross-table Note by Emotion. The chi-square test was significant 2 33 n 1560 274.39, p .001, with a small effect size Cramer's V .24. Table 1: Results of the chi-square test that was conducted on the cross-table Note-by-Emotion. The chi-square test was significant 2 33 n 1560 274.39, p .001, with a small effect size Cramer's V .24. The significant ARSs are highlighted in bold. Emotion Note Sadness Anger Fear Joy D Count 21a 16a, b 31b 62c Adj. Res. -5.586 -0.593 0.217 6.664 D Count 25a 26b 31b 48b Adj. Res. -4.835 2.044 0.217 3.654 C Count 39a 8a 30a, b 53b Adj. Res. -2.206 -2.703 0.000 4.729 C Count 29a 13a 29a 59b Adj. Res. -4.084 -1.384 -0.217 6.019 B Count 48a 13a 26a 43a Adj. Res. -0.516 -1.384 -0.870 2.580 A Count 46a 20a 32a 32a Adj. Res. -0.892 0.461 0.435 0.215 A Count 52a 25a 32a 21a Adj. Res. 0.235 1.780 0.435 -2.150 G Count 75a 12b 27a, b 16b Adj. Res. 4.554 -1.648 -0.652 -3.224 G Count 73a 20a, b 23b, c 14c Adj. Res. 4.178 0.461 -1.522 -3.654 F Count 69a 19a 36a 6b Adj. Res. 3.427 0.198 1.305 -5.374 F Count 74a 14a, b 32a 10b Adj. Res. 4.366 -1.121 0.435 -4.514 E Count 58a 33a 31a 8b Adj. Res. 1.361 3.890 0.217 -4.944 To answer Q2, a multinomial stepwise multiple regression model was carried out in which the dependent variable was the single musical notes. Using the variables obtained from the data in the survey as models, Age, Gender, Musical Experience, Sound Sensitivity, Auditory-to-Other-Sense, Other-Sense-to-Auditory, Learning Style and the five personality traits of Openness, Conscientiousness, Extra-version, Agreeableness and Neuroticism were analysed. Table 2: Result of multinomial stepwise multiple regression model. Only the significant predictors are reported. The reference emotion is always Joy. Estimate Std. Error Wald Sig. Odds ratio 95 Conf. Int. L Bound U Bound Note: C Anger Male 1.946 0.811 5.762 .016 7.000 1.429 34.286 Fear Male 1.302 0.508 6.573 .010 3.675 1.359 9.940 Note: C Anger Aural 1.946 0.886 4.818 .028 7.001 1.232 39.766 Anger Physical 1.946 0.976 3.976 .046 7.001 1.034 47.418 Note: A Sadness Proficiency -1.034 0.324 10.186 .001 .355 .188 .671 Anger Proficiency -0.825 0.365 5.102 .024 .438 .214 .897 Anger Conscientiousness -0.051 0.024 4.550 .033 .950 .906 .996 Fear Proficiency -1.042 0.353 8.728 .003 .353 .177 .704 Note: G Anger age -0.817 0.360 5.146 .023 .442 .218 .895 Fear age -0.853 0.307 7.746 .005 .426 .234 .777 Note: G Anger age -1.095 0.345 10.084 .001 .335 .170 .658 Note: E Anger sensitivesound 0.836 0.373 5.022 .025 2.308 1.111 4.795 3.5 Results 3.5.1 Q1 - Auditory-emotion association. For Q1, the results in Table 1 show that very basic emotion can be experienced from basic auditory stimuli, like a single musical note. For four out of five of the lowest notes E,F,F,G,G, Table 1 shows that the adjusted standardised residual ASR scores are significantly higher for Sadness. The one exception was the lowest note E where Anger has a higher ASR score but not at a significant level; however, Sadness was chosen by most participants, but not at a significant level. For the higher notes, results show that the four highest notes C,C,D,D are significantly higher for Joy in the ASR scores. Table 1 also highlights that only the emotions of Joy and Sadness found significance, either positively or negatively in ASR scores, that could suggest very basic emotion associations to single musical notes. Results show that for notes A,A, and B, which are placed in the middle of the octave of notes tested, no significant statistics are shown. 3.5.2 Q2 - Impact of Independent Variables on Sound-emotion Association. Results pertaining to Q2 highlighted some interesting insights. Q2 looked at how the individual variables, mentioned in Section 2.2, can affect the emotion choice for each single musical note. Analysis was performed on each note, and Table 2 shows significant results at p .05 where the individual variables had an impact on the association between single musical notes and emotion. As shown in Table 2, six out of the twelve musical notes showed statistical significance in relation to one or more predictors. The odds ratio column in Table 2 represents the relative likelihood of each predictor type yielding a choice for a particular emotion. For example, Table 2 shows that a person with a higher sensitivity to sound is 2.308 times more likely to choose Anger than Joy the emotion used as a reference in relation to the E note. For individual variables with an odds ratio of less than 1, the predictor is less likely to choose the stated emotion than the referenced emotion Joy, i.e., participants with musical proficiency are less likely to experience Sadness, Anger and Fear compared to Joy. 3.6 Discussion 3.6.1 Q1 - Sound-emotion Association. Similar to the findings of O'Toole et al., associations between basic auditory stimuli and emotion can be viewed at a very basic level of emotional perception 31. The results from the chi square test, illustrated in Table 1, show a significant association between Lower-notesSadness and Higher-notesJoy. In Section 2.1, research on different emotion models were presented, with the basicdiscrete model considered the most appropriate for an experiment using single musical notes and for perceiving rather than feeling emotions 45. For future research, adopting a multi-modal approach that can understand multiple features from auditory stimuli would be beneficial. However for this study on basic auditory stimuli, results show a basic emotion choice for single musical notes, validating the use of the basic emotion model in this experiment. With the three notes A, A, B in the middle of the octave that showed no significance, all three notes had Sadness as the most chosen emotion. A Study by Chau et al. has shown that the timbre of an instrument can impact the emotion response when hearing a sound. Results showed that the guitar rated higher for negative emotional characteristics compared to a piano which was emotionally neutral and instruments like a vibraphone or marimba which was emotionally positive 9. Findings from another study found that timbre independently affects the perception of emotions in music after controlling for other acoustic, cognitive and performance factors 19 While timbre is one of many acoustic properties that can impact the emotion response to sound, if we are analysing single musical notes that lack tempo, rhythm and modes, it could have a bigger impact on how a person responds to sound emotionally. To answer Q1, emotion can be experienced from basic auditory stimuli, such as a single musical note at a very basic level of emotion perception. Without melody, harmony, rhythm, modes and other audio features a single note can be perceived to be mostly happy or sad and this is mostly dependant on pitch. 3.6.2 Q2 - Independent Variables Impact on Sound-emotion Association. Q2 investigated how different individual variables can impact a participants emotion word choices. In Table 2, significant scores in seven out of twelve individual variables across six of the twelve musical notes were found. Conscientiousness provided a significant negative correlation with Anger on the note A. This result would fit the personality trait. Previous studies have shown that the Conscientiousness trait is positively associated with the optimistic-conventional dimension in relation to musical preference 14. Studies have shown that a person high in Conscientiousness is likely to; like high tempo music in a major key rather than low tempo in the minor key 14; use music in a more cognitive rather than an emotional way 8; and usually can regulate or control impulses better 13. Table 2 shows that musically proficient participants significantly chose Joy over Sadness, Anger and Fear when listening to the note A. This correlation could be explained by the fact that the A note is the standard tuning note on the guitar. 34 participants were musically proficient, with guitar as their choice instrument. Hearing this particular note could have contributed to elicit positive emotions, and it could be possible that other guitar players have an overall more positive emotion perception when hearing sounds from a guitar. It could be also similar to both the optimistic Joy and the conventional standard tuning note dimensions for the Conscientiousness trait mentioned above. While both musicians and non-musicians can have an understanding of communicating expressive content, as presented in Section 2.2.2, musicians use a greater amount of temporal fine structure TFS information and a higher emotional resolvability curve 27. This could impact how musicians respond to musical notes in different ways to non-musicians and why musical proficiency correlates to Joy with regard to the A note. Table 2 shows interesting correlations between the Note C and the Aural and Physical learning styles. Participants who chose Aural or Physical as their preferred learning style were around seven times more likely to chose Anger. Studies have shown that, when learning an instrument, aural learning was the main learning style, but participants also used more logical and practical approaches when learning new songs 17. For aural learners, with musical experience 87.5 of aural learners, this correlation between the C note and the Aural learning style could be due to frustration of not having the option to use more logical and practical learning styles to help them understand the musical note being elicited. Looking at the positive correlation between Anger and the Physical learning style for the note C, some interesting insights can be seen. The C note is the fourth highest frequency tested in this study and while overall we can see from Table 1 that Joy is the significant emotion, higher frequencies can also garner annoyance in some people. A study measuring noise annoyance levels in working environments has shown that higher frequencies had the highest annoyance rating with the lowest frequencies having the lowest annoyance rating 25. It is possible that the Anger associated with the Physical, as well as the Aural learning styles, comes from the higher frequency of the C note. 4 CONCLUSIONS In this paper, we presented an experiment on auditory-emotion associations, looking to reveal the emotional potential of an apparently-basic stimulus such as a single musical note and the impact of other variables such as, personality, gender, age and musical experience on this association. Through this approach, we were able to more clearly distinguish the combination of key factors accounting for associations between auditory stimuli and emotion. Results showed significance on basic associations in lower notes to sadness and higher notes to joy, with a positive correlation between the four highest notes and the emotion joy, and a positive correlation between three of the four lowest notes and the emotion, sadness. Results pertaining to the impact of individual variables on auditory-emotion associations showed significant results on six of the twelve musical notes. The independent variables of gender, aural and physical learning styles, musical proficiency, the Conscientiousness personality trait, age and sound sensitivity were found to be significant variables in six of the twelve single musical notes, thus potentially impacting the emotional response to the auditory stimuli. Some limitations of this experiment include, for example, the use of only one octave and the choice of instrument for auditory stimuli. One octave was used in this experiment, to reproduce the test setup from 31, with a larger test group. In the future, it would be beneficial to study auditory-emotion associations in a range of octaves, to understand if pitch height-emotion associations are the most dominant associations regarding single musical notes. Also, the use of a single instrument type is an important factor in associations between single musical notes and emotions. Each instrument has a different timbre which elicits different emotion characteristics. As mentioned in Section 3.6.1, it has been shown that the timbre of the guitar is highly rated for negative emotional characteristics, where a Piano is rated as neutral, and a vibraphone is highly rated for positive emotional characteristics 9. Changing the instrument that elicits auditory stimuli, by adopting a piano neutral emotional characteristics or using multiple instrument sounds, with different emotional characteristics is important for future studies and will help understand single musical note-emotion associations and the impact of timbre and multiple octaves have on the emotion response. The findings of the experiment presented in the paper can help towards the formulation of a new auditory-visual framework that uses understandings on emotion, personality and other variables in the development of more personalised human-computer interfaces. These findings can improve the design of programs such as SoundStrokes presented in 31, that can assist in sharpening our associations between our senses. Further research into the aforementioned auditory-visual framework could lead to the creation of applications using computer-generated cross-modal stimuli, e.g., synthesised audio-visual tutorials, to learn or improve skills. These applications could be used for a multitude of purposes, such as learning an instrument or to paint; and in exploring new creative ways of expression, e.g., writing a song with a paint brush as the instrument or painting a picture with a piano as your bush. Creativity should not be limited to one form of expression and understanding the intricacies of our senses, combined with emotion, personality traits and other variables, it can provided a platform for multi-modal expression using digital tools. ACKNOWLEDGMENTS This publication has emanated from research supported in part by a Grant from Science Foundation Ireland under Grant number 18CRT6222. REFERENCES Toni Antonucci, Hiroko Akiyama, and Keiko Takahashi. 2004. Attachment and Close Relationships across the Life Span. Attachment Human Development 6, 4 Dec. 2004, 353370. Navigate to citation 1 Filippo Bonini Baraldi, Giovanni De Poli, and Antonio Rodà. 2006. Communicating Expressive Intentions with a Single Piano Note. Journal of New Music Research 35, 3 Sept. 2006, 197210. Navigate to citation 1 citation 2 Lisa Feldman Barrett. 1998. Discrete Emotions or Dimensions? The Role of Valence Focus and Arousal Focus. Cognition Emotion 12, 4 July 1998, 579599. Navigate to citation 1 Thomas Baumgartner, Michaela Esslen, and Lutz Jäncke. 2006. From Emotion Perception to Emotion Experience: Emotions Evoked by Pictures and Classical Music. International Journal of Psychophysiology 60, 1 April 2006, 3443. Navigate to citation 1 Kira S. Birditt and Karen L. Fingerman. 2005. Do We Get Better at Picking Our Battles? Age Group Differences in Descriptions of Behavioral Reactions to Interpersonal Tensions. The Journals of Gerontology Series B: Psychological Sciences and Social Sciences 60, 3 May 2005, P121P128. Navigate to citation 1 Roberto Bresin. 2004. Real-Time Visualization of Musical Expression.. In HUMAINE Workshop From Signals to Signs of Emotion and Vice Versa. Santorini, 5. Navigate to citation 1 Clair Cassiello-Robbins, Deepika Anand, Kibby McMahon, Rachel Guetta, Jacqueline Trumbull, Lisalynn Kelley, and M. Zachary Rosenthal. 2020. The Mediating Role of Emotion Regulation Within the Relationship Between Neuroticism and Misophonia: A Preliminary Investigation. Frontiers in Psychiatry 11 Aug. 2020, 847. Navigate to citation 1 citation 2 Tomas Chamorro-Premuzic and Adrian Furnham. 2007. Personality and Music: Can Traits Explain How People Use Music in Everyday Life?British Journal of Psychology 98, 2 2007, 175185. Navigate to citation 1 citation 2 citation 3 Chuck-Jee Chau, Bin Wu, and Andrew Horner. 2015. The Emotional Characteristics and Timbre of Nonsustaining Instrument Sounds. Journal of the Audio Engineering Society 63, 4 April 2015, 228244. Navigate to citation 1 citation 2 Oliver Collignon, Simon Girard, Frédéric Gosselin, Dave Saint-Amour, Franco Lepore, and Maryse Lassonde. 2010. Women Process Multisensory Emotion Expressions More Efficiently than Men. Neuropsychologia 48, 1 Jan. 2010, 220225. Navigate to citation 1 Caroline Curwen. 2018. Music-Colour Synaesthesia: Concept, Context and Qualia. Consciousness and Cognition 61 May 2018, 94106. Navigate to citation 1 Richard J Davidson, Klaus R Scherer, and H. Hill Goldsmith. 2009. Handbook of Affective Sciences. Oxford University Press, New York; Oxford. Navigate to citation 1 Colin G. DeYoung. 2010. Personality Neuroscience and the Biology of Traits: Personality Neuroscience. Social and Personality Psychology Compass 4, 12 Dec. 2010, 11651180. Navigate to citation 1 Snjeana Dobrota and Ina Rei Ercegovac. 2015. The Relationship between Music Preferences of Different Mode and Tempo and Personality Traits Implications for Music Pedagogy. Music Education Research 17, 2 April 2015, 234247. Navigate to citation 1 citation 2 Ryan Donovan, Aoife Johnson, Aine deRoiste, and Ruairi O'Reilly. 2020. Quantifying the Links Between Personality Sub-Traits and the Basic Emotions. In Computational Science and Its Applications ICCSA 2020, Osvaldo Gervasi, Beniamino Murgante, Sanjay Misra, Chiara Garau, Ivan Blei, David Taniar, Bernady O. Apduhan, Ana Maria A.C. Rocha, Eufemia Tarantino, Carmelo Maria Torre, and Yeliz KaracaEds.. Vol. 12250. Springer International Publishing, Cham, 521537. Navigate to citation 1 Howard Gardner. 1993. Frames of Mind: The Theory of Multiple Intelligences 2nd ed ed.. Fontana Press, London. Navigate to citation 1 Lucy Green. 2012. Musical Learning Styles and Learning Strategies in the Instrumental Lesson: Some Emergent Findings from a Pilot Study. Psychology of Music 40, 1 Jan. 2012, 4265. Navigate to citation 1 Annaliese Micallef Grimaud, Tuomas Eerola, and Nick Collins. 2019. EmoteControl: A System for Live-Manipulation of Emotional Cues in Music. In Proceedings of the 14th International Audio Mostly Conference: A Journey in Sound. ACM, Nottingham United Kingdom, 111115. Navigate to citation 1 Julia C. Hailstone, Rohani Omar, Susie M. D. Henley, Chris Frost, Michael G. Kenward, and Jason D. Warren. 2009. It's Not What You Play, It's How You Play It: Timbre Affects Perception of Emotion in Music. Quarterly Journal of Experimental Psychology 62, 11 Nov. 2009, 21412155. Navigate to citation 1 Kate Hevner. 1936. Experimental Studies of the Elements of Expression in Music. The American Journal of Psychology 48, 2 April 1936, 246. Navigate to citation 1 Edyta Monika Hunter, Louise H. Phillips, and Sarah E. MacPherson. 2010. Effects of Age on Cross-Modal Emotion Perception.Psychology and Aging 25, 4 Dec. 2010, 779787. Navigate to citation 1 citation 2 citation 3 Carroll E. Izard, Deborah Z. Libero, Priscilla Putnam, and O. Maurice Haynes. 1993. Stability of Emotion Experiences and Their Relations to Traits of Personality.Journal of Personality and Social Psychology 64, 51993, 847860. Navigate to citation 1 citation 2 Patrik N. Juslin. 2013. What Does Music Express? Basic Emotions and Beyond. Frontiers in Psychology 4 2013. Navigate to citation 1 Lena Lambrecht, Benjamin Kreifelts, and Dirk Wildgruber. 2014. Gender Differences in Emotion Recognition: Impact of Sensory Modality and Emotional Category. Cognition and Emotion 28, 3 April 2014, 452469. Navigate to citation 1 Ulf Landström, Elisabeth Åkerlund, Anders Kjellberg, and Maria Tesarz. 1995. Exposure Levels, Tonal Components, and Noise Annoyance in Working Environments. Environment International 21, 3 Jan. 1995, 265275. Navigate to citation 1 Lawrence E Marks. 2004. Cross-Modal Interactions in Speeded Classification. In The Handbook of Multisensory Processes, Gemma Calvert, Charles Spence, and Barry E. SteinEds.. MIT Press, Cambridge, Mass, 85105. Navigate to citation 1 citation 2 Francis A. M. Manno, Raul R. Cruces, Condon Lau, and Fernando A. Barrios. 2019. Uncertain Emotion Discrimination Differences Between Musicians and Non-Musicians Is Determined by Fine Structure Association: Hilbert Transform Psychophysics. Frontiers in Neuroscience 13 Sept. 2019. Navigate to citation 1 citation 2 citation 3 Solange Mardaga and Michel Hansenne. 2009. Do Personality Traits Modulate the Effect of Emotional Visual Stimuli on Auditory Information Processing?Journal of Individual Differences 30, 1 Jan. 2009, 2834. Navigate to citation 1 Gail Martino and Lawrence E Marks. 2000. Cross-Modal Interaction between Vision and Touch: The Role of Synesthetic Correspondence. Perception 29, 6 June 2000, 745754. Navigate to citation 1 citation 2 Vincenzo Moscato, Antonio Picariello, and Giancarlo Sperli. 2020. An Emotional Recommender System for Music. IEEE Intelligent Systems2020, 11. Navigate to citation 1 Patrick O'Toole, Donald Glowinski, and Maurizio Mancini. 2019. Understanding Chromaesthesia by Strengthening Auditory-Visual-Emotional Associations. In 2019 8th International Conference on Affective Computing and Intelligent Interaction ACII. IEEE, Cambridge, United Kingdom, 17. Navigate to citation 1 citation 2 citation 3 citation 4 Stephen E. Palmer, Karen B. Schloss, Zoe Xu, and Lilia R. Prado-Leon. 2013. Music-Color Associations Are Mediated by Emotion. Proceedings of the National Academy of Sciences 110, 22 May 2013, 88368841. Navigate to citation 1 citation 2 citation 3 Rosalind W Picard. 1997. Affective Computing. MIT Press, Cambridge, Mass. Navigate to citation 1 Beatrice Rammstedt and Oliver P. John. 2007. Measuring Personality in One Minute or Less: A 10-Item Short Version of the Big Five Inventory in English and German. Journal of Research in Personality 41, 1 Feb. 2007, 203212. Navigate to citation 1 Ted Ruffman, Julie D. Henry, Vicki Livingstone, and Louise H. Phillips. 2008. A Meta-Analytic Review of Emotion Recognition and Aging: Implications for Neuropsychological Models of Aging. Neuroscience Biobehavioral Reviews 32, 4 Jan. 2008, 863881. Navigate to citation 1 citation 2 Charles Spence. 2011. Crossmodal Correspondences: A Tutorial Review. Attention, Perception, Psychophysics 73, 4 May 2011, 971995. Navigate to citation 1 citation 2 citation 3 Charles Spence. 2020. Assessing the Role of Emotional Mediation in Explaining Crossmodal Correspondences Involving Musical Stimuli. Multisensory Research 33, 1 July 2020, 129. Navigate to citation 1 citation 2 citation 3 citation 4 citation 5 Xiuwen Sun, Xiaoling Li, Lingyu Ji, Feng Han, Huifen Wang, Yang Liu, Yao Chen, Zhiyuan Lou, and Zhuoyun Li. 2018. An Extended Research of Crossmodal Correspondence between Color and Sound in Psychology and Cognitive Ergonomics. PeerJ 6 March 2018, e4443. Navigate to citation 1 Tawney Tsang and Karen B. Schloss. 2010. Associations between Color and Music Are Mediated by Emotion and Influenced by Tempo: 525772013-006. Navigate to citation 1 citation 2 Jonna K. Vuoskoski and Tuomas Eerola. 2011. Measuring Music-Induced Emotion: A Comparison of Emotion Models, Personality Biases, and Intensity of Experiences. Musicae Scientiae 15, 2 July 2011, 159173. Navigate to citation 1 citation 2 citation 3 citation 4 Teija Waaramaa. 2017. Gender Differences in Identifying Emotions from Auditory and Visual Stimuli. Logopedics Phoniatrics Vocology 42, 4 Oct. 2017, 160166. Navigate to citation 1 J Ward, B Huckstep, and E Tsakanikos. 2006. Sound-Colour Synaesthesia: To What Extent Does It Use Cross-Modal Mechanisms Common to Us All?Cortex 42, 2 2006, 264280. Navigate to citation 1 Kelly L. Whiteford, Karen B. Schloss, Nathaniel E. Helwig, and Stephen E. Palmer. 2018. Color, Music, and Emotion: Bach to the Blues. i-Perception 9, 6 Nov. 2018, 204166951880853. Navigate to citation 1 Lee H. Wurm, Gisela Labouvie-Vief, Joanna Aycock, Kristine A. Rebucal, and Heather E. Koch. 2004. Performance in Auditory and Visual Emotional Stroop Tasks: A Comparison of Older and Younger Adults.Psychology and Aging 19, 3 2004, 523535. Navigate to citation 1 Marcel Zentner, Didier Grandjean, and Klaus R. Scherer. 2008. Emotions Evoked by the Sound of Music: Characterization, Classification, and Measurement.Emotion 8, 4 2008, 494521. Navigate to citation 1 citation 2 citation 3 Zhihong Zeng, M. Pantic, G.I. Roisman, and T.S. Huang. 2009. A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions. IEEE Transactions on Pattern Analysis and Machine Intelligence 31, 1 Jan. 2009, 3958. Navigate to citation 1 FOOTNOTE 1 This work is licensed under a Creative Commons Attribution International 4.0 License. ICMI '21 Companion, October 1822, 2021, Montréal, QC, Canada 2021 Copyright held by the ownerauthors. ACM ISBN 978-1-4503-8471-12110. DOI:",
    "https://www.unprofesor.com/musica/tipos-de-cadencia-musical-3912.html?utm_source=chatgpt.com#anchor_1": "DESCUBRE Buscar unPROFESOR Música Lenguaje musical Los sonidos Tipos de cadencia musical Los sonidos Tipos de cadencia musical Valoración: 4 4 votos 4 comentarios Por Ana Sofía Rivera. 21 enero 2020 Polifonia musical: características y... Qué es el contrapunto en música Una de las cualidades innegables en el arte es la capacidad de crear un discurso, un a serie de ideas que están entrelazadas entre sí para poder expresar un sentimiento concreto. Gracias a la estructura y las formas del arte como método de comunicación, podemos establecer una forma elegante de hacer llegar nuestro mensaje de la manera mas sensible posible. La música es uno de esos artes que nos permite hacer entender a nivel sentimental, esto en gran parte lo debemos a la forma tan refinada que tiene de llevarnos casi inconscientemente a la comprensión, de manera casi misteriosa por medio de la sucesión de sonidos. En esta lección de unPROFESOR hablaremos acerca de un componente de la música que nos permite lograr esto: los tipos de cadencia musical. También te puede interesar: Tipos de textura musical Índice Qué es una cadencia musical Los grados de la tonalidad en la música Los diferentes tipos de cadencia que existen Qué es una cadencia musical La música posee el elemento del tiempo, es decir que no es algo estático sino todo lo contrario, va cambiando mientras transcurre y es esto lo que le hace tener tanta vida. Otro factor crucial en la música es la dinámica que causa en cuanto a percepción de tensión y resolución, es decir que auditivamente tenemos la sensación de que cuando algo es muy denso, debe soltar esa tensión eventualmente. Es esta liberación lo que nos resulta tan placentero en la música y lo que hace que pueda transcurrir en el tiempo, tener ciclos, evolución y así crear un discurso. La palabra cadencia viene del italiano y significa caer. Con esta palabra nos referimos a la necesidad de resolución antes mencionada. La estructura de una obra o canción musical se construye por una serie de acordes en un orden determinado. Si bien este orden queda a la merced del compositor, no es una sucesión aleatoria, ya que se rige por las reglas armónicas que le dan sentido a la música. Toda progresión de acordes debe terminar eventualmente, desembocar en algún sitio y es precisamente a la acción de resolver a lo que le llamamos cadencia, es el momento y acorde en la música en la que un acorde cae. Imagen: Pinterest Los grados de la tonalidad en la música Antes de conocer los tipos de cadencia debemos tener muy claro el concepto de los grados de la tonalidad y por supuesto, de tonalidad en sí. En resumen, la tonalidad son las reglas que nos dictan el contexto armónico, las notas que podemos utilizar en una composición. Gracias a estas reglas logramos saber que tipos de acordes podemos utilizar y con esto, establecemos los grados de la tonalidad. Los grados de la tonalidad se encuentran en la escala base que estamos utilizando ejemplo: escala mayor, escala menor, escala dórica... etc. En una escala estándar tenemos 7 notas, y por lo tanto obtenemos 7 grados. Cada uno de los grados posee un nombre específico según su función, que esta definido por la cantidad de tensión o sensación de resolución. Estos son los nombres de los grados de tonalidad: I Primer grado: tónica fundamental II Segundo grado: supertónica III Tercer grado: mediante IV Cuarto grado: subdominante V Quinto grado: dominante VI Sexto grado: superdominante o submediante VII Séptimo grado: sensible El grado de resolución por excelencia es el primer grado, tónica o fundamental, ya que nos provee la mayor sensación de conclusión gracias a su estabilidad armónica. Imagen: Slideshare Los diferentes tipos de cadencia que existen Existen muchos tipos de cadencia ya que la música es un arte y depende de la creatividad. Sin embargo en el transcurso de la historia se han utilizado con frecuencia ciertas cadencias gracias a su conocida funcionalidad. Existen dos categorías para las cadencias: cadencias conclusivas y cadencias suspensivas. Las cadencias conclusivas son aquellas que alcanzan una resolución y por lo tanto una finalización clara. Por el contrario, una cadencia suspensiva es aquella que no brinda una resolución y que provoca la sensación de que el sonido debe continuar. Para ambos tipos de cadencia existen algunos tipos estándar. Cadencias conclusivas Cadencia perfecta: Va directamente de la dominante V a la tónica I. Cadencia imperfecta: Va de la dominantes V a la tónica I pero la organización de las notas se encuentra invertido o la voz más aguda no resuelve al primer grado. Cadencia plagal: Va de la subdominante IV a la tónica I. Cadencia compuesta: Sucesión subdominante IV - dominante V - tónica I. Cadencia preclásica: Sucesión subdominante IV - dominante V dominante octavada y tónica I. Cadencias suspensivas Semicadencia: Cuando se da un reposo en la subdominante IV o dominante V, pero no resuelve aún. Cadencia imperfecta: cuando en una cadencia perfecta el acorde de tónica se produce en un tiempo débil o si alguno de sus acordes se encuentra invertido cuando la nota fundamental del acorde no es la más grave. Cadencia rota o deceptiva: cuando una cadencia da la sensación de que resolverá a la tónica pero en cambio ese acorde es sustituido por un grado no resolutivo. Con estos conocimientos de la cadencias ya tienes un acercamiento para las bases de la música y de su composición. La música se compone de muchas relaciones que debemos ir estudiando poco a poco, para poder expresarnos en el arte con propiedad. Imagen: Musicnet Si deseas leer más artículos parecidos a Tipos de cadencia musical, te recomendamos que entres en nuestra categoría de Lenguaje musical. Polifonia musical: características y... Qué es el contrapunto en música Categorías relacionadas Historia de la MúsicaInstrumentos musicales Lo más visto 1. Cuáles son las notas musicales en el pentagrama 2. Etapas de la música 3. Nombres de los instrumentos de cuerda 4. Principales elementos de la música 5. Notas musicales: símbolos y nombres 6. Figuras musicales y su duración 7. Instrumentos de viento madera 8. Qué son las claves musicales Más lecciones de Los sonidos Lección 5 de 14 Qué es la forma musical y sus clasificaciones Lección 6 de 14 Estructura de la forma sonata Lección 7 de 14 Tipos de melodías y sus características Lección 8 de 14 Polifonia musical: características y ejemplos Lección 9 de 14 Tipos de cadencia musical Lección 10 de 14 Qué es el contrapunto en música Lección 11 de 14 Contrapunto musical: ejemplos Lección 12 de 14 Canon musical: definición y ejemplos Quiero ver más lecciones! Pregunta al profesor sobre Tipos de cadencia musical Tipos de cadencia musical Redes sociales Aprender matemáticas Aprender lengua española Aprender ciencias sociales Aprender ciencias naturales Aprender música Aprender física Aprender química Aprender inglés Aprender tecnología e informática Consejos para estudiar unprofesor.com 2025 Quiénes somos Contacta con nosotros Términos y Condiciones Política de privacidad Política de cookies Compartir en:",
    "https://eldiaadiariomusica.wordpress.com/2013/06/23/sentido-y-personalidad-de-las-tonalidades/": "Música que siento Un rincón literario Artículos Montserrat Figueras y el Cant de la Sibilla Tonalidad de Si bemol menor Sentido y personalidad de las Tonalidades Publicado el 23 de junio de 2013 por Manel Artero Hubo un tiempo en el que se aceptaba otorgar una cierta personalidad a cada una de las tonalidades que pueden construirse con el sistema de afinación temperada, establecido desde el barroco y hasta la ruptura que supuso la nueva Armonía que estableció Arnold Schöenberg. Ese concepto de personalidad debió tener su apogeo en el Romanticismo, decayendo a finales del siglo XIX y principios del XX. No hay nadie, al menos yo no lo he encontrado, que haya demostrado en la práctica que esa convención es cierta. A pesar de ello sí que podemos encontrarnos con tonalidades que nos transmites distintas sensaciones. Pero también es cierto que eso puede atribuirse a la calidad de la melodía de los temas, a su orquestación, a su armonización e incluso al modo de utilizar la mezcla de timbres de los distintos instrumentos. A pesar de ello comparto con vosotros una tabla con lo que se petende que transmite cada tonalidad. A partir de ésta entrada intentaré publicar una obra que esté escrita en cada una de ellas para que podamos juzgarlo entre todos. Tonalidades Mayores Tonalidad Personalidad Do mayor Alegre, guerrero, completamente puro. Su carácter es de inocencia y de simplicidad. Do sostenido mayor Miradas lascivas. Pena y éxtasis. No puede reír, pero puede sonreír. No puede aullar, sólo puede hacer una mueca de su llanto. Caracteres y sentimientos inusuales. Re mayor Feliz y muy guerrero. El triunfo, Aleluyas, júbilo, victoria. Mi bemol mayor Crueldad, dureza, amor, devoción, conversación íntima con Dios. Mi mayor Querellante, chillón, gritos ruidosos de alegría, placer al reírse. Fa mayor Furioso y arrebatado. Fa sostenido mayor Triunfo sobre la dificultad, libertad, alivio, superación de obstáculos, el eco de un alma que ferozmente ha lidiado y finalmente conquistó. Sol mayor Dulcemente jovial, idílico, lírico, calmado, pasión satisfecha, gratitud por la amistad verdadera y el amor esperanzado, emociones gentiles y pacíficas. La bemol mayor Gravedad, muerte y putrefacción. La mayor Alegre, campestre, declaración de amor inocente, satisfacción, la esperanza de volver lo que le pertenece a uno de nuevo al regresar de una partida, juventud, aplausos y creencia en Dios. Si bemol mayor Magnífico, alegría, amor alegre, conciencia limpia, metas y deseos por un mundo mejor. Si mayor Duro, doliente, deslumbrante, fuertemente coloreado, anunciando pasiones salvajes, enfado, odios y resentimientos. Tonalidades menores Tonalidad Personalidad Do menor Oscuro y triste. Declaración de amor y a la vez lamento de un amor no correspondido. Anhelos y suspiros. Do sostenido menor Sentimientos de ansiedad, angustia y dolor profundo en el alma, desesperación, depresión, sentimientos sombríos, miedos, indecisiones, escalofríos. Si los fantasmas hablaran se aproximarían a esta tonalidad. Re menor Grave y devoto. Melancolía femenina. El rencor. Mi bemol menor Horrible, espantoso. Mi menor Afeminado, amoroso, melancólico. Fa menor Oscuro, doliente, depresivo, lamento funerario, gemidos de miseria, nostalgia solemne. Fa sostenido menor Pesimista, triste, sombrío, oscuro, terco a la pasión, resentimientos, descontentos. Sol menor Serio, magnífico, descontento, preocupado por el rompimiento de los esquemas, mal templado, rechinamiento de dientes, disgusto. La bemolmenor Quejándose todo el tiempo, incomplaciente, insatisfecho, corazón sofocado, lamentos, dificultades. La menor Tierno, lloroso, piedad femenina. Si bemol menor Oscuro, terrible, criatura pintoresca y curiosa, ropa de noche, tosco, maleducado, burlesco, descortés, descontento con sí mismo, sonidos del suicidio. Si menor Solitario, melancólico, ermitaño, paciencia, fe y sumisión esperando el perdón divino. Tu voto: 60 10 Rate This Comparte esto: Haz clic para compartir en Twitter Se abre en una ventana nueva Haz clic para compartir en Facebook Se abre en una ventana nueva Haz clic para compartir en Tumblr Se abre en una ventana nueva Haz clic para compartir en Pinterest Se abre en una ventana nueva Haz clic para compartir en LinkedIn Se abre en una ventana nueva Haz clic para compartir en WhatsApp Se abre en una ventana nueva Haz clic para enviar un enlace por correo electrónico a un amigo Se abre en una ventana nueva Haz clic para imprimir Se abre en una ventana nueva Cargando... Relacionado De OM a Amén, el sentido de la Música 7 de agosto de 2019 En La palabra entre el Arte Una interpretación del preludio en Do M del Clave bien temperado 18 de agosto de 2023 En Bach Sentido de las tonalidades. Hoy: Do Mayor 11 de octubre de 2014 En Barroco Acerca de Manel Artero Manel Artero, nacido en Barcelona, en el barrio de Poble Sec, dedicó gran parte de su vida a la informática, compaginando con ella su amor por la lectura y por la música. De esta última cursó un grado de Historia. Más tarde haría los tres cursos de narrativa y novela de lEscola descriptura de lAteneu barcelonès que le abriría las puertas al mundo de la escritura del que siempre formó parte sin saberlo. Desde entonces ganado diversos premios en concursos de relatos. El más sobresaliente, el de la Asociación El coloquio de los perros de Córdoba. Compagina su tiempo entre la escritura y diversos talleres y charlas sobre música, lectura y cultura de paz, que imparte en Cerdanyola del Vallès. El ladrón de rostros es su primera novela. Editada originalmente en 2017 por la editorial Maluma y6 reeditada por su hijo, Roger Artero, en 2023. Ver todas las entradas por Manel Artero Esta entrada fue publicada en Tonalidad y etiquetada afinación, personalidad, tonalidad. Guarda el enlace permanente. Montserrat Figueras y el Cant de la Sibilla Tonalidad de Si bemol menor 26 respuestas a Sentido y personalidad de las Tonalidades Pingback: Tonalidad de Si bemol menor Pingback: Sentido de las tonalidades. Hoy: Do Mayor Música que siento Roberto Reffray dijo: 30 de marzo de 2016 en 16:48 Hola. Estoy haciendo mi tesis de pregrado de la traducción del Dichterliebe de Schumann para ser interpretado. Ahora, me pareció muy interesante la relación que existe entre tonalidad y personalidad. Puedes decirme la fuente, por favor? Gracias. Responder Agustí dijo: 23 de octubre de 2023 en 15:04 Hola, existe un libro de un talChristian Friedrich Daniel Schubart que hizo un estudio sobre el tema allá por el año 1806 y que se llama deen zu einer Ästhetik der Tonkunst, que viene a decir Ideas para una estética del arte musical. Sólo lo he visto en catalán bajo el título Idees per a una estètica de lart musical pero aún no he conseguido encontrarlo en las librerias. Si alguien tiene más suerte que yo ya lo comentareis. Saludos a tods. Responder Manel Artero dijo: 25 de octubre de 2023 en 5:44 Muchísimas gracias por tu aportación. Recibe un cordial saludo. Roberto Reffray dijo: 30 de marzo de 2016 en 16:59 Hola. Estoy haciendo una tesis de pregrado sobre la traducción del Dichterliebe de Schumann, para ser interpretada al español. Me parece muy interesante la relación que existe entre las tonalidades y la personalidad que cada armadura transmite, podrías decirme la fuente, por favor? Gracias. Responder Manel Artero dijo: 30 de marzo de 2016 en 22:30 Hola Roberto, Lo cierto es que la fuente no es demasiado rebuscada. Copié directamente la tabla que ofrece la Wikipedia: Pero creo recordar que la comparé con algún otro lugar y eran muy parecidas. No pensé en contrastarla con ninguno de los libros que tengo de música. No sé si te habrá servido. Un saludo Responder Rafael dijo: 3 de octubre de 2019 en 0:52 Hola. Me identifico un poco con como las veo yo. No venía buscando algo científico u objetivo sino la experiencia de una persona. Me interesa conocer lo que se dice de las tonalidades para ver si coincidimos en sensaciones las personas que les buscamos un significado a las tonalidades. Añadiría,bajo mi punto de vista: Re Mayor: el cielo, visto como algo espiritual. Mi bemol Mayor: calor maternal, una madre. Mi Mayor: Humanidad. Unión humana. Fa Mayor: Juventud, pasión aventurera. La persona joven que se lanza a hacer algo con mucho corazón pero poco conocimiento por no haber vivido suficiente. Amor romántico. El no importar qué pasará mañana. Sol Mayor: el mar, profundidad, misterio, paz, inspiración, arte, romanticismo en el sentido artístico La Mayor: aire, viento, algo ligero, claro en sentido de color y despreocupado, libertad. Si bemol Mayor: la tonalidad usual de los brindis. Celebración. Si Mayor: intranquilidad. Re menor: el destino. La fuerza natural de la realidad. Mi menor: melancolía, nostalgia, el llanto que sana. Profundidad. Fuerza. Sol menor: rabia, batalla. La menor: delicadeza, canción de amor, lamento. Responder Manel Artero dijo: 3 de octubre de 2019 en 12:34 Hola Rafael, ante todo gracias por tu comentario. En mi caso, todo y que vivo para el sonido desde bastante joven, no tengo una percepción tan afinada como la tuya. Imagino que son niveles de sensibilidad o de interés. En cambio me identifico con los timbres y las disonancias. Ellos sí me transportan a otro estado de percepción. Te felicito por tanta sensibilidad. Recibe un cordial saludo. Responder María Hornos Miller dijo: 16 de abril de 2020 en 3:57 El Preludio de Rigoletto, de Verdi, está escrito en la tonalidad de do menor y es tal como tú lo describes: tragedia, desesperación, certeza de que todo se hará pedazos. Gracias por tu artículo. Responder Manel Artero dijo: 16 de abril de 2020 en 12:43 Gracias a ti por pasar por el blog y leerlo. Saludos cordiales. Responder Pingback: Download Mp3 COMO ESCRIBIR UNA CANCIÓN. PARTE 1. LOS MODOS Songs carlos dijo: 31 de octubre de 2020 en 15:05 Muy interesante, llevo ya cuatro años trasteando con FL Studio que no solo sirve para hacer trap y bastantes sintetizadores virtuales y aprendiendo un poco sobre música, escalas y demás; sé que no alcanzaré nunca alguien que la ha estudiado de verdad pero componer aunque sea modestamente me llena de verdad. Ahora gracias a este Post estoy experimentando con los 24 modos entre mayor y menor de una manera consciente y tengo que decir que sí que suenan esos matices que se apuntan y sabiéndolo, trato de buscar esos adjetivos y el resultado es más potente. Estaría bien un post con otros modos tipo Frigio, eólico e incluso de otras culturas como la india que tienen matices diferentes si se trasladan a la sonoridad occidental un saludo. Responder carlos dijo: 31 de octubre de 2020 en 15:09 Muy interesante, llevo ya cuatro años trasteando con FL Studio y bastantes sintetizadores virtuales y aprendiendo un poco sobre música, escalas y demás; sé que no alcanzaré nunca alguien que la ha estudiado de verdad pero componer aunque sea modestamente me llena de verdad. Ahora gracias a este Post estoy experimentando con los 24 modos entre mayor y menor de una manera consciente y tengo que decir que sí que suenan esos matices que se apuntan y sabiéndolo, trato de buscar esos adjetivos y el resultado es más potente. Ahora gracias a este Post estoy experimentando con los 24 modos entre mayor y menor de una manera consciente y tengo que decir que sí que suenan esos matices que se apuntan y sabiéndolo, trato de buscar esos adjetivos y el resultado es más potente. Estaría bien un post con otros modos tipo Frigio, eólico e incluso de otras culturas como la india que tienen matices diferentes si se trasladan a la sonoridad occidental un saludo. Responder Manel Artero dijo: 31 de octubre de 2020 en 17:29 Hola Carlos, Ante todo gracias por tu aportación. Sí que me encantaría entrar en la complejidad de la música india. Su división de la octava es más compleja, sus ritmos también Pero me falta tiempo. Me es imposible dedicarle al blog todo el tiempo que desearía. No obstante tomo nota. En cuanto a las escalas y modos griegos puedo asegurarte que en Youtube hay muchísmas cosas. Un saludo. Responder Manel Artero dijo: 31 de octubre de 2020 en 17:31 Olvidé decirte que me alegra muchísimo que el Post te haya servido para experimentar con los modos. Responder carlosgecheverria dijo: 31 de octubre de 2020 en 19:30 Sí, muchas gracias. Aún sigo con ello. Saludos Estaba de paso dijo: 31 de octubre de 2020 en 22:03 No tengo mucha idea de música, pero me parece una manera muy buena de aprender música con la ayuda de las emociones. Es importante darle un sentido al solfeo.Muy interesante gracias! Responder Manel Artero dijo: 31 de octubre de 2020 en 22:32 Gracias a ti por pasarte a leerme. Un saludo. Responder Elide dijo: 7 de junio de 2021 en 20:43 Tonalidad de Fa M, totalmente queda con el Va Pensiero! de Nabbuco de Verdi Responder Paula dijo: 22 de marzo de 2023 en 10:44 Buenos días, Me preguntaba cuál es la personalidad de re bemol mayor. No aparece en tu tabla ni en wikipedia, me parece extraño. Atentamente, Responder Manel Artero dijo: 3 de abril de 2023 en 13:51 Hola Paula, encantado de encontrarte por aquí. Preguntas por una de las tonalidades que no pude encontrar en su momento. No sé si puede servirte, pero buscando he encontrado esto: Antes de finales del siglo XIX, se afinaba con temperamentos distintos o sea, cada tonalidad sonaba ligeramente distinta. Por lo tanto, cada tonalidad tenía asociada unas cualidades. Re bemol mayor, la que trajo tu comentario, era la una tonalidad lasciva que degenera en pena y arrebato. Hector Berlioz llamó a esta tonalidad majestuosa en su Gran Tratado de Instrumentación5 mientas que a su tonalidad enarmónica, do sostenido mayor, la definió como menos imprecisa que do mayor y más elegante que esta. Espero que te sirva. Recibe un cordial saludo. Responder Vanessa dijo: 31 de marzo de 2023 en 18:01 Hola a todos, alguien sabe las emociones que emite Sol sostenido menor? Muchas gracias. Responder Manel Artero dijo: 1 de abril de 2023 en 23:40 Hola Vanessa, he buscado pero no la encuentro. De todos modos, su equivalente sonoro sería la de La bemol menor ya que Sol sostenido es la nota enarmónica suena igual que el La bemol. Espero que pueda servirte. Recibe un cordial saludo. Responder Noa Ayra Yishay dijo: 17 de julio de 2024 en 9:53 Hola! Tiene algun nombre en concreto esta teoria?? Gracias Responder Manel Artero dijo: 18 de julio de 2024 en 10:54 Hola, Noa, Ante todo decirte que las definiciones y asociaciones que se hacen con ellas son meramente subjetivas. Dicho esto, se las conoce generalmente como características afectivas de las tonalidades o afectos de las tonalidades. Aunque no tiene un nombre específico universalmente aceptado. Recibe un cordial saludo. Responder Deja un comentario Ejemplo de audio del libro Reproductor de audio 00:00 00:00 Utiliza las teclas de flecha arribaabajo para aumentar o disminuir el volumen. Ir a descargar Y a escuchar todo el Podcast Suscríbete al Blog Enter your email address to follow this blog and receive notifications of new posts by email. Suscripción Únete a otros 269 suscriptores Anécdotas Ars Antiqua Bach banda sonora Barroco Belleza canción concierto De Otros Grecia Historia de la música jazz Jordi Savall king Crimson mozart Novela Pasion perfeccion poesía Robert Fripp Rock Progresivo sencillez sensibilidad sensualidad Sinfonía Tonalidad tristeza trovadores Uncategorized wagner Archivos Elegir el mes enero 2025 septiembre 2024 junio 2024 noviembre 2023 octubre 2023 agosto 2023 julio 2023 May 2023 abril 2023 marzo 2022 noviembre 2021 marzo 2021 febrero 2021 octubre 2020 septiembre 2020 junio 2020 May 2020 abril 2020 noviembre 2019 agosto 2019 julio 2019 junio 2019 abril 2019 marzo 2019 enero 2019 diciembre 2018 noviembre 2018 agosto 2018 abril 2018 marzo 2018 febrero 2018 enero 2018 diciembre 2017 noviembre 2017 octubre 2017 septiembre 2017 agosto 2017 julio 2017 junio 2017 abril 2017 marzo 2017 diciembre 2016 noviembre 2016 octubre 2016 julio 2016 junio 2016 May 2016 abril 2016 marzo 2016 febrero 2016 enero 2016 diciembre 2015 noviembre 2015 octubre 2015 agosto 2015 junio 2015 May 2015 abril 2015 marzo 2015 febrero 2015 enero 2015 diciembre 2014 noviembre 2014 octubre 2014 agosto 2014 junio 2014 May 2014 abril 2014 marzo 2014 febrero 2014 diciembre 2013 noviembre 2013 octubre 2013 septiembre 2013 agosto 2013 julio 2013 junio 2013 May 2013 abril 2013 marzo 2013 enero 2013 diciembre 2012 noviembre 2012 octubre 2012 septiembre 2012 agosto 2012 julio 2012 junio 2012 Blogroll Ancha es mi casa Aula de Música Azul, el principio fue azul Bach tras Bach Bachiano Belleza sin palabras Camino de música Conciertos en el Delibes EL cavaller del Cigne Generador de Frecuencias sonoras Historia de la Música La mejor guitarra La vuelta al mundo en 80 músicas Música antigua música con nocturnidad y alevosía Música y literatura en clave personal Melomanía y otros estados sensoriales Radio Kras Tarab Al Andalus Tono menor, un blog de clásica Voces para la Paz El Día a Diario Música by Manel Artero Badenes is licensed under a Creative Commons Reconocimiento-SinObraDerivada 3.0 Unported License. Permissions beyond the scope of this license may be available at Meta Registro Iniciar sesión Feed de entradas Feed de comentarios WordPress.com Enlaces RSS RSS - Comentarios Comptador functioni,s,o,g,r,a,mi'GoogleAnalyticsObject'r;irirfunction ir.qir.q.pusharguments,ir.l1new Date;as.createElemento, ms.getElementsByTagNameo0;a.async1;a.srcg;m.parentNode.insertBeforea,m window,document,'script',' ga'create', 'UA-50423519-1', 'wordpress.com'; ga'send', 'pageview'; está Dísticas? 122.579 hits document.writeunescape3Cscript src27 type27textjavascript273E3Cscript3E; try Histats.start1,1967963,4,107,170,20,00001010; Histats.framedpage; Histats.trackhits; catcherr; Música que siento Web construida con WordPress.com. Comentar Rebloguear Suscribirse Privacidad Diseña un sitio como este con WordPress.com Comenzar"
}