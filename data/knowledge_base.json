{
    "https://www.kennedy-center.org/education/resources-for-educators/classroom-resources/media-and-interactives/media/music/your-brain-on-music/your-brain-on-music/your-brain-on-music-tearjerkers/": "www.kennedy-center.org\nVerificar que usted es un ser humano. Esto podría tardar algunos segundos.\nwww.kennedy-center.org necesita revisar la seguridad de su conexión antes de continuar.\nRay ID: 90bd396a8ee9f76c\nRendimiento y seguridad de Cloudflare",
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3779798/": "An official website of the United States government\nHere's how you know\n\n\nSearch\nLog in\nSearch in PMC\nAdvanced Search\nJournal List\nUser Guide\nAs a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with, the contents by NLM or the National Institutes of Health.\nLearn more: PMC Disclaimer | PMC Copyright Notice\nFront Psychol. 2013 Sep 23;4:656. doi: 10.3389/fpsyg.2013.00656\nPreattentive processing of emotional musical tones: a multidimensional scaling and ERP study\nKatja N Spreckelmeyer\n1, Eckart Altenmüller\n2, Hans Colonius\n3, Thomas F Münte\n4,*\nAuthor information\nArticle notes\nCopyright and License information\nPMCID: PMC3779798  PMID: 24065950\nAbstract\nMusical emotion can be conveyed by subtle variations in timbre. Here, we investigated whether the brain is capable to discriminate tones differing in emotional expression by recording event-related potentials (ERPs) in an oddball paradigm under preattentive listening conditions. First, using multidimensional Fechnerian scaling, pairs of violin tones played with a happy or sad intonation were rated same or different by a group of non-musicians. Three happy and three sad tones were selected for the ERP experiment. The Fechnerian distances between tones within an emotion were in the same range as the distances between tones of different emotions. In two conditions, either 3 happy and 1 sad or 3 sad and 1 happy tone were presented in pseudo-random order. A mismatch negativity for the emotional deviant was observed, indicating that in spite of considerable perceptual differences between the three equiprobable tones of the standard emotion, a template was formed based on timbral cues against which the emotional deviant was compared. Based on Juslin's assumption of redundant code usage, we propose that tones were grouped together, because they were identified as belonging to one emotional category based on different emotion-specific cues. These results indicate that the brain forms an emotional memory trace at a preattentive level and thus, extends previous investigations in which emotional deviance was confounded with physical dissimilarity. Differences between sad and happy tones were observed which might be due to the fact that the happy emotion is mostly communicated by suprasegmental features.\nKeywords: preattentive processing, musical emotion, timbre, event-related potential, mismatch negativity, multidimensional scaling\nIntroduction\nMusic, as well as language, can be used to transport emotional information and, from an evolutionary perspective, it does not come as a surprise that the way emotion is encoded in music is similar to the encoding of emotion in human or animal vocalizations. Interestingly, the emotional and semantic processing of speech has been shown to be supported by different brain systems by the method of double dissociation (e.g., Heilman et al., 1975). While six patients with right temporoparietal lesions and left unilateral neglect were demonstrated to have a deficit in the comprehension of affective speech, six patients with left temporoparietal lesions exhibited fluent aphasia, i.e., problems with the content of speech, but no problems with affective processing. Likewise, in music processing the Montreal group around Isabelle Peretz has described a patient that is selectively impaired in the deciphering of emotions from music while being unimpaired for the processing of other aspects of music (Peretz et al., 2001).\nResearchers have tried to identify segmental and suprasegmental features that are used to encode emotional information in human speech, animal vocalizations, and music. With regard to animals, similar acoustic features are used by different species to communicate emotions (Owings and Morton, 1998). In humans, perceived emotion appears to be mainly driven by the mean level and the range of the fundamental frequency (F0) (Williams and Stevens, 1972; Scherer, 1988; Sloboda, 1990; Pihan et al., 2000) with low F0 being related to sadness and, conversely, high mean F0 level being related to happiness. In music, Hevner (1935, 1936, 1937) in her classical studies found that tempo and mode had the largest effects on listeners' judgments, followed by pitch level, harmony, and rhythm. According to Juslin (2001) musical features encoding sadness include slow mean tempo, legato articulation, small articulation variability, low sound level, dull timbre, large timing variations, soft duration contrasts, slow tone attacks, flat micro-intonation, slow vibrato, and final ritardando, whereas happiness is encoded by fast mean tempo, small tempo variability, staccato articulation, large articulation variability, fairly high sound level, little sound level variability, bright timbre, fast tone attacks, small timing variations, sharp duration contrasts, and rising micro-intonation.\nWhile suprasegmental features are thought to be, at least in part, the result of a lifelong sociocultural conventionalization and therefore, maybe less hardwired (Sloboda, 1990), a considerable part of the emotional information is transmitted by segmental features concerning individual tones. For example, a single violin tone might be recognized as sad or happy with a rather high accuracy. Indeed, string and wind instruments which afford a high degree of control over the intonation can be used to mimic the segmental features also used by singers to convey emotional information.\nSegmental emotional information can be encoded into a single tone by varying its timbre, which might be defined as reflecting the different quality of sounds aside from variations in pitch, loudness, and duration. In addition to different distributions of amplitudes of the harmonic components of a complex tone in a steady state (Helmholtz, 1885/1954), dynamic variations of the sound such as attack time and spectral flux (Grey, 1977; Grey and Moorer, 1977) are also important, particularly with regard to onset characteristics. Multidimensional scaling procedures on tones differing in timbre, because they were produced by different by different musical instruments, showed that this aspect of a tone is determined by variations along three dimensions termed attack time, spectral centroid, and spectral flux (McAdams et al., 1995). Likewise, in a recent study using multidimensional scaling (MDS) procedures to investigate the emotional information transmitted by variations in timbre, Eerola et al. (2012) found that affect dimensions could be explained in terms of three kinds of acoustic features: spectral (= ratio of high-frequency to low-frequency energy), temporal (= attack slope), and spectro-temporal (= spectral flux).\nFrom the discussion above, there is no question as to the importance of detection of emotional timbre in voice and—by extension—in music. The question that we ask here pertains to when in the auditory processing stream emotional timbre is differentially processed. Given the high evolutionary benefit that might be afforded by the rapid decoding of emotional information from single tones (or human calls), we hypothesize that such information might be processed “early” in the processing stream and in an automatic fashion. Indeed, there are a number of studies that have investigated rapid and preattentive classification of emotional sounds. In particular, our group presented normal non-musician participants with tone series comprising a frequent (standard) single violin tone played with a certain emotional connotation (happy or sad) and a rare (deviant) violin tone played with the “opposite” intonation (Goydke et al., 2004). In parallel to the tone series, the EEG was recorded with a focus on the mismatch negativity (MMN). The MMN has been shown to be an ideal tool to address the early, automatic stages of sound evaluation (Näätänen, 1992; Picton et al., 2000; Näätänen et al., 2001). It is a component of the auditory event related potential (ERP) which is elicited during passive listening by an infrequent change in a repetitive series of sounds. In the original incarnation of the MMN paradigm, it occurs in response to any stimulus which is physically deviant (in frequency, duration or intensity) to the standard tone. Importantly, the standard stimulus in typical MMN experiments is the same throughout the experiment. It has been shown, however, that the MMN can also be obtained to deviations within complex series of sounds (Picton et al., 2000; Näätänen et al., 2001), in which the memory trace is defined by some abstract property (e.g., ascending series of tones). Thus, it appears that the notion of a standard/memory trace can be extended such that the auditory system is capable to extract systematic properties of sound series. Moreover, and important for Goydke et al. (2004) and the present study, the MMN is sensitive to changes in the spectral component of tonal timbre (Tervaniemi et al., 1997). The onset latency of the MMN varies according to the nature of the stimulus deviance. Whereas simple, physically deviant stimuli show an onset latency of the MMN of about 150 ms, much later MMNs have been seen with more complex forms of deviance. Finally, it is important to stress the fact that the analysis of the incoming stimulus as well as its encoding appears to take place automatically since the MMN typically occurs when the subjects do not attend to the eliciting stimuli, for example during engagement in a different task such as reading a book (Näätänen, 1992). Returning to the Goydke et al. (2004) study, deviant tones were associated with an MMN. The MMN scalp topography for the emotional deviant was similar to an MMN for a control pitch deviant tone. These results were taken to indicate that the brain can categorize tones preattentively on the basis of subtle cues related to the emotional status of the tone (Goydke et al., 2004). Studies using a similar logic using both emotionally voiced words (Schröder et al., 2006) or vocalizations (Bostanov and Kotchoubey, 2004) have revealed analogous findings. Further, investigating different timbral dimensions (attack time, spectral centroid, and spectrum fine structure) and their consequences for behavioral classification latencies and ERPs in preattentive (Caclin et al., 2006) and attentive (Caclin et al., 2008) listening conditions, Caclin and colleagues showed that these different timbral features are separately represented in sensory auditory memory.\nOne important aspect has been neglected by these studies, however, in the Goydke et al. (2004) study, a single (e.g., happy) tone was presented repeatedly as a standard and a single (e.g., sad) tone was presented repeatedly as the emotional deviant. Thus, it is possible, that the MMN observed for the deviants in this study might have been driven by the physical differences between the standard and deviant stimuli rather than by the postulated preattentive emotional categorization of the stimulus. Indeed, different mechanisms of deviance detection (termed sensory and cognitive) have been demonstrated for other types of stimulus materials (Schröger and Wolff, 1996; Jääskeläinen et al., 2004; Opitz et al., 2005).\nTherefore, to answer this question and extend our previous findings (Goydke et al., 2004), we conducted the present study. As pointed out before, segmental features encoding emotion seem to be varied. Thus, what makes the study of acoustical emotion difficult is, that the set of features encoding the same emotion does not seem to be very well defined and that there is a great variance of feature combinations found within individual emotion categories. We modified the design of our previous MMN study to see whether affective expressions are pre-attentively categorized even when their acoustical structure differs. In other words, several (n = 3, probability of occurrence for each tone 25%) instances of sad (or happy) tones were defined as standards to which an equally probable deviant stimulus (25%) of the other emotion had to be compared preattentively. To the extent that the MMN reflects deviance in the sense of “being rare,” an MMN under these circumstances would indicate that the standards have been grouped to define a single “emotional” entity.\nTo test whether the brain automatically builds up categories of basic emotions across tones of different (psycho)-acoustical structure, it was necessary to create two sets of tones, where tones within one set could clearly be categorized as happy and sad, respectively but differed with respect to their acoustical structure. To this end, we first performed extensive studies to define the stimulus set for the MMN study using MDS methods. Two types of criteria were set for tones to be used as standards in the MMN study: first, each tone needed to be consistently categorized as happy or sad and, second, tones within one set as well as across sets needed to be perceived as different. The first point was addressed by performing affect-ratings on a set of violin tones which only differed in emotional expression but not in pitch or instrumental timbre. To tackle point 2, pairwise same-different-comparisons were collected for all tones and fed into a Fechnerian scaling procedure to assess the perceived similarity among the tones. We will first describe the scaling experiment and will then turn to the MMN experiment.\nFor the latter, we had a straightforward expectation: If the brain categorizes tones preattentively on the basis of an automatic emotional grouping, we should observe an MMN for emotional deviant stimuli regardless of the fact that these emotional deviants were as probable as each of the three different standard stimuli.\nScaling experiment\nMultidimensional Fechnerian scaling (Dzhafarov and Colonius, 1999, 2001) is a tool for studying the perceptual relationship among stimuli. The general aim of MDS is to arrange a set of stimuli in a low-dimensional (typically Euclidean) space such that the distances among the stimuli represent their subjective (dis)similarity as perceived by a group of judges. Judges generally perform their ratings in pairwise comparisons between all stimuli in question. Based on the dissimilarity data a MDS procedure finds the best fitting spatial constellation by use of a function minimization algorithm that evaluates different configurations with the goal of maximizing the goodness-of-fit (Kruskal, 1964a,b). Though the dimensions found to span the scaling space can often be interpreted as psychologically meaningful attributes that underlie the judgment, no a priori assumptions have to be made about the nature of the dimensions. Thus, with MDS perceptual similarity can be studied without the need to introduce predefined feature concepts (as labels for the dimensions) which might bias people's judgments.\nFechnerian scaling is a development of classical MDS which is more suitable to be used with psychophysical data. Dzhafarov and Colonius (2006) have pointed out that certain requirements for data to be used with classical MDS are usually violated in empirical data, namely the property of symmetry and the property of constant self-dissimilarity. The property of symmetry assumes that discrimination probability is independent of presentation order, and, thus, that the probability to judge a stimulus x as different from a stimulus y is the same no matter whether x or y is presented first [p(x; y) = p(y; x)]. It has been known since Fechner (1860) that this is not true. The property of constant self-dissimilarity expects that any given stimulus is never perceived as different from itself, thus, that the probability to judge stimulus x as different from itself is 0 [p(x; x) = p(y; y)]. However, it has been shown repeatedly that this is not the case in psychophysical data (e.g., Rothkopf, 1957). The only requirement made by Fechnerian scaling is that of regular minimality, requesting that the probability to judge a stimulus as different from itself needs to be lower than any other discrimination probability.\nIn the present experiment Fechnerian scaling is used to establish subjective distances for a set of tones where tones differ with respect to their emotional expression.\nMaterials and methods\nStimulus material\nTo generate the stimulus material, 9 female violinists (all students of the Hanover University for Music and Drama) were asked to play brief melodic phrases all ending on c-sharp. Melodies were to be played several times with happy, neutral, or sad expressions. Before each musician started with a new expression, she was shown a sequence of pictures from the IAPS (Lang et al., 2008) which depicted happy, neutral or sad scenes, to give her an idea of what was meant by happy, neutral, and sad. All violinists were recorded on the same day in the same room using the same recording technique: stereo (2 Neumann-microphones TLM127), 44.1 kHz sampling rate, 24 bit, distance from the instrument to the microphones was always 50 cm. Each musician filled out a form describing the changes in technique that she had applied to achieve the different expressions. From 200 melodic phrases the last tone (always c-sharp) was extracted using Adobe Audition. Only those tones were selected which were between 1450 and 1700 ms in length and had a pitch between 550 and 570 Hz. Tones from two violinists had to be discarded altogether because they were consistently below pitch level. The resulting pre-selection comprised 35 tones by 7 different violinists. To soften the tone onset a smooth fade-in envelope was created from 0 to 100 ms post-tone onset. The pre-selection was rated on a 5-point scale from very sad (1) to very happy (5) by 9 student subjects (mean age = 25.9 years, 5 males) naive to the purpose of the study and different from the participants taking part in the final experiment. Each tone was rated twice by each participant to test the raters' consistency. Tones were not amplitude-normalized, because it was found that differences in affective expression could not be differentiated properly in a normalized version. Based on the affect ratings and their consistency 10 tones were selected for the final stimulus set (Table 1).\nTable 1.\nFeatures of the stimulus material.\nTone Duration (ms) Frequency (Hz), (SD) Mean level [dB(A)]\ntone01 1676 559.69 (2.41) 64.5\ntone02 1526 558.99 (2.04) 66.2\ntone03 1658 559.98 (4.45) 72.2\ntone04 1628 554.39 (3.55) 71.6\ntone05 1506 555.86 (1.13) 68.8\ntone06 1534 561.86 (4.35) 68.5\ntone07 1660 563.00 (4.58) 66.6\ntone08 1630 561.31 (3.61) 67.8\ntone09 1570 556.96 (1.25) 72.4\ntone10 1608 557.64 (0.35) 68.8\nMean (SD) 1599 (61.5) 559.3 (2.75) 68.74 (2.66)\nOpen in a new tab\nDesign of the same-different forced-choice experiment\nParticipants were 10 students (mean age = 25.4 years, 5 females) with no musical expertise who took part in two separate sessions. In session 1 they performed a same-different forced-choice task on the violin tones to provide data for MDS. In session 2 (approximately 1 week later) they were asked to rate the emotional expression of the tones on a five-point-scale.\nFor the forced-choice task, participants were tested individually while sitting in a comfortable chair 120 cm away from a 20-zoll-computer screen. All auditory stimuli were presented via closed head-phones (Beyerdynamic DT 770 M) with a level ranging from 64 to 73 dB. Presentation software (Neurobehavioral Systems) was used to present trials and to record responses. All 10 tones were combined with each other including themselves, resulting in 10 × 10 = 100 pairs; all 100 pairs were presented ten times, each time in a different randomized order (resulting in 1000 trials altogether). The stimulus onset asynchrony (SOA) between the two tones of a pair was 3500 ms. Participants had to strike one of two keys to respond same or different (forced choice). To make sure participants judged the psychoacoustical similarity of the tones unbiased, they were kept uninformed on the purpose of the experiment. Trial duration was about 6000 ms. The next trial was automatically started when one of the two buttons was pressed. Participants performed a short training to familiarize them with the procedure and were allowed to pause after each block of 25 trials. There were 40 blocks altogether. Participants could end the pause by pressing a button on the keyboard. The duration of the whole experiment was about 2 hours. Participants were verbally instructed to decide whether the two tones comprising a pair were same or different. For the data analysis responses were recorded as 0 (same) and 1 (different). Mean values (discrimination probabilities) per pair of tones were calculated over all participants and all responses. Minimum number of responses per pair was 90. The resulting discrimination probabilities were transformed into Fechnerian distances using FSDOS (Fechnerian Analysis of Discrete Object Sets by Dzhafarov and Colonius, see http://www.psych.purdue.edu/~ehtibar/).\nAffect rating\nIn session 2 each participant from the scaling experiment performed an affect rating of each individual violin tone. All stimuli were presented twice with the order being randomized for each participant. Participants were asked to rate each tone on a 5-point-scale ranging from very sad (1) to very happy (5) by pressing one of the keys from F1 to F5 on the keyboard. Emblematic faces illustrated the sad and the happy end of the scale.\nValence and arousal rating\nStimulus material was also rated according to valence and arousal by two additional groups of participants. All stimuli were presented twice but the order was randomized for each participant. To give participants an idea what was meant by the terms valence and arousal they performed a short test trial on pictures taken from the IAPS. Group A (valence) (5 women, 5 men, mean age = 27.6) was asked to rate all 10 tones on a 5-point-scale ranging from very negative (1) to very positive (5). Group B (5 women, 5 men, mean age = 24.4) was asked to rate the 10 tones from very relaxed (German = “sehr entspannt”) (1) to highly aroused (German = “sehr erregt”) (5).\nResults\nSame-different forced-choice experiment\nDiscrimination probabilities for each pair of tones based on participants' same-different- judgments are shown in Table 2. Fechnerian distances for each pair of tones calculated from discrimination probabilities are shown in Table 3. Given values reflect the relative distances between pairs of tones as perceived by the mean participant. For example, tone04 (abbreviated t.04 in the row), is perceived about 1.5 times more distant from tone05 than from tone07.\nTable 2.\nDiscrimination probabilities for the 10 tones.\ntone01 tone02 tone03 tone04 tone05 tone06 tone07 tone08 tone09 tone10\nt.01 0.06 0.12 1 0.89 0.74 0.81 0.86 0.94 0.88 0.89\nt.02 0.16 0.08 0.98 0.91 0.69 0.72 0.85 0.89 0.88 0.93\nt.03 0.99 0.97 0.04 0.93 0.97 0.93 0.85 0.88 0.98 0.95\nt.04 0.9 0.93 0.96 0.08 0.82 0.42 0.51 0.64 0.6 0.96\nt.05 0.7 0.77 1 0.84 0.08 0.79 0.85 0.91 0.78 0.74\nt.06 0.89 0.8 0.94 0.62 0.93 0.07 0.3 0.35 0.74 0.79\nt.07 0.92 0.91 0.97 0.69 0.86 0.41 0.09 0.2 0.89 0.93\nt.08 0.9 0.91 0.94 0.75 0.9 0.31 0.16 0.1 0.86 0.83\nt.09 0.88 0.95 0.96 0.66 0.82 0.77 0.8 0.76 0.08 0.26\nt.10 0.91 0.94 1 0.91 0.65 0.77 0.89 0.82 0.34 0.06\nOpen in a new tab\nGiven are probabilities with which the mean perceiver judged the row tones to be different from the column tones.\nTable 3.\nFechnerian distances.\ntone01 tone02 tone03 tone04 tone05 tone06 tone07 tone08 tone09 tone10\nt.01 0.000 0.140 1.890 1.650 1.290 1.510 1.630 1.670 1.620 1.680\nt.02 0.140 0.000 1.830 1.680 1.290 1.370 1.590 1.620 1.660 1.730\nt.03 1.890 1.830 0.000 1.770 1.850 1.760 1.690 1.680 1.820 1.850\nt.04 1.650 1.680 1.770 0.000 1.500 0.890 1.030 1.190 1.100 1.550\nt.05 1.290 1.290 1.850 1.500 0.000 1.570 1.540 1.630 1.440 1.250\nt.06 1.510 1.370 1.760 0.890 1.570 0.000 0.550 0.490 1.360 1.430\nt.07 1.630 1.590 1.690 1.030 1.540 0.550 0.000 0.170 1.520 1.660\nt.08 1.670 1.620 1.680 1.190 1.630 0.490 0.170 0.000 1.440 1.490\nt.09 1.620 1.660 1.820 1.100 1.440 1.360 1.520 1.440 0.000 0.460\nt.10 1.680 1.730 1.850 1.550 1.250 1.430 1.660 1.490 0.460 0.000\nOpen in a new tab\nDistances were calculated by FSDOS (the larger the value the more distant the tones).\nAffect, arousal, and valence rating\nResults of the affect, arousal, and valence ratings are shown in Table 4 collapsed over the first and second presentation which did not differ significantly. Please note, that the affect rating was performed by the same group of participants that also took part in the same-different forced choice experiment, whereas the arousal and valence ratings were performed by two different groups of subjects. Though stemming from different groups of participants, there was a high correlation between the affect and the arousal ratings [r = 0.937, p < 0.001]. In contrast, the correlation between valence and affect ratings was rather low [r = 0.651, p = 0.042]. This is surprising for it was expected that valence and affect are closely related. It has to be noted, though, that during the testing it became apparent that participants used different concepts for the valence dimension. While some understood positive—negative in the sense of pleasant—unpleasant, others linked positive—negative to the two ends of the dimension to happy and sad. This problem is paralleled by a heterogeneous use of the valence-term in the literature (see Russell and Barrett, 1999, for a discussion) and might serve as an explanation for the incongruous pattern. In the current experiment the valence ratings will therefore, be interpreted with caution.\nTable 4.\nResults of the affect, arousal, and valence ratings.\nAffect Arousal Valence Label\ntone01 1.90 (0.61) 1.75 (0.42) 2.80 (1.40) sad01\ntone02 1.95 (0.61) 1.90 (0.66) 3.20 (0.98) sad02\ntone03 4.40 (0.94) 4.55 (0.44) 3.55 (0.90)\ntone04 2.90 (0.39) 3.15 (1.00) 3.35 (0.67)\ntone05 2.20 (0.71) 1.80 (0.54) 2.70 (0.63) sad03\ntone06 2.70 (0.59) 3.00 (0.62) 3.25 (0.49)\ntone07 3.45 (0.98) 2.95 (0.55) 2.95 (0.44) hap01\ntone08 3.60 (0.77) 3.20 (0.71) 3.30 (0.63) hap02\ntone09 3.35 (0.71) 3.40 (0.81) 3.25 (1.03) hap03\ntone10 2.55 (0.55) 2.80 (0.63) 2.70 (1.01)\nOpen in a new tab\nEach scale ranged from 1 to 5; last column gives the label of the tone for the MMN study.\nSelection of stimuli for the MMN experiment\nThree sad tones [tone01 (sad01), tone02 (sad02), tone05 (sad03)] and 3 happy tones [tone07 (hap01), tone08 (hap02), tone09 (hap03)] were chosen from the data set based on their affect ratings. The happy tones had mean affect ratings of 3.45, 3.60, and 3.35; sad tones were rated 1.90, 1.95, and 2.20, respectively. Affect ratings of happy and sad tones were significantly different [F(9, 90) = 12.9 p < 0.001] and scaling procedures demonstrated that tones were perceived as different even when belonging to the same emotion category. Fechnerian distances between happy and sad tones fell between 1.44 and 1.67. Distances were 0.17, 1.52, and 1.44 among happy tones and 0.14 and 1.29 among sad tones.\nEvent-related potential experiment\nMethods\nParticipants\nOf a total of 19 participants three had to be excluded because of technical error (two) or too many blink artifacts in the ERP data (one). The remaining 16 participants (8 women) were aged between 21 and 29 years (mean = 24.9). None was a professional musician.\nDesign\nStimuli were the 6 different single violin tones chosen on the basis of the scaling experiment. Two conditions were set up in a modified oddball-design. In condition A 3 sad tones were presented in random order (standards) with 1 happy tone (deviant) randomly interspersed. In condition B 3 happy tones were presented as standards with 1 sad tone randomly interspersed as deviant tone. As deviants, the tones with the lowest and highest affect ratings were chosen. The probability of occurrence was 25% for each of the three standard tones and the deviant tone, resulting in an overall probability of 75% for the standard stimuli and 25% for the affective deviant. In both conditions each tone was presented 340 times resulting in a total of 1360 tones per condition. A randomization algorithm guaranteed that identical tones were never presented back-to-back. Both conditions were divided in two blocks of 680 tones. The order of blocks was ABAB or BABA. All four blocks were presented in one session with one pause between block 2 and 3. The total duration of the experiment was about 90 min.\nTones were presented via insert ear phones used with Earlink ear-tips (Aearo Comp.). Stimulus onset asynchrony between two tones was 2000 ms. Mean sound pressure level of the presentation of all tones was 70 dB. To realize a non-attentive listening paradigm, participants were instructed to pay attention to cartoons (Tom and Jerry—The classical collection 1) presented silently on a computer screen in front of them. To control how well participants had attended the film a difficult post-test was performed after the experiment requiring participants to recognize selected scenes. On average, 85% of the scenes were classified correctly, indicating that the participants had indeed attended the film.\nERP-recording\nThe electroencephalogram (EEG) was recorded from 32 tin electrodes mounted in an elastic cap according to the 10–20-system. Electrode impedance was kept below 5 kΩ. The EEG was amplified (bandpass 0.1–40 Hz) and digitized continuously at 250 Hz. Electrodes were referenced on-line to the left mastoid. Subsequently, off-line re-referencing to an electrode placed on the nose-tip was performed. Electrodes placed at the outer canthus of each eye were used to monitor horizontal eye movements. Vertical eye movements and blinks were monitored by electrodes above and below the right eye. Averages were obtained for 1024 ms epochs including a 100 ms pre-stimulus baseline period. Trials contaminated by eye movements or amplifier blocking or other artifacts within the critical time window were rejected prior to averaging. For this, different artifact rejection thresholds were defined for the eye- and EEG channels.\nSeparate averages were calculated for each tone in both conditions. ERPs were quantified by mean amplitude measures using the mean voltage of the 100 ms period preceding the onset of the stimulus as a reference. Time windows and electrode sites are specified at the appropriate places of the result section. Effects were tested for significance in separate ANOVAs, with stimulus type (standard or deviant) and electrode site as factors. The Huynh-Feldt epsilon correction (Huynh and Feldt, 1980) was used to correct for violations of the sphericity assumption. Reported are the original degrees of freedom and the corrected p-values. Significance level was set to p < 0.05.\nResults\nThe grand average waveforms to the standard and deviant tones (Figure 1) are characterized by a N1-P2-complex as typically found in auditory stimulation (Näätänen et al., 1988), followed by a long-duration negative component with a frontal maximum and a peak around 400–500 ms. The current design allows two different ways to assess emotional deviants. Firstly, deviants and standards collected in the same experimental blocks can be compared (i.e., happy standard vs. sad deviant or sad standard vs. happy deviant). These stimulus classes are emotionally as well as physically different. Secondly, the ERP to the deviant can be compared with the same tone when it was presented as standard in the other condition, such that the compared stimuli are physically identical but differ in their functional significance as standard and deviant (i.e., sad standard vs. sad deviant and happy standard vs. happy deviant, see Table 5). Time windows for the statistical analysis were set as follows: 100–200 ms (N1), 200–300 ms (P2), and 380–600 ms. Electrode sites included in the analysis were F3, F4, FC5, FC6, C3, C4, Fz, FCz, Cz.\nFigure 1.\nOpen in a new tab\nGrand average ERPs for condition (A) (top) and (B) (bottom); the respective standard-ERP (bold line) is depicted with the ERP to the emotionally deviating tone when it was presented as deviant (dotted line) or as standard in the other condition (dashed line). Highlighted time windows mark significant differences in both standard-deviant comparisons.\nTable 5.\nComparison of standard vs. deviant stimuli.\nComparison Standard Deviant 100–200 ms 200–300 ms 380–600 ms\nCondition A Sad standards HAP02 0.93 2.40 7.32*\nCondition B Happy standards SAD01 0.06 10.94** 0.00\nAcross conditions HAP02 as std. HAP02 0.27 0.55 9.20**\nAcross conditions SAD01 as std. SAD01 3.04 0.00 0.01\nOpen in a new tab\nGiven are the F-values (df = 1,15).\n**p < 0.01;\n*p< 0.05.\nIn condition A, emotional (happy) deviants elicited a more negative waveform in a late latency range (from 380 ms), regardless of the comparison (Figure 1, top; Table 5). Thus, the mismatch response cannot be explained by the fact that physically different tones elicited the different ERP waveforms. To illustrate the scalp distribution of this effect, the difference happy deviant minus sad standards was computed and the mean amplitude of the difference waveform in the time-window 500–600 ms was used to create spline-interpolated isovoltage maps. The topographical distribution was typical for an MMN response. In particular, we observed a polarity inversion at temporobasal (mastoid) electrode sites (Figure 2). In condition B (Figure 1, bottom; Table 5), sad deviants, too, elicited a more negative waveform than the happy standards, though in an earlier latency range (P2, 200–300 ms). However, no difference was found when the ERPs to the sad tone were compared across conditions, suggesting that this effect was triggered by the structural difference of happy and sad tones rather than their functional significance as standard and deviant. To summarize the result: presenting a happy tone in a series of sad tones resulted in a late negativity that was larger in amplitude than the ERP to the same happy tone functioning as standard in the opposite condition. In contrast, no difference that could be related to its functional significance was found for the sad tone presented in a train of differing happy tones.\nFigure 2.\nOpen in a new tab\nSpline-interpolated isovoltage maps depicting the mean amplitude of the “happy deviant minus sad standard” difference wave from condition A. A typical frontal maximum was observed. The polarity inversion at temporobasal electrodes suggests that this response belongs to the MMN family.\nDiscussion\nThe affective deviant in condition A evoked a clear mismatch reaction. Though the latency was rather long, its topographic distribution, including the typical inversion of polarity over temporal regions (see Figure 2) in our nose-tip referenced data, suggests that it belongs to the MMN-family. Indeed, it is a known fact that MMN-latency increases with discrimination difficulty. In this regard, we would like to point to the predecessor study (Goydke et al., 2004), in which we obtained a rather long latency of the MMN response for emotional deviants, even though the latency was still shorter than in the present study. No doubt, discrimination was particularly difficult in the present experiment, because the difference in timbre was reduced to subtle changes in the expression of same-pitch and same-instrument tones. The mismatch reaction observed for condition A suggests that a happy tone was pre-attentively categorized as different from a group of different sad tones. An MMN reflects change detection in a previously established context (Näätänen, 1992). Thus, for it to occur, a context needs to be set up first. Consequently, the important question in the present experiment is not, what is so particular about the happy tone? The question is, what has led to grouping the standard (sad) tones into one mutual category, so that the single happy tone was perceived as standing out? For the happy tone to be categorized as deviant it was required that the sad tones—though different in structure—were perceived as belonging to the same context, i.e., category. The question thus, arises: what has led to grouping of the sad tones? Three possibilities seem plausible:\nperceptual similarity\nemotional similarity or\nemotion-specific perceptual similarity\nPerceptual similarity\nFrom the result of the scaling-experiment it can be derived, that tones within the sad category were perceived quite as different from each other on a perceptual basis (e.g., sad01 and sad03: Fechnerian distance = 1.29) as was the happy deviant from the sad standards (e.g., sad03 vs. happy deviant: Fechnerian distance = 1.44). Relative distances are visualized in Figure 3. The arrangement of tones in a three dimensional space results from feeding Fechnerian distance values into a MDS procedure (Alscal in SPSS) which finds the optimal constellation of stimuli in an n-dimensional space based on dissimilarity data. Three dimensions were found to explain 99% of variance. Note that the orientation of the dimensions is arbitrary. Though the positions of SAD01 and SAD02 are relatively close, both are rather distant from SAD03. Grouping, thus, cannot be explained by perceptual similarity alone.\nFigure 3.\nOpen in a new tab\nArrangement of tones in a three dimensional space based on the multidimensional scaling procedure. Note that orientation of dimensions is arbitrary.\nEmotional similarity\nAffect ratings (1.90, 1.95, and 2.20) indicate that the tones were perceived as equally sad in expression. There thus, is some support for the hypothesis that the tones were grouped together based on their emotional category. However, if it was the emotional expression that has led to the automatic categorization why did it not work in condition B? No index was found for a mismatch reaction in response to a sad tone randomly interspersed in a train of different happy tones. Arguing along the same line as before, this (non)-finding implies that either no mutual standard memory trace was built for the happy tones or that this memory trace was considerably weaker for these tones. Since the affect ratings of the happy tones were just as homogeneous (3.35, 3.45, and 3.60) as those of the sad tones, the question arises, if the affect ratings gave a good enough representation of the emotion as it was decoded by the listeners. Against the background that decoding accuracy of acoustical emotion expressions has repeatedly been reported to be better for sadness than for happiness (Johnstone and Scherer, 2000; Elfenbein and Ambady, 2002; Juslin and Laukka, 2003), it might be necessary to take a second look at the stimulus material. Banse and Scherer (1996) found that if participants had the option to choose among many different emotional labels to rate an example of vocal expression, happiness was often confused with other emotions. In the present experiment participants had given their rating on bipolar dimensions ranging from happy to sad. It cannot be ruled out that the response format biased the outcome. It is, for example, possible that in some cases participants chose to rate happy because the tone was found to be definitely not-sad, even if it was not perceived as being really happy either. In an attempt to examine the perceived similarity of the tones with respect to the expressed emotion without pre-selected response categories, a similarity rating on emotional expression was performed post-hoc. For that purpose, the same students who had participated in the first scaling-experiment were asked to perform another same-different-judgment on the same stimulus material, though this time with regard to the emotion expressed in the tone. The results are depicted in Table 6 and show that sad tones (t.01, t.02, t.05) were perceived considerably more similar to each other with respect to the emotion expressed than the happy tones (t.07, t.08, t.09). In fact, sad tones were judged half as dissimilar from each other than the happy tones (0.503 vs. 1.02). Figure 4 shows the relation of same and different responses given for happy and sad tone pairs, respectively. Sad tones were considerably more often considered to belong to the same emotional category than happy tones (80% vs. 57% “same”-responses). It can be assumed that in the MMN-experiment, too, sad tones (in condition A) were perceived as belonging into one emotional category while happy tones (in condition B) were not. The difficulty to attribute the happy tones to the same “standard” category can serve as explanation why the sad tone did not evoke a MMN. It was not registered as deviant against a happy context, because no such context existed. Nevertheless, the hypothesis that the MMN reflects deviance detection based on emotional categorization can at least be maintained for condition A.\nTable 6.\nFechnerian distances as calculated from same-different-judgments of emotional expression for the 10 tones.\ntone01 tone02 tone03 tone04 tone05 tone06 tone07 tone08 tone09 tone10\nt.01 0.000 0.012 1.763 1.003 0.491 0.943 1.103 1.003 1.072 0.983\nt.02 0.012 0.000 1.751 0.991 0.503 0.931 1.091 0.991 1.072 0.971\nt.03 1.763 1.751 0.000 1.390 1.700 1.040 0.880 0.990 1.420 1.560\nt.04 1.003 0.991 1.390 0.000 0.820 0.580 0.630 0.620 0.600 0.750\nt.05 0.491 0.503 1.700 0.820 0.000 1.020 1.170 1.080 0.730 0.650\nt.06 0.943 0.931 1.040 0.580 1.020 0.000 0.160 0.060 0.860 0.850\nt.07 1.103 1.091 0.880 0.630 1.170 0.160 0.000 0.110 1.020 1.010\nt.08 1.003 0.991 0.990 0.620 1.080 0.060 0.110 0.000 0.920 0.910\nt.09 1.072 1.072 1.420 0.600 0.730 0.860 1.020 0.920 0.000 0.150\nt.10 0.983 0.971 1.560 0.750 0.650 0.850 1.010 0.910 0.150 0.000\nOpen in a new tab\nGiven are perceived distances of row tones and column tones with respect to their emotional expression; sad tones were t.01, t.02, and t.05, happy tones were t.07, t.08, and t.09.\nFigure 4.\nOpen in a new tab\nSame and different responses for tone pairs in the categories sad (left) and happy (right), respectively.\nEmotion-specific perceptual similarity\nIt was presupposed that emotion recognition in acoustical stimuli is based on certain acoustical cues coding the emotion intended to be expressed by the sender. To test whether the sad tones in the present experiment were similar with regard to prototypical cues for sadness an acoustical analysis was performed on the stimulus set. Tones were analyzed on the parameters found to be relevant in the expression of emotion on single tones (Juslin, 2001). Using PRAAT (Boersma, 2001) and dBSonic, tones were assessed for the following features: high frequency energy, attack, mean pitch, pitch contour, vibrato amplitude, vibrato rate, sound level. For each feature, the range of values was divided into three categories (low, medium, high) and each tone was classified accordingly (Table 7). The acoustical analysis revealed that some though not all parameters were manipulated the way it would have been expected based on previous findings. However, Table 7 indicates that the cues were not used homogeneously. For example, mean pitch level was not a reliable cue. Moreover, vibrato was manipulated in individual ways by the musicians. Timbre, however, was well in line with expectations. All sad tones were characterized by little energy in the high frequency spectrum. In contrast, more energy in high frequencies was found in the spectrum of the deviant happy tone. Based on the findings by Tervaniemi et al. (1994) it appears that a difference in spectral structure alone can trigger the MMN. That would mean that the sad tones were grouped together as standards based on their mutual feature of attenuated higher partials. It has to be noted though that the high-frequency energy parameter is a very coarse means to describe timbre. Especially in natural tones [compared to synthesized tones as used by Tervaniemi et al. (1994)] the spectrum comprises a large number of frequencies with different relative intensities. As a consequence, the tones still have very individual spectra (and consequently sounds), even if they all display a relatively low high-frequency energy level. This fact is also reflected in the low perceptual similarity ratings. Moreover, if the spectral structure really was the major grouping principle, it should also have applied to the happy tones in condition B. Here, all happy tones were characterized by a high amount of energy in high frequencies, while the sad deviant was not. Nevertheless, no MMN was triggered. To conclude, though the possibility cannot be completely ruled out, it is not very likely that the grouping of the sad tones was based solely on similarities of timbre structure. Instead, the heterogeneity of parameters in Table 7 provides support for Juslin's idea of redundant code usage in emotion communication (Juslin, 1997b, 2001). Obviously, expressive cues were combined differently in different sad tones. Thus, though the sad tones did not display homogeneous patterns of emotion-specific cues, each tone was characterized by at least two prototypical cues for sadness expression. Based on the model assumption of redundant code usage, it seems likely that tones were grouped together because they were identified as belonging to one emotional category based on emotion specific-cues.\nTable 7.\nResults of the acoustical analysis of the sad tones.\nSAD01 SAD02 SAD03\nTimbre (high frequency energy) Low Low Low\nAttack Medium Medium Medium\nMean pitch Low Medium Medium\nPitch contour Normal Down Down\nVibrato amplitude Medium Medium Low\nVibrato rate Slow Medium Slow\nSound level Low Medium Medium\nOpen in a new tab\nTested were parameters expected to be relevant cues to express emotion on single tones. Categorization as low, medium, and high was based on comparison with the “happy” tones.\nWhat implication does this consideration have for the question of grouping principles in the MMN-experiment? From what is known about the principles of the MMN, the results imply that the representation of the standard in memory included invariances across several different physical features. The invariances, however, needed to be in line with a certain template on how sadness is acoustically encoded. Several researchers have suggested the existence of such hard-wired templates for the rapid processing of emotional signals (Lazarus, 1991; LeDoux, 1991; Ekman, 1999; Scherer, 2001). It is assumed that to allow for quick adaptational behavior, stimulus evaluation happens fast and automatic. Incoming stimuli are expected to run through a matching process in which comparison with a number of schemes or templates takes place. Templates can be innate and/or formed by social learning (Ekman, 1999). The present study, while blind with respect to the origin of the template, provides some information as to how such a matching process might be performed on a pre-attentive level. Given the long latency of the MMN in the present experiment, it can be assumed that basic sensory processing has already taken place before the mismatch reaction occurs. Therefore, the MMN in the current experiment appears to reflect the mismatch between the pattern of acoustic cues identified as emotionally significant and the template for sad stimuli activated by the preceding standard tones. Our data is thus, in line with considerations that the MMN does not only occur in response to basic acoustical feature processing. Several authors have suggested that the MMN can also reflect “holistic” (Gomes et al., 1997; Sussman et al., 1998) or “gestalt-like” (Lattner et al., 2005) perception. They assume that the representation of the “standard” in the auditory memory system is not merely built up based on the just presented standard-stimuli, but that it can be influenced by prototypical representations stored in other areas of the brain (Phillips et al., 2000). Evidence from a speech-specific phoneme processing task suggested that the MMN-response does not only rely on matching processes in the transient memory store but that long-term representations for prototypical stimuli were accessed already at a pre-attentive level. For phonemes, (Näätänen and Winkler, 1999) assumed the existence of long-term memory traces serving as recognition patterns or templates in speech perception. He further posited that these can be activated by sounds “nearly matching with the phoneme-specific invariant codes” (p. 14). In another contribution, Näätänen et al. (2005) point out that the “mechanisms of generation of these more cognitive kinds of MMNs of course involve other, obviously higher-order, neural populations than those activated by a mere frequency change.” (p. 27).\nIn the model of Schirmer and Kotz (2006) emotional-prosodic processing is conceptualized as a hierarchical process. Stage 1 comprises initial sensory processing of the auditory information before emotionally significant cues are integrated (stage 2) and cognitive evaluation processes (stage 3) take place. The MMN in response to emotional auditory stimuli might reflect the stage of integrating emotionally significant cues (Schirmer et al., 2005). The present data is compatible with the model albeit in the area of nonverbal auditory emotion processing. The current data contributes to disentangling the processes underlying emotion recognition in the auditory domain. It has to be pointed out though that the present results can only give a first glimpse on the mechanisms underlying processing of emotionally expressive tones. More studies with a larger set of tones characterized by different cues are needed to systematically examine the nature of the stimulus evaluation process.\nAlso, a critical issue for emotion recognition from musical sounds might be the time over which a listener can integrate the information. This might be the answer to the question as to why the happy tones were perceived less homogeneous than the sad tones. While all musicians had the intention to express happiness, it is possible that happiness can just not be expressed very well on single tones. Juslin (1997a), when looking for predictors of emotional ratings of musical performances, found that the best predictors for happiness were tempo and articulation. Both parameters are suprasegmental features and require a whole sequence of tones. In contrast, sadness ratings could be predicted by a number of cues, including segmental features such as sound level, spectrum, and attack.\nConflict of interest statement\nThe authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.\nAcknowledgments\nThis research was supported by the Studienstiftung des Deutschen Volkes (Katja N. Spreckelmeyer) and the Deutsche Forschungsgemeinschaft (Katja N. Spreckelmeyer, Hans Colonius, Eckart Altenmüller, Thomas F. Münte). Hans Colonius and Thomas F. Münte were members of the SFB TR31 “Active Listening” during the time of the experiment.\nReferences\nBanse R., Scherer K. R. (1996). Acoustic profiles in vocal emotion expression. J. Pers. Soc. Psychol. 70, 614–636 10.1037/0022-3514.70.3.614 [DOI] [PubMed] [Google Scholar]\nBoersma P. (2001). Praat, a system for doing phonetics by computer. Glot Int. 5, 341–345 [Google Scholar]\nBostanov V., Kotchoubey B. (2004). Recognition of affective prosody: continuous wavelet measures of event-related brain potentials to emotional exclamations. Psychophysiology 41, 259–268 10.1111/j.1469-8986.2003.00142.x [DOI] [PubMed] [Google Scholar]\nCaclin A., Brattico E., Tervaniemi M., Näätänen R., Morlet D., Giard M. H., et al. (2006). Separate neural processing of timbre dimensions in auditory sensory memory. J. Cogn. Neurosci. 18, 1959–1972 10.1162/jocn.2006.18.12.1959 [DOI] [PubMed] [Google Scholar]\nCaclin A., McAdams S., Smith B. K., Giard M. H. (2008). Interactive processing of timbre dimensions: an exploration with event-related potentials. J. Cogn. Neurosci. 20, 49–64 10.1162/jocn.2008.20001 [DOI] [PubMed] [Google Scholar]\nDzhafarov E., Colonius H. (2006). Generalized fechnerian scaling, in Measurement and Representation of Sensations, eds Colonius H., Dzhafarov E. (Mahwah, NJ: Erlbaum; ), 47–87 [Google Scholar]\nDzhafarov E. N., Colonius H. (1999). Fechnerian metrics in unidimensional and multidimensional stimulus spaces. Psychon. Bull. Rev. 6, 239–268 10.3758/BF03212329 [DOI] [PubMed] [Google Scholar]\nDzhafarov E. N., Colonius H. (2001). Multidimensional fechnerian scaling: basics. J. Math. Psychol. 45, 670–719 10.1006/jmps.2000.1341 [DOI] [PubMed] [Google Scholar]\nEerola T., Ferrer R., Alluri V. (2012). Timbre and affect dimensions: evidence from affect and similarity ratings and acoustic correlates of isolated instrument sounds. Music Percept. 30, 49–70 10.1525/mp.2012.30.1.49 [DOI] [Google Scholar]\nEkman P. (1999). Basic emotions, in Handbook of Cognition and Emotion, eds Dalgleish T., Power M. (Sussex: John Wiley and Sons Ltd.) 45–60 [Google Scholar]\nElfenbein H. A., Ambady N. (2002). On the universality and cultural specificity of emotion recognition: a meta-analysis. Psychol. Bull. 128, 203–235 10.1037/0033-2909.128.2.203 [DOI] [PubMed] [Google Scholar]\nFechner G. (1860). Elemente der Psychophysik. Leipzig: Breitkopf und Härtel [Google Scholar]\nGomes H., Bernstein R., Ritter W., Vaughan H. G., Miller J. (1997). Storage of feature conjunctions in transient auditory memory. Psychophysiology 34, 712–716 10.1111/j.1469-8986.1997.tb02146.x [DOI] [PubMed] [Google Scholar]\nGoydke K. N., Altenmüller E., Möller J., Münte T. F. (2004). Changes in emotional tone and instrumental timbre are reflected by the mismatch negativity. Brain Res. Cogn. Brain Res. 21, 351–359 10.1016/j.cogbrainres.2004.06.009 [DOI] [PubMed] [Google Scholar]\nGrey J. (1977). Multidimensional perceptual scaling. J. Acoust. Soc. Am. 61, 1270–1277 10.1121/1.381428 [DOI] [PubMed] [Google Scholar]\nGrey J., Moorer J. A. (1977). Perceptual evaluation of synthetic music instrument tones. J. Acoust. Soc. Am. 62, 454–462 10.1121/1.381508 [DOI] [Google Scholar]\nHeilman K. M., Scholes R., Watson R. T. (1975). Auditory affective agnosia. J. Neurol. Neurosurg. Psychiatry 38, 69–72 10.1136/jnnp.38.1.69 [DOI] [PMC free article] [PubMed] [Google Scholar]\nHelmholtz H. (1885/1954). On the Sensations of Tone. New York, NY: Dover Publications [Google Scholar]\nHevner K. (1935). The affective character of the major and minor modes in music. Am. J. Psychol. 47, 103–118 10.2307/141671023914179 [DOI] [Google Scholar]\nHevner K. (1936). Experimental studies of the elements of expression in music. Am. J. Psychol. 48, 246–268 10.2307/141574619884142 [DOI] [Google Scholar]\nHevner K. (1937). The affective value of pitch and tempo in music. Am. J. Psychol. 49, 621–630 10.2307/1416385 [DOI] [Google Scholar]\nHuynh H., Feldt L. (1980). Conditions under which mean square ratios in repeated measure designs have excact f-distributions. J. Am. Stat. Assoc. 65, 1582–1589 10.1080/01621459.1970.10481187 [DOI] [Google Scholar]\nJääskeläinen I. P., Ahveninen J., Bonmassar G., Dale A. M., Ilmoniemi R. J., Levanen S., et al. (2004). Human posterior auditory cortex gates novel sounds to consciousness. Proc. Natl. Acad.Sci. U.S.A. 101, 6809–6814 10.1073/pnas.0303760101 [DOI] [PMC free article] [PubMed] [Google Scholar]\nJohnstone T., Scherer K. R. (2000). Vocal communication of emotion, in Handbook of Emotions, eds Lewis M., Haviland-Jones J. (New York, NY: Guilford Press; ), 220–235 [Google Scholar]\nJuslin P. (1997a). Perceived emotional expression in synthesized performances of a short melody: capturing the listener's judgment policy. Musicae Scientiae 1, 225–256 [Google Scholar]\nJuslin P. (1997b). Emotional communication in music performance: a functionalist perspective and some data. Music Percept. 14, 383–418 10.2307/40285731 [DOI] [Google Scholar]\nJuslin P. (2001). Communicating emotion in music performance: a review and theoretical framework, in Music and Emotion, eds Juslin P., Sloboda J. (Oxford: Oxford University Press; ), 309–337 [Google Scholar]\nJuslin P., Laukka P. (2003). Communication of emotions in vocal expression and music performance: different channels, same code? Psychol. Bull. 129, 770–814 10.1037/0033-2909.129.5.770 [DOI] [PubMed] [Google Scholar]\nKruskal J. (1964a). Multidimensional scaling by optimizing goodness of fit to a non-metric hypothesis. Psychometrika 29, 1–27 10.1007/BF02289565 [DOI] [Google Scholar]\nKruskal J. (1964b). Non-metric multidimensional scaling: a numerical method. Psychometrika 29, 115–129 10.1007/BF02289694 [DOI] [Google Scholar]\nLang P. J., Bradley M. M., Cuthbert B. N. (2008). International affective picture system (IAPS): Affective Ratings of Pictures and Instruction Manual. Technical Report A-8. University of Florida, Gainesville, FL. [Google Scholar]\nLattner S., Meyer M. E., Friederici A. D. (2005). Voice perception: sex, pitch, and the right hemisphere. Hum. Brain Mapp. 24, 11–20 10.1002/hbm.20065 [DOI] [PMC free article] [PubMed] [Google Scholar]\nLazarus R. (1991). Emotion and Adaptation. New York, NY: Oxford University Press [Google Scholar]\nLeDoux J. E. (1991). Emotion and the brain. J. NIH Res. 3, 49–51 [Google Scholar]\nMcAdams S., Winsberg S., Donnadieu S., De Soete G., Krimphoff J. (1995). Perceptual scaling of synthesized musical timbres: common dimensions, specificities, and latent subject classes. Psychol. Res. 58, 177–192 10.1007/BF00419633 [DOI] [PubMed] [Google Scholar]\nNäätänen R. (1992). Attention and Brain Function. Hillsdale, NJ: Erlbaum [Google Scholar]\nNäätänen R., Jacobsen T., Winkler I. (2005). Memory-based or afferent processes in mismatch negativity (MMN): a review of the evidence. Psychophysiology 42, 25–32 10.1111/j.1469-8986.2005.00256.x [DOI] [PubMed] [Google Scholar]\nNäätänen R., Sams M., Alho K., Paavilainen P., Reinikainen K., Sokolov E. N. (1988). Frequency and location specificity of the human vertex n1 wave. Electroencephalogr. Clin. Neurophysiol. 69, 523–531 10.1016/0013-4694(88)90164-2 [DOI] [PubMed] [Google Scholar]\nNäätänen R., Tervaniemi M., Sussman E., Paavilainen P., Winkler I. (2001). Primitive intelligence” in the auditory cortex. Trends Neurosci. 24, 283–288 10.1016/S0166-2236(00)01790-2 [DOI] [PubMed] [Google Scholar]\nNäätänen R., Winkler I. (1999). The concept of auditory stimulus presentation in cognitive neuroscience. Psychol. Bull. 6, 826–859 10.1037/0033-2909.125.6.826 [DOI] [PubMed] [Google Scholar]\nOpitz B., Schröger E., von Cramon D. Y. (2005). Sensory and cognitive mechanisms for preattentive change detection in auditory cortex. Eur. J. Neurosci. 21, 531–535 10.1111/j.1460-9568.2005.03839.x [DOI] [PubMed] [Google Scholar]\nOwings D., Morton E. (1998). Animal Vocal Communication: A New Approach. Cambridge: Cambridge University Press; 10.1017/CBO9781139167901 [DOI] [Google Scholar]\nPeretz I., Blood A. J., Penhune V., Zatorre R. (2001). Cortical deafness to dissonance. Brain 124, 928–940 10.1093/brain/124.5.928 [DOI] [PubMed] [Google Scholar]\nPhillips C., Pellathy T., Marantz A., Yellin E., Wexler K., Poeppel D., et al. (2000). Auditory cortex accesses phonological categories: an MEG mismatch study. J. Cogn. Neurosci. 12, 1038–1055 10.1162/08989290051137567 [DOI] [PubMed] [Google Scholar]\nPicton T. W., Alain C., Otten L., Ritter W., Achim A. (2000). Mismatch negativity: different water in the same river. Audiol. Neurootol. 5, 111–139 10.1159/000013875 [DOI] [PubMed] [Google Scholar]\nPihan H., Altenmüller E., Hertrich I., Ackermann H. (2000). Cortical activation patterns of affective speech processing depend on concurrent demands on the subvocal rehearsal system. A DC-potential study. Brain 123, 2338–2349 10.1093/brain/123.11.2338 [DOI] [PubMed] [Google Scholar]\nRothkopf E. Z. (1957). A measure of stimulus similarity and errors in some paired- associate learning tasks. J. Exp. Psychol. 53, 94–101 10.1037/h0041867 [DOI] [PubMed] [Google Scholar]\nRussell J., Barrett L. (1999). Core affect, prototypical emotional episodes, and other things called emotion: dissecting the elephant. J. Pers. Soc. Psychol. 76, 805–819 10.1037/0022-3514.76.5.805 [DOI] [PubMed] [Google Scholar]\nScherer K. R. (1988). On the symbolic function of vocal affect expression. J. Lang. Soc. Psychol. 7, 79–100 10.1177/0261927X8800700201 [DOI] [Google Scholar]\nScherer K. R. (2001). The nature and study of appraisal: a review of the issue, in Appraisal Processes in Emotion: Theory, Methods, Research, eds Scherer K. R., Schorr A., Johnstone T. (Oxford: Oxford University Press; ), 369–391 [Google Scholar]\nSchirmer A., Kotz S. A. (2006). Beyond the right hemisphere: brain mechanisms mediating vocal emotional processing. Trends Cogn. Sci. 10, 24–30 10.1016/j.tics.2005.11.009 [DOI] [PubMed] [Google Scholar]\nSchirmer A., Striano T., Friederici A. D. (2005). Sex differences in the preattentive processing of vocal emotional expressions. Neuroreport 16, 635–639 10.1097/00001756-200504250-00024 [DOI] [PubMed] [Google Scholar]\nSchröder C., Möbes J., Schütze M., Szymanowski F., Nager W., Bangert M., et al. (2006). Perception of emotional speech in Parkinson's disease. Mov. Disord. 21, 1774–1778 10.1002/mds.21038 [DOI] [PubMed] [Google Scholar]\nSchröger E., Wolff C. (1996). Mismatch response of the human brain to changes in sound location. Neuroreport 7, 3005–3008 10.1097/00001756-199611250-00041 [DOI] [PubMed] [Google Scholar]\nSloboda J. (1990). Empirical studies of the emotional response to music, in Cognitive Bases of Musical Communication, eds Jones M., Holleran S. (Washington, DC: American Psychological Association; ), 33–46 [Google Scholar]\nSussman E., Gomes H., Nousak J. M., Ritter W., Vaughan H. G. (1998). Feature conjunctions and auditory sensory memory. Brain Res. 793, 95–102 10.1016/S0006-8993(98)00164-4 [DOI] [PubMed] [Google Scholar]\nTervaniemi M., Maury S., Näätänen R. (1994). Neural representations of abstract stimulus features in the human brain as reflected by the mismatch negativity. Neuroreport 5, 844–846 10.1097/00001756-199403000-00027 [DOI] [PubMed] [Google Scholar]\nTervaniemi M., Winkler I., Näätänen R. (1997). Pre-attentive categorization of sounds by timbre as revealed by event-related potentials. Neuroreport 8, 2571–2574 10.1097/00001756-199707280-00030 [DOI] [PubMed] [Google Scholar]\nWilliams C. E., Stevens K. N. (1972). Emotions and speech: some acoustical correlates. J. Acoust. Soc. Am. 52, 1238–1250 10.1121/1.1913238 [DOI] [PubMed] [Google Scholar]\nArticles from Frontiers in Psychology are provided here courtesy of Frontiers Media SA\nACTIONS\nView on publisher site\nPDF (921.3 KB)\nCite\nCollections\nPermalink\nRESOURCES\nSimilar articles\nCited by other articles\nLinks to NCBI Databases\nON THIS PAGE\nAbstract\nIntroduction\nScaling experiment\nMaterials and methods\nResults\nEvent-related potential experiment\nDiscussion\nAcknowledgments\nReferences\n\nFOLLOW NCBI\nConnect with NLM\nNational Library of Medicine\n8600 Rockville Pike\nBethesda, MD 20894\nWeb Policies\nFOIA\nHHS Vulnerability Disclosure\nHelp\nAccessibility\nCareers\nNLM\nNIH\nHHS\nUSA.gov",
    "https://www.psypost.org/new-research-uncovers-atonal-musics-distinct-emotional-and-neural-effects/": "SUBSCRIBE\nThe latest psychology and neuroscience discoveries.\nMY ACCOUNT\nMENTAL HEALTH\nSOCIAL PSYCHOLOGY\nCOGNITIVE SCIENCE\nPSYCHOPHARMACOLOGY\nNEUROSCIENCE\nABOUT\nHome Exclusive Music\nNew research uncovers atonal music’s distinct emotional and neural effects\nby Eric W. Dolan May 30, 2024 in Music\n(Photo credit: Adobe Stock)\nStay on top of the latest psychology findings: Subscribe now!\nA recent study published in the journal Behavioral Neuroscience has shed light on how atonal music affects our emotions and brain activity. The researchers found that atonal music, unlike the more familiar tonal music, tends to be perceived as less pleasurable and less familiar, evoking distinct neural responses.\nPrevious studies have mostly focused on tonal music, which follows conventional harmonic structures familiar to Western listeners. However, there is a growing interest in understanding how atonal music — compositions that do not follow these traditional structures — affects our emotions and brain activity.\nTonal music is built on a hierarchical system of pitches centered around a tonic note, creating a sense of resolution and familiarity through chord progressions and harmonic relationships. This structure is deeply ingrained in Western music, making it easy for listeners to predict and follow the music’s flow, often leading to feelings of pleasure and emotional engagement.\nIn contrast, atonal music breaks away from these conventional harmonic structures. It does not adhere to a single key or tonal center, and instead uses a more equal treatment of all twelve pitches in the octave. This can result in music that sounds unpredictable, dissonant, or unsettling to those accustomed to tonal harmony.\n“As a musicologist and neuroscientist, I’m very interested in the study of the brain processes underlying music cognition, particularly music-induced emotions,” said study author Pablo Valdés-Alemán of the Centro de Investigación Transdisciplinar en Psicología (Center for Transdisciplinary Research in Psychology) in Mexico.\n“In this case, our approach was to base our study on a previous model of brain electrical activity, measured with electroencephalography (EEG), where frontal asymmetries are associated with certain types of emotions. This model had been studied before with various emotional stimuli, including music. Nonetheless, we wanted to test a particular type of music (atonal music), which is less familiar to the average Western listener and also less studied in this field.”\nThe study involved 25 Mexican participants, a mix of men and women aged around 38 years, who were not musicians and had no hearing impairments. Participants were all right-handed, as left-handed individuals might process emotions differently due to variations in brain hemispheric specialization.\nThe participants listened to 16 different musical pieces — eight tonal and eight atonal. These pieces were chosen to evoke either joy or sadness, aligning with Russell’s model of emotional dimensions that categorize emotions based on valence (positive or negative) and arousal (high or low). Each piece lasted about 25 seconds and was played through speakers while participants were seated comfortably in a quiet, isolated room.\nTo measure the brain’s electrical activity, researchers used electroencephalography (EEG), which involves placing electrodes on the scalp. This method offers excellent temporal resolution, allowing researchers to track brain activity in real-time. The electrodes recorded activity in various brain regions, focusing on the frontal and parietal areas, which are crucial for processing emotions.\nParticipants also answered questions about their emotional reactions to each piece, rating the music on scales of valence, arousal, pleasure, and familiarity.\nThe researchers found clear differences in how tonal and atonal music were perceived and processed by the brain. Tonal music was generally rated as more positive and pleasurable compared to atonal music. Joyful pieces of tonal music were particularly highly rated in terms of valence (positivity) and arousal (excitement). In contrast, atonal music was perceived as less familiar and less pleasurable.\nInterestingly, despite being perceived as negative in terms of valence and arousal, sad music — whether tonal or atonal — could still be rated as pleasurable by some participants. This finding aligns with previous research suggesting that people can find pleasure in sad music, highlighting the complex relationship between emotional valence and pleasure.\nThe EEG data revealed that listening to joyful music was associated with increased activity in the left frontal brain regions, a pattern consistent with positive emotional responses. This is known as frontal alpha asymmetry, where greater left frontal activation correlates with positive emotions. Atonal music, however, was linked to increased right frontal brain activity, which is often associated with negative emotions and arousal states.\n“Music is a powerful emotional stimulus, and it can impact and modulate our brain dynamics, as seen with EEG activity,” Valdés-Alemán told PsyPost. “In this case, music-induced emotions can change frontal brain activity in an asymmetrical manner. Music that induces pleasant and positive emotions is associated with increased left frontal activity.\n“This is interesting because affective disorders, like depression, which are associated with recurrent negative affect, are linked to left frontal hypoactivation. In some cases, alternative treatments may include non-invasive brain stimulation of this area. The fact that music alone may stimulate this cortical region adds to the evidence that music listening has a positive effect on mental health.”\nThe findings highlighted a stark contrast between how participants perceived and neurologically processed tonal and atonal music\n“Specifically, tonal music, which is classical music inspired by the European tradition, is perceived as more familiar and pleasurable than its atonal counterpart,” Valdés-Alemán explained. “Atonal music includes music from other cultures with different musical systems or classical music that has intentionally removed tonality to challenge the traditional tonal system. This difference is accompanied by changes in frontal EEG asymmetries, as mentioned before.”\n“In that sense, another conclusion is that familiar music tends to be more enjoyable, evoking positive emotions and possibly providing mental health benefits and modulation of underlying brain activity associated with emotional processing. Familiarity may be culture-specific, but also influenced by individual differences such as personality, music education, and past experiences.”\nWhile the study provides valuable insights, it has some limitations. The sample size was relatively small and limited to non-musicians from a specific cultural background. Future research should include larger, more diverse populations to generalize the findings. Additionally, only a limited number of brain regions were monitored, and other brainwave frequencies were not explored, which could provide a more comprehensive understanding of musical emotion processing.\n“The major caveat of this research was that EEG recordings were conducted while we were still under COVID-19 confinement,” Valdés-Alemán noted. “As you can imagine, we had a limited sample and, in general, everyone was under an emotional burden, which might have biased the emotional assessment of music during this period. I would say that the next step for this research is to study music’s emotional effects on EEG activity for people living with some type of affective disorder, like depression.”\nNevertheless, the findings provide evidence that our brain’s response to music is influenced by both its emotional content and our familiarity with it.\n“Remember that music can bring emotional comfort and has the power to positively influence our feelings,” Valdés-Alemán added. “If you are feeling down, listening to some of your favorite music might help uplift your mood. However, it is important to recognize that while music can be a helpful tool for emotional regulation, it is not a substitute for professional help.”\n“If you are experiencing persistent emotional distress or mental health issues, never hesitate to consult a mental health professional for advice and support. They can provide the necessary guidance and treatment to help you navigate your challenges effectively.”\nThe study, “Brain Electrical Patterns Associated With Pleasure and Emotion Induced by Tonal and Atonal Music,” was authored by Pablo Valdés-Alemán, Bernarda Téllez-Alanís, and Adriana Zamudio-Gurrola.\nTweet\nSend\nScan\nShare\nSend\nPin2\nShare\nShare\nShare\nShare\nShare\nRELATED\nMUSIC\nCan music heal emotional wounds? New research suggests it might\nJANUARY 21, 2025\nListening to music during memory recall can alter the emotional tone of memories, making them more positive or negative, by activating emotion- and memory-related brain regions. This discovery suggests potential therapeutic uses for reframing difficult memories.\nREAD MORE\nMUSIC\nYour personality might determine if music helps or hinders your productivity\nDECEMBER 26, 2024\nWhether music boosts productivity depends on your personality, task, and music type. Extroverts benefit more, simple tasks are easier with music, and calm tunes work best.\nREAD MORE\nDEMENTIA\nMusic-induced neuroplasticity: Implications for dementia treatment\nDECEMBER 1, 2024\nMusic therapy improves dementia care by reducing anxiety, agitation, and depression, enhancing mood, memory, and cognitive function. Familiar songs trigger emotional and physical responses, potentially strengthening neural connections and supporting overall wellbeing.\nREAD MORE\nCOGNITIVE SCIENCE\nSurprising precision: Nearly half of “earworms” match original pitch perfectly\nNOVEMBER 9, 2024\nA recent study found that a large portion of people’s sung earworms matched the pitch of the original songs exactly, adding new evidence that our brains may retain detailed musical information in ways we never realized.\nREAD MORE\nMUSIC\nLeft-handed musicians appear to develop unique brain pathways for language processing\nOCTOBER 16, 2024\nLeft-handed musicians and non-musicians appear to develop atypical brain patterns for language through different pathways, with musicians showing enhanced interhemispheric connections and non-musicians having reduced intrahemispheric connectivity in language-related brain areas.\nREAD MORE\nMUSIC\nClassical music enhances mood by triggering triple-time locking in the extended amygdala\nSEPTEMBER 15, 2024\nListening to classical music is associated with synchronized brain activity in regions involved in sound and emotion processing, with personal music preference linked to stronger neural alignment and potential improvements in depressive symptoms.\nREAD MORE\nMEMORY\nMusical memory remains resilient in old age, even for unfamiliar tunes\nAUGUST 29, 2024\nA new study finds that older adults retain the ability to recognize and remember new music, suggesting that musical memory remains resilient with age, even in complex, real-world settings like live concerts.\nREAD MORE\nMUSIC\nScientists observe a remarkable synchronization effect among classical music listeners\nAUGUST 24, 2024\nClassical music concerts synchronize the heart rates, skin conductance, and breathing rates of audience members, creating a shared physiological experience. This synchronization varies with music type and is influenced by listeners' personality traits and focus on the music.\nREAD MORE\nSUBSCRIBE\nGo Ad-Free! Click here to subscribe to PsyPost and support independent science journalism!\nSTAY CONNECTED\nRECENT\nFathers’ emotional awareness and testosterone linked to children’s prosocial behavior\nIs psychology getting race wrong? Harvard study reveals racial categories may not predict shared views on racism\nParenting stress alters the link between attachment avoidance and sexual satisfaction in couples with children\nGamers with lower social skills are more likely to make impulsive in-game purchases\nHow brain connectivity differs in healthy aging and semantic dementia\nLarge-scale neuroimaging study finds no evidence of atypical amygdala connectivity in autism\nScientists uncover a new mechanical pathway linked to Alzheimer’s disease\nLoneliness skews partner perceptions, harming relationships and reinforcing isolation\n                Contact usPrivacy policyTerms and Conditions\n[Do not sell my information]",
    "https://online.ucpress.edu/mp/article/40/3/202/195230/The-Perceptual-and-Emotional-Consequences-of": "online.ucpress.edu\nVerificar que usted es un ser humano. Esto podría tardar algunos segundos.\nonline.ucpress.edu necesita revisar la seguridad de su conexión antes de continuar.\nRay ID: 90bd3bb24cf22f8e\nRendimiento y seguridad de Cloudflare",
    "https://dl.acm.org/doi/fullHtml/10.1145/3461615.3485419": "☰ Article Navigation\nWhen Emotions are Triggered by Single Musical Notes: Revealing the Underlying Factors of Auditory-Emotion Associations\nPatrick O'Toole, School of Computer Science and Information Technology, University College Cork, Ireland, patrick.otoole@umail.ucc.ie\nDonald Glowinski, Department of Psychology and Educational Sciences, University of Geneva, Switzerland, donald.glowinski@unige.ch\nIan Pitt, School of Computer Science and Information Technology, University College Cork, Ireland, ianp@cs.ucc.ie\nMaurizio Mancini, Department of Computer Science, Sapienza University of Rome, Italy, m.mancini@di.uniroma1.it\n\nDOI: https://doi.org/10.1145/3461615.3485419\nICMI '21 Companion: Companion Publication of the 2021 International Conference on Multimodal Interaction, Montréal, QC, Canada, October 2021\nCan emotion be experienced when the auditory sense is stimulated by a single musical note (Q1), and do variables such as musical skills, age, and personality traits have an influence in auditory-emotion associations (Q2)? An experiment was conducted, in which 130 participants were asked to listen to single musical notes and rate their experienced emotional state. They also had to rate their musical proficiency, sound sensitivity, strongest learning style, and complete a reduced version of the Big-Five personality test (BFI-10). Results regarding Q1 show a correlation between lower notes and sadness, and higher notes and joy, confirming previous auditory-emotion association research, while presenting new knowledge into how emotion associates with single musical notes. Results regarding Q2 show that musical proficiency (low vs high), learning style (aural vs physical), personality (level of Conscientiousness) had an effect on how participants emotionally experienced single musical notes. The results presented in this study will provide a starting point that can help develop a new auditory-visual framework that uses understandings on emotion, personality and other variables in the development of more personalised human-computer interfaces. This new framework can be used in applications that can help in learning to paint or play an instrument; promoting positive mental health, or exploring new forms of creative expression e.g., writing a song with a paint brush as the instrument or painting a picture with a piano as your brush.\nCCS Concepts: • Human-centered computing → Sound-based input / output; Interaction design process and methods; • Applied computing~Sound and music computing; • Social and professional topics → User characteristics;\n\nKeywords: auditory-emotion associations, multi-modal interactions, music, personality\n\nACM Reference Format:\nPatrick O'Toole, Donald Glowinski, Ian Pitt, and Maurizio Mancini. 2021. When Emotions are Triggered by Single Musical Notes: Revealing the Underlying Factors of Auditory-Emotion Associations. In Companion Publication of the 2021 International Conference on Multimodal Interaction (ICMI '21 Companion), October 18–22, 2021, Montréal, QC, Canada. ACM, New York, NY, USA 8 Pages. https://doi.org/10.1145/3461615.3485419\n1 INTRODUCTION\nThis paper presents an experiment to help gain a deeper understanding into associations between single musical notes and emotions, and what impact different individual variables, such as personality, musical experience and learning style can have on auditory-emotion associations. Emotion and personality have become popular areas of research in recent years, especially in relation to technology [12, 33]. With a great amount of personal data being captured by people's devices, companies are using this data to understand their users and personalise tools and products to each user. Multi-modal interaction is an area that can benefit from adopting more personalised approaches to how we form associations between our senses [18, 30]. Interesting insights from a study, that trained participants in sound-colour associations using an ad-hoc program, while also investigating the association between sound and emotion of participants, were found. The fore-mentioned study found a correlation between very basic emotions (sadness and joy) and basic auditory stimuli (lower and higher pitch), but not at a significant level and with a low testing sample [31]. The motivation for our paper is to explore the understandings of associations between single musical notes and emotion, and if individual variables can play an important part in the process of auditory-emotion associations. With this new understanding, an improved model of auditory-visual associations can be applied in designing human-computer interfaces, one that takes account of emotion as well as personality, age, gender, and other variables. Figure 1 highlights the research of single musical notes-emotion associations, that is presented in this paper, as part of a larger framework of auditory-emotion-colour associations. This paper will help contribute to this larger framework of auditory-emotion-visual associations and contribute to future designs of intelligent and personalised human-computer interfaces, that can harness the power of multi-modal associations in creative and artistic digital environments.\nFigure 1: The work presented in the paper is highlighted by the dotted line: while cross-modal melody-emotion-colour and melody-colour associations are widely studied [20, 29, 32, 36, 37, 38, 39, 40, 42], we focus on single note-emotion associations by also looking at the individual variables in Section 2.2, in the larger framework of studying sound-emotion-colour associations.\n2 BACKGROUND\n2.1 Auditory-Emotion Research\nPrevious research into associations between auditory stimuli and emotion has been mostly situated in auditory-visual cross-modal association research, that mainly explores the role of emotion in this relationship, mostly using musical excerpts instead of single musical notes [4, 6, 21, 28, 37, 46]. Palmer et al. present the emotional meditation hypothesis, which is understood to be, when people listen to music, they have emotional responses and pick colors with similar emotional content [32]. The study from Palmer et al., and other studies have provided interesting insights into how auditory stimuli is perceived emotionally in relation to colors, with music in a major scale and at a faster tempo to be perceived lighter in colour and happier, with the opposite being true for a minor scale [32, 39, 43]. Other studies have focused solely on the auditory-emotion associations, however, these studies (like the ones mentioned above) focus on musical excerpts and not basic auditory stimuli [27, 40].\nVery few studies have been conducted on the association between basic auditory stimuli, like single musical notes, and emotion [2, 26, 37]. Like Palmer et al., Spence also supports emotion mediation as one of the key factors in cross-modal correspondences, not just for complex auditory stimuli but also more simple stimuli [37]. Comparing simple auditory stimuli to more complex stimuli shows that emotion mediation counts for less variance in the empirical matching data, and therefore elicits less pronounced perceived emotion [37]. If we can understand auditory-emotion associations in a similar way to auditory-visual associations, that are mediated by emotion, lower notes (darker colours) should associate with negative emotions and higher notes (lighter colours) with positive emotions [29, 36].\nStudies on the impact of different emotion models has been carried out in research around auditory-emotion associations, with three emotion models highlighted (Basic/discrete, dimensional and musically-induced) [40, 45]. While the musically-induced emotion model would seem appropriate for a study on single musical note-emotion associations, the comparative test of these models and also information on personality bias help formulate the best approach for the experiment presented in this paper. The study by Zentner et al., showed that, compared to the other two models, the basic emotion model performed rather well when rating perceived emotions, even if the musically-induced model was more consistent [45]. In another study, the dimensional model performed better than both the basic and musically-induced emotion one in the discrimination of musical excerpts. However, “Personality-related differences were the most pronounced in the case of the discrete emotion model” [40]. Three different studies on emotion words expressed when listening to music stimuli, show that happiness, sadness, anger and fear were among the top choices across all three studies [23]. Taking into account the studies presented above, and bearing in mind that our study focuses on single musical notes rather than musical excerpts, and includes personality traits as an independent variable to be analysed, a basic emotion model (joy, sadness, anger and fear) is proposed.\n2.2 Individual Variables\nIn this paper, we also seek to understand how twelve individual variables (Age, Gender, Musical Experience, Sound Sensitivity, Auditory-to-Other-Sense, Other-Sense-to-Auditory, Learning Style and the Big Five personality traits (BFI) of Openness, Conscientiousness, Extraversion, Agreeableness and Neuroticism), discussed in this section, can impact on the associations between single musical notes and emotion.\n2.2.1 Personality traits. The relationship between personality traits and emotional responses to music is well researched. Studies have found significant correlations between the traits Neuroticism/Extraversion and positive/negative emotions [3, 8, 22]. Chamorro‐Premuzic & Furnham found that personality and intelligence can be a factor on how people use music. A person with a high IQ and high Extraversion might use music in a rational cognitive way, while someone with high Neuroticism and low Conscientiousness might use music for emotional regulation [8]. In another study, Neuroticism was found to significantly map to all negative emotions, while Extraversion significantly mapped to the positive emotions, “interest” and “enjoyment” as well as the negative emotion “shyness” [22]. From a review on present literature on the correlation between basic emotions and personality traits, limited findings regarding the traits of Openness to Experience, Conscientiousness and Agreeableness were found [15]. The Big Five personality test is a widely adopted test used in studies relating to personality traits. It is simple and easy to interpret, and is also an effective way to understand the personality traits of an individual. Rammstedt and John conducted research into the accuracy of the BFI-10 test (using only 10 questions) and found it sufficient to use in a research settings with limited time constraints, compared to the BFI-44 test (using 44 questions) [34].\n2.2.2 Musical vs Non-Musical Experience. A study by Manno et al., investigated how musicians and non-musicians identify emotion in music. Music uses Temporal Fine Structures (TFS) and Envelope (ENV) modulation to resolve emotion in music, however the exact contributions of TFS and ENV is not known. The study showed that TFS is essential in identifying emotion in music and that there is a difference in how much TFS is used by musicians and non-musicians. Non-musicians use less fine structure information and have reduced emotional resolvability curves compared to musicians [27]. While the study above shows that musical experience can have a difference in how much TFS is used in identifying emotion in music, it is unclear if the same logic can be adopted when identifying emotion from single musical notes. A study on expressive intentions with single piano notes, shows that musical expertise has no mean effect on performers when tested on the four acoustical parameters of pitch, intensity, articulation and rhythmic density, with both musicians and non-musicians using these acoustic parameters in very similar ways when listening to a single piano note [2].\n2.2.3 Age and Gender. Studies show that age can have an impact on how we associate between our senses and how we use emotion in response to stimuli, and that it can change over time [21, 35, 44]. Hunter et al., using two variants of an emotional Stroop task, found that older adults can match congruent cross-modal stimuli just as well as younger adults however, with incongruent stimuli, the older adults performed worse than their younger counterparts [21]. Older adults tend to weaken the ability to perceive negative emotions the older they get [35], this can be caused by the fact they tend to remove problematic relationships and avoid interpersonal conflict, leading to a more positive social environment [1, 5].\nAs well as age, studies into the role of gender in auditory-emotion associations research has shown some interesting insights. Studies between men and women with regards to both uni and multi sensory emotional stimuli have shown that women are more accurate than men in recognition of emotional prosody [10, 24, 41].\n2.2.4 Other Variables. Sound sensitivity, in general and when sound is triggered as the primary and secondary source, are also investigated in this paper. Misophonia is a condition characterized by heightened emotional reactivity to common repetitive sounds, accompanied by difficulties responding to these sounds and associated impairment in functioning [7]. A study on the relationship between the Neuroticism trait and Misophonia, with regard to the role of emotion regulation found that “difficulties with emotion regulation and Neuroticism were significantly positively correlated with symptoms of misophonia” [7]. Past research in auditory-visual cross-modal associations and chromaesthesia have also informed our understanding of the way in which perception operates between auditory/other sense and other sense/auditory with regard to sound sensitivity [11, 26, 36].\nSeven learning-style terms from Gardner's Theory of Multiple Intelligences (Aural, Visual, Logical, Verbal, Physical, Social Interpersonal and Solitary Intrapersonal) have been used to understand the different ways people learn and take in information [16]. While the method used in our study to understand learning styles relies on the participants perception of the strongest learning style, and not assessed with a well-known method, like the BFI test for personality traits, some interesting insights that can inform future work is expected.\n3 EXPERIMENT\nThis experiment looks to understand associations between single musical notes and basic emotions, and how other independent variables impact these associations, with two specific questions in mind:\nQ1 - Can emotion be experienced from a single musical note and if so, how basic a level is the emotional perception that is experienced?\nQ2 - Is there a significant correlation between auditory-emotion choices when factoring in the twelve independent variables mentioned in Section 2.2?\n3.1 Participants\n130 people (female = 95, male = 34, other = 1) participated in the online survey, with participants identifying with the following age groups: 35-44 (38), 24-34 (32), 45-55 (29), 55-64 (14), 18-24 (9), over 65 (6) and under 18 (2). Information was also gathered on participants’ musical experience (82% with musical experience and 18% with no experience) and creativeness (61%, regularly partaking in a creative hobby; 24%, not so regularly; and 15%, with no creative hobby). While musical experience was used as one of the individual variables that could impact the emotion response to auditory stimuli, the questions regarding musical instruments played by participants and their level of creative expression was used to gather an extra layer of understanding to the results of the experiment.\n3.2 Sound and Emotion Stimuli\nIn this experiment, the auditory stimuli were recorded on the Logic Pro X software using a Fender Telecaster electric guitar as the instrument. Each note was recorded on a mono track with each note lasting 6 seconds with a smooth short fade at the end of each clip. A basic EQ, compressor and room reverb are used to obtain a more natural sound, with each sound file rendered to an mp3 file with the normalised function turned on. The envelope of the recorded notes consist of a sharp attack to the maximum amplitude, with a steady decay as the note rings out. As this was an online survey, we had no control of the absolute volume participants listened to the stimuli, but asked them to set it to a comfortable listening level. The first octave of the guitar was chosen to elicit the auditory stimuli, from note E2 (82.41Hz) to note D#3 (155.6Hz). Final Cut Pro was used to create short video files with a white background with black text counting down from three, with audio stimuli playing on zero. These video files provided better functionality in Limesurvey 1, than audio only files and also added a visual instruction to participants. For the emotion stimuli, the four emotion words, sadness, anger, fear and joy were used. These emotions are taken from the discrete emotion model mentioned in Section 2.1.\n3.3 Procedure\nThe online survey was distributed through the University College Cork survey mailing list, as well as being shared on LinkedIn and Twitter. Once participants clicked on the survey link, they landed on the welcome page and were presented with a detailed explanation of the survey, as well as contact information of the researchers, if they had any questions regarding the survey and could click “Next” to agree to start the survey. The survey was broken up into four sections: General Information, Personality Test, Cross-modal Association Questions and Auditory-emotion Questions. The survey is described in following four sections.\n3.3.1 General Questions Section. Each section was displayed on a separate page with participants instructed to click the “Next” button to move to the next section. The first section contained five questions regarding age, gender, musical ability, instruments played (this question only appeared if the participant had stated they had musical experience in the previous question) and creative hobbies.\n3.3.2 Personality Test Section. This section involved the participants taking the short Big Five personality test (BFI-10). Participants responded to ten questions on a five point Likert scale ranging from “Strongly Disagree” to “Strongly Agree”. These questions were randomised to avoid any bias in the survey. After the ten questions were answered, participants could consent to share their email address to get their BFI-10 scores sent to them at a later time. If consent was given, the participant could input their email address on the next page.\n3.3.3 Cross-modal Association Section. The third section presented four questions, with the first three designed to gain an understanding of the participants’ relationship with sound, and how other senses impact and are impacted by auditory stimuli. The fourth question looked to find what learning style participants associated with their style of learning. The participants rated the first three questions, between 1 (weak) and 5 (strong) in sensitivity/sensation. The first question asked participants to rate their sensitivity to noise/sound; the next two questions asked participants to rate the sensation strength in other senses when the auditory sense is the primary trigger (i.e., when a bell is rung and triggers sensation), and in the auditory sense when the other sense is the primary trigger (i.e., when a flash of light triggers sensation). For the last question participants were given the options of Visual, Logical, Aural, Verbal, Physical, Social Interpersonal and Solitary Interpersonal learning styles from Section 2.2.4, and asked to choose their top three, in order of perceived strongest to weakest.\n3.3.4 Auditory-emotion Questions Section. The final section contained twelve questions presented in randomised order, to avoid selection bias. Each question contained a nine second video, which instructed participants to press play when ready. The video presented a count down from three with the auditory stimuli playing on zero, giving the participant time to fully listen to the note played. As mentioned in Section 3.2, the first octave of the guitar was used for the auditory stimuli. They were informed to listen once and to choose an emotion word from the options presented (Sadness, Joy, Anger, Fear).\nFigure 2: This figure shows the percentage between the four emotion words that participants chose for each musical note. Sadness significantly (*) associated with the lower notes, with Joy significantly (*) associating with the higher notes.\nOnce the twelve sound-emotion questions were answered, they were directed to submit the survey and a conclusion page was displayed thanking participants for taking part.\n3.4 Data Handling and Analysis\nThe survey data was stored within the Limesurvey application running from the researcher's hosting site. All data was anonymised and information on IP addresses, date stamp and referrer URLs were not collected. Email addresses were collected only from the participants who consented to leave it for the purpose of sharing the results of the personality test. The email data was deleted once the results were obtained and emailed to the participants.\nTo answer Q1, a chi-square test was conducted on the cross-table Note by Emotion. The chi-square test was significant ((χ2 (33 n = 1560) = 274.39, p <.001), with a small effect size (Cramer's V =.24).\nTable 1: Results of the chi-square test that was conducted on the cross-table Note-by-Emotion. The chi-square test was significant (χ2 (33 n = 1560) = 274.39, p <.001), with a small effect size (Cramer's V =.24). The significant ARSs are highlighted in bold.\nEmotion\nNote Sadness Anger Fear Joy\nD# Count 21a 16a, b 31b 62c\nAdj. Res. -5.586 -0.593 0.217 6.664\nD Count 25a 26b 31b 48b\nAdj. Res. -4.835 2.044 0.217 3.654\nC# Count 39a 8a 30a, b 53b\nAdj. Res. -2.206 -2.703 0.000 4.729\nC Count 29a 13a 29a 59b\nAdj. Res. -4.084 -1.384 -0.217 6.019\nB Count 48a 13a 26a 43a\nAdj. Res. -0.516 -1.384 -0.870 2.580\nA# Count 46a 20a 32a 32a\nAdj. Res. -0.892 0.461 0.435 0.215\nA Count 52a 25a 32a 21a\nAdj. Res. 0.235 1.780 0.435 -2.150\nG# Count 75a 12b 27a, b 16b\nAdj. Res. 4.554 -1.648 -0.652 -3.224\nG Count 73a 20a, b 23b, c 14c\nAdj. Res. 4.178 0.461 -1.522 -3.654\nF# Count 69a 19a 36a 6b\nAdj. Res. 3.427 0.198 1.305 -5.374\nF Count 74a 14a, b 32a 10b\nAdj. Res. 4.366 -1.121 0.435 -4.514\nE Count 58a 33a 31a 8b\nAdj. Res. 1.361 3.890 0.217 -4.944\nTo answer Q2, a multinomial stepwise multiple regression model was carried out in which the dependent variable was the single musical notes. Using the variables obtained from the data in the survey as models, Age, Gender, Musical Experience, Sound Sensitivity, Auditory-to-Other-Sense, Other-Sense-to-Auditory, Learning Style and the five personality traits of Openness, Conscientiousness, Extra-version, Agreeableness and Neuroticism were analysed.\nTable 2: Result of multinomial stepwise multiple regression model. Only the significant predictors are reported. The reference emotion is always Joy.\nEstimate\nStd.\nError\nWald Sig.\nOdds\nratio\n95% Conf. Int.\nL Bound U Bound\nNote: C#\nAnger Male 1.946 0.811 5.762 .016 7.000 1.429 34.286\nFear Male 1.302 0.508 6.573 .010 3.675 1.359 9.940\nNote: C\nAnger Aural 1.946 0.886 4.818 .028 7.001 1.232 39.766\nAnger Physical 1.946 0.976 3.976 .046 7.001 1.034 47.418\nNote: A\nSadness Proficiency -1.034 0.324 10.186 .001 .355 .188 .671\nAnger Proficiency -0.825 0.365 5.102 .024 .438 .214 .897\nAnger Conscientiousness -0.051 0.024 4.550 .033 .950 .906 .996\nFear Proficiency -1.042 0.353 8.728 .003 .353 .177 .704\nNote: G#\nAnger age -0.817 0.360 5.146 .023 .442 .218 .895\nFear age -0.853 0.307 7.746 .005 .426 .234 .777\nNote: G\nAnger age -1.095 0.345 10.084 .001 .335 .170 .658\nNote: E\nAnger sensitive_sound 0.836 0.373 5.022 .025 2.308 1.111 4.795\n3.5 Results\n3.5.1 Q1 - Auditory-emotion association. For Q1, the results in Table 1 show that very basic emotion can be experienced from basic auditory stimuli, like a single musical note. For four out of five of the lowest notes (E,F,F#,G,G#), Table 1 shows that the adjusted standardised residual (ASR) scores are significantly higher for Sadness. The one exception was the lowest note (E) where Anger has a higher ASR score but not at a significant level; however, Sadness was chosen by most participants, but not at a significant level. For the higher notes, results show that the four highest notes (C,C#,D,D#) are significantly higher for Joy in the ASR scores. Table 1 also highlights that only the emotions of Joy and Sadness found significance, either positively or negatively in ASR scores, that could suggest very basic emotion associations to single musical notes. Results show that for notes A,A#, and B, which are placed in the middle of the octave of notes tested, no significant statistics are shown.\n3.5.2 Q2 - Impact of Independent Variables on Sound-emotion Association. Results pertaining to Q2 highlighted some interesting insights. Q2 looked at how the individual variables, mentioned in Section 2.2, can affect the emotion choice for each single musical note. Analysis was performed on each note, and Table 2 shows significant results at p <.05 where the individual variables had an impact on the association between single musical notes and emotion. As shown in Table 2, six out of the twelve musical notes showed statistical significance in relation to one or more predictors. The odds ratio column in Table 2 represents the relative likelihood of each predictor type yielding a choice for a particular emotion. For example, Table 2 shows that a person with a higher sensitivity to sound is 2.308 times more likely to choose Anger than Joy (the emotion used as a reference) in relation to the E note. For individual variables with an odds ratio of less than 1, the predictor is less likely to choose the stated emotion than the referenced emotion Joy, i.e., participants with musical proficiency are less likely to experience Sadness, Anger and Fear compared to Joy.\n3.6 Discussion\n3.6.1 Q1 - Sound-emotion Association. Similar to the findings of O'Toole et al., associations between basic auditory stimuli and emotion can be viewed at a very basic level of emotional perception [31]. The results from the chi square test, illustrated in Table 1, show a significant association between Lower-notes/Sadness and Higher-notes/Joy. In Section 2.1, research on different emotion models were presented, with the basic/discrete model considered the most appropriate for an experiment using single musical notes and for perceiving rather than feeling emotions [45]. For future research, adopting a multi-modal approach that can understand multiple features from auditory stimuli would be beneficial. However for this study on basic auditory stimuli, results show a basic emotion choice for single musical notes, validating the use of the basic emotion model in this experiment.\nWith the three notes (A, A#, B) in the middle of the octave that showed no significance, all three notes had Sadness as the most chosen emotion. A Study by Chau et al. has shown that the timbre of an instrument can impact the emotion response when hearing a sound. Results showed that the guitar rated higher for negative emotional characteristics compared to a piano (which was emotionally neutral) and instruments like a vibraphone or marimba (which was emotionally positive) [9]. Findings from another study found that “timbre independently affects the perception of emotions in music after controlling for other acoustic, cognitive and performance factors” [19] While timbre is one of many acoustic properties that can impact the emotion response to sound, if we are analysing single musical notes that lack tempo, rhythm and modes, it could have a bigger impact on how a person responds to sound emotionally. To answer Q1, emotion can be experienced from basic auditory stimuli, such as a single musical note at a very basic level of emotion perception. Without melody, harmony, rhythm, modes and other audio features a single note can be perceived to be mostly happy or sad and this is mostly dependant on pitch.\n3.6.2 Q2 - Independent Variables Impact on Sound-emotion Association. Q2 investigated how different individual variables can impact a participants’ emotion word choices. In Table 2, significant scores in seven out of twelve individual variables across six of the twelve musical notes were found.\nConscientiousness provided a significant negative correlation with Anger on the note A. This result would fit the personality trait. Previous studies have shown that the Conscientiousness trait is positively associated with the optimistic-conventional dimension in relation to musical preference [14]. Studies have shown that a person high in Conscientiousness is likely to; like high tempo music in a major key rather than low tempo in the minor key [14]; use music in a more cognitive rather than an emotional way [8]; and usually can regulate or control impulses better [13].\nTable 2 shows that musically proficient participants significantly chose Joy over Sadness, Anger and Fear when listening to the note A. This correlation could be explained by the fact that the A note is the standard tuning note on the guitar. 34 participants were musically proficient, with guitar as their choice instrument. Hearing this particular note could have contributed to elicit positive emotions, and it could be possible that other guitar players have an overall more positive emotion perception when hearing sounds from a guitar.\nIt could be also similar to both the optimistic (Joy) and the conventional (standard tuning note) dimensions for the Conscientiousness trait mentioned above. While both musicians and non-musicians can have an understanding of communicating expressive content, as presented in Section 2.2.2, musicians use a greater amount of temporal fine structure (TFS) information and a higher emotional resolvability curve [27]. This could impact how musicians respond to musical notes in different ways to non-musicians and why musical proficiency correlates to Joy with regard to the A note.\nTable 2 shows interesting correlations between the Note C and the Aural and Physical learning styles. Participants who chose Aural or Physical as their preferred learning style were around seven times more likely to chose Anger. Studies have shown that, when learning an instrument, aural learning was the main learning style, but participants also used more logical and practical approaches when learning new songs [17]. For aural learners, with musical experience (87.5% of aural learners), this correlation between the C note and the Aural learning style could be due to frustration of not having the option to use more logical and practical learning styles to help them understand the musical note being elicited. Looking at the positive correlation between Anger and the Physical learning style for the note C, some interesting insights can be seen. The C note is the fourth highest frequency tested in this study and while overall we can see from Table 1 that Joy is the significant emotion, higher frequencies can also garner annoyance in some people. A study measuring noise annoyance levels in working environments has shown that higher frequencies had the highest annoyance rating with the lowest frequencies having the lowest annoyance rating [25]. It is possible that the Anger associated with the Physical, as well as the Aural learning styles, comes from the higher frequency of the C note.\n4 CONCLUSIONS\nIn this paper, we presented an experiment on auditory-emotion associations, looking to reveal the emotional potential of an apparently-basic stimulus such as a single musical note and the impact of other variables such as, personality, gender, age and musical experience on this association. Through this approach, we were able to more clearly distinguish the combination of key factors accounting for associations between auditory stimuli and emotion. Results showed significance on basic associations in lower notes to sadness and higher notes to joy, with a positive correlation between the four highest notes and the emotion joy, and a positive correlation between three of the four lowest notes and the emotion, sadness. Results pertaining to the impact of individual variables on auditory-emotion associations showed significant results on six of the twelve musical notes. The independent variables of gender, aural and physical learning styles, musical proficiency, the Conscientiousness personality trait, age and sound sensitivity were found to be significant variables in six of the twelve single musical notes, thus potentially impacting the emotional response to the auditory stimuli.\nSome limitations of this experiment include, for example, the use of only one octave and the choice of instrument for auditory stimuli. One octave was used in this experiment, to reproduce the test setup from [31], with a larger test group. In the future, it would be beneficial to study auditory-emotion associations in a range of octaves, to understand if pitch height-emotion associations are the most dominant associations regarding single musical notes. Also, the use of a single instrument type is an important factor in associations between single musical notes and emotions. Each instrument has a different timbre which elicits different emotion characteristics. As mentioned in Section 3.6.1, it has been shown that the timbre of the guitar is highly rated for negative emotional characteristics, where a Piano is rated as neutral, and a vibraphone is highly rated for positive emotional characteristics [9]. Changing the instrument that elicits auditory stimuli, by adopting a piano (neutral emotional characteristics) or using multiple instrument sounds, with different emotional characteristics is important for future studies and will help understand single musical note-emotion associations and the impact of timbre and multiple octaves have on the emotion response.\nThe findings of the experiment presented in the paper can help towards the formulation of a new auditory-visual framework that uses understandings on emotion, personality and other variables in the development of more personalised human-computer interfaces. These findings can improve the design of programs such as SoundStrokes presented in [31], that can assist in sharpening our associations between our senses. Further research into the aforementioned auditory-visual framework could lead to the creation of applications using computer-generated cross-modal stimuli, e.g., synthesised audio-visual tutorials, to learn or improve skills. These applications could be used for a multitude of purposes, such as learning an instrument or to paint; and in exploring new creative ways of expression, e.g., writing a song with a paint brush as the instrument or painting a picture with a piano as your bush. Creativity should not be limited to one form of expression and understanding the intricacies of our senses, combined with emotion, personality traits and other variables, it can provided a platform for multi-modal expression using digital tools.\nACKNOWLEDGMENTS\nThis publication has emanated from research supported in part by a Grant from Science Foundation Ireland under Grant number 18/CRT/6222.\nREFERENCES\nToni Antonucci, Hiroko Akiyama, and Keiko Takahashi. 2004. Attachment and Close Relationships across the Life Span. Attachment & Human Development 6, 4 (Dec. 2004), 353–370. https://doi.org/10.1080/1461673042000303136\nNavigate to\ncitation 1\nFilippo Bonini Baraldi, Giovanni De Poli, and Antonio Rodà. 2006. Communicating Expressive Intentions with a Single Piano Note. Journal of New Music Research 35, 3 (Sept. 2006), 197–210. https://doi.org/10.1080/09298210601045575\nNavigate to\ncitation 1\ncitation 2\nLisa Feldman Barrett. 1998. Discrete Emotions or Dimensions? The Role of Valence Focus and Arousal Focus. Cognition & Emotion 12, 4 (July 1998), 579–599. https://doi.org/10.1080/026999398379574\nNavigate to\ncitation 1\nThomas Baumgartner, Michaela Esslen, and Lutz Jäncke. 2006. From Emotion Perception to Emotion Experience: Emotions Evoked by Pictures and Classical Music. International Journal of Psychophysiology 60, 1 (April 2006), 34–43. https://doi.org/10.1016/j.ijpsycho.2005.04.007\nNavigate to\ncitation 1\nKira S. Birditt and Karen L. Fingerman. 2005. Do We Get Better at Picking Our Battles? Age Group Differences in Descriptions of Behavioral Reactions to Interpersonal Tensions. The Journals of Gerontology Series B: Psychological Sciences and Social Sciences 60, 3 (May 2005), P121–P128. https://doi.org/10.1093/geronb/60.3.P121\nNavigate to\ncitation 1\nRoberto Bresin. 2004. Real-Time Visualization of Musical Expression.. In HUMAINE Workshop “From Signals to Signs of Emotion and Vice Versa”. Santorini, 5.\nNavigate to\ncitation 1\nClair Cassiello-Robbins, Deepika Anand, Kibby McMahon, Rachel Guetta, Jacqueline Trumbull, Lisalynn Kelley, and M. Zachary Rosenthal. 2020. The Mediating Role of Emotion Regulation Within the Relationship Between Neuroticism and Misophonia: A Preliminary Investigation. Frontiers in Psychiatry 11 (Aug. 2020), 847. https://doi.org/10.3389/fpsyt.2020.00847\nNavigate to\ncitation 1\ncitation 2\nTomas Chamorro-Premuzic and Adrian Furnham. 2007. Personality and Music: Can Traits Explain How People Use Music in Everyday Life?British Journal of Psychology 98, 2 (2007), 175–185. https://doi.org/10.1348/000712606X111177\nNavigate to\ncitation 1\ncitation 2\ncitation 3\nChuck-Jee Chau, Bin Wu, and Andrew Horner. 2015. The Emotional Characteristics and Timbre of Nonsustaining Instrument Sounds. Journal of the Audio Engineering Society 63, 4 (April 2015), 228–244. https://doi.org/10.17743/jaes.2015.0016\nNavigate to\ncitation 1\ncitation 2\nOliver Collignon, Simon Girard, Frédéric Gosselin, Dave Saint-Amour, Franco Lepore, and Maryse Lassonde. 2010. Women Process Multisensory Emotion Expressions More Efficiently than Men. Neuropsychologia 48, 1 (Jan. 2010), 220–225. https://doi.org/10.1016/j.neuropsychologia.2009.09.007\nNavigate to\ncitation 1\nCaroline Curwen. 2018. Music-Colour Synaesthesia: Concept, Context and Qualia. Consciousness and Cognition 61 (May 2018), 94–106. https://doi.org/10.1016/j.concog.2018.04.005\nNavigate to\ncitation 1\nRichard J Davidson, Klaus R Scherer, and H. Hill Goldsmith. 2009. Handbook of Affective Sciences. Oxford University Press, New York; Oxford.\nNavigate to\ncitation 1\nColin G. DeYoung. 2010. Personality Neuroscience and the Biology of Traits: Personality Neuroscience. Social and Personality Psychology Compass 4, 12 (Dec. 2010), 1165–1180. https://doi.org/10.1111/j.1751-9004.2010.00327.x\nNavigate to\ncitation 1\nSnježana Dobrota and Ina Reić Ercegovac. 2015. The Relationship between Music Preferences of Different Mode and Tempo and Personality Traits – Implications for Music Pedagogy. Music Education Research 17, 2 (April 2015), 234–247. https://doi.org/10.1080/14613808.2014.933790\nNavigate to\ncitation 1\ncitation 2\nRyan Donovan, Aoife Johnson, Aine deRoiste, and Ruairi O'Reilly. 2020. Quantifying the Links Between Personality Sub-Traits and the Basic Emotions. In Computational Science and Its Applications – ICCSA 2020, Osvaldo Gervasi, Beniamino Murgante, Sanjay Misra, Chiara Garau, Ivan Blečić, David Taniar, Bernady O. Apduhan, Ana Maria A.C. Rocha, Eufemia Tarantino, Carmelo Maria Torre, and Yeliz Karaca(Eds.). Vol. 12250. Springer International Publishing, Cham, 521–537. https://doi.org/10.1007/978-3-030-58802-1_37\nNavigate to\ncitation 1\nHoward Gardner. 1993. Frames of Mind: The Theory of Multiple Intelligences (2nd ed ed.). Fontana Press, London.\nNavigate to\ncitation 1\nLucy Green. 2012. Musical “Learning Styles” and “Learning Strategies” in the Instrumental Lesson: Some Emergent Findings from a Pilot Study. Psychology of Music 40, 1 (Jan. 2012), 42–65. https://doi.org/10.1177/0305735610385510\nNavigate to\ncitation 1\nAnnaliese Micallef Grimaud, Tuomas Eerola, and Nick Collins. 2019. EmoteControl: A System for Live-Manipulation of Emotional Cues in Music. In Proceedings of the 14th International Audio Mostly Conference: A Journey in Sound. ACM, Nottingham United Kingdom, 111–115. https://doi.org/10.1145/3356590.3356608\nNavigate to\ncitation 1\nJulia C. Hailstone, Rohani Omar, Susie M. D. Henley, Chris Frost, Michael G. Kenward, and Jason D. Warren. 2009. It's Not What You Play, It's How You Play It: Timbre Affects Perception of Emotion in Music. Quarterly Journal of Experimental Psychology 62, 11 (Nov. 2009), 2141–2155. https://doi.org/10.1080/17470210902765957\nNavigate to\ncitation 1\nKate Hevner. 1936. Experimental Studies of the Elements of Expression in Music. The American Journal of Psychology 48, 2 (April 1936), 246. https://doi.org/10.2307/1415746\nNavigate to\ncitation 1\nEdyta Monika Hunter, Louise H. Phillips, and Sarah E. MacPherson. 2010. Effects of Age on Cross-Modal Emotion Perception.Psychology and Aging 25, 4 (Dec. 2010), 779–787. https://doi.org/10.1037/a0020528\nNavigate to\ncitation 1\ncitation 2\ncitation 3\nCarroll E. Izard, Deborah Z. Libero, Priscilla Putnam, and O. Maurice Haynes. 1993. Stability of Emotion Experiences and Their Relations to Traits of Personality.Journal of Personality and Social Psychology 64, 5(1993), 847–860. https://doi.org/10.1037/0022-3514.64.5.847\nNavigate to\ncitation 1\ncitation 2\nPatrik N. Juslin. 2013. What Does Music Express? Basic Emotions and Beyond. Frontiers in Psychology 4 (2013). https://doi.org/10.3389/fpsyg.2013.00596\nNavigate to\ncitation 1\nLena Lambrecht, Benjamin Kreifelts, and Dirk Wildgruber. 2014. Gender Differences in Emotion Recognition: Impact of Sensory Modality and Emotional Category. Cognition and Emotion 28, 3 (April 2014), 452–469. https://doi.org/10.1080/02699931.2013.837378\nNavigate to\ncitation 1\nUlf Landström, Elisabeth Åkerlund, Anders Kjellberg, and Maria Tesarz. 1995. Exposure Levels, Tonal Components, and Noise Annoyance in Working Environments. Environment International 21, 3 (Jan. 1995), 265–275. https://doi.org/10.1016/0160-4120(95)00017-F\nNavigate to\ncitation 1\nLawrence E Marks. 2004. Cross-Modal Interactions in Speeded Classification. In The Handbook of Multisensory Processes, Gemma Calvert, Charles Spence, and Barry E. Stein(Eds.). MIT Press, Cambridge, Mass, 85–105.\nNavigate to\ncitation 1\ncitation 2\nFrancis A. M. Manno, Raul R. Cruces, Condon Lau, and Fernando A. Barrios. 2019. Uncertain Emotion Discrimination Differences Between Musicians and Non-Musicians Is Determined by Fine Structure Association: Hilbert Transform Psychophysics. Frontiers in Neuroscience 13 (Sept. 2019). https://doi.org/10.3389/fnins.2019.00902\nNavigate to\ncitation 1\ncitation 2\ncitation 3\nSolange Mardaga and Michel Hansenne. 2009. Do Personality Traits Modulate the Effect of Emotional Visual Stimuli on Auditory Information Processing?Journal of Individual Differences 30, 1 (Jan. 2009), 28–34. https://doi.org/10.1027/1614-0001.30.1.28\nNavigate to\ncitation 1\nGail Martino and Lawrence E Marks. 2000. Cross-Modal Interaction between Vision and Touch: The Role of Synesthetic Correspondence. Perception 29, 6 (June 2000), 745–754. https://doi.org/10.1068/p2984\nNavigate to\ncitation 1\ncitation 2\nVincenzo Moscato, Antonio Picariello, and Giancarlo Sperli. 2020. An Emotional Recommender System for Music. IEEE Intelligent Systems(2020), 1–1. https://doi.org/10.1109/MIS.2020.3026000\nNavigate to\ncitation 1\nPatrick O'Toole, Donald Glowinski, and Maurizio Mancini. 2019. Understanding Chromaesthesia by Strengthening Auditory-Visual-Emotional Associations. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII). IEEE, Cambridge, United Kingdom, 1–7. https://doi.org/10.1109/ACII.2019.8925465\nNavigate to\ncitation 1\ncitation 2\ncitation 3\ncitation 4\nStephen E. Palmer, Karen B. Schloss, Zoe Xu, and Lilia R. Prado-Leon. 2013. Music-Color Associations Are Mediated by Emotion. Proceedings of the National Academy of Sciences 110, 22 (May 2013), 8836–8841. https://doi.org/10.1073/pnas.1212562110\nNavigate to\ncitation 1\ncitation 2\ncitation 3\nRosalind W Picard. 1997. Affective Computing. MIT Press, Cambridge, Mass.\nNavigate to\ncitation 1\nBeatrice Rammstedt and Oliver P. John. 2007. Measuring Personality in One Minute or Less: A 10-Item Short Version of the Big Five Inventory in English and German. Journal of Research in Personality 41, 1 (Feb. 2007), 203–212. https://doi.org/10.1016/j.jrp.2006.02.001\nNavigate to\ncitation 1\nTed Ruffman, Julie D. Henry, Vicki Livingstone, and Louise H. Phillips. 2008. A Meta-Analytic Review of Emotion Recognition and Aging: Implications for Neuropsychological Models of Aging. Neuroscience & Biobehavioral Reviews 32, 4 (Jan. 2008), 863–881. https://doi.org/10.1016/j.neubiorev.2008.01.001\nNavigate to\ncitation 1\ncitation 2\nCharles Spence. 2011. Crossmodal Correspondences: A Tutorial Review. Attention, Perception, & Psychophysics 73, 4 (May 2011), 971–995. https://doi.org/10.3758/s13414-010-0073-7\nNavigate to\ncitation 1\ncitation 2\ncitation 3\nCharles Spence. 2020. Assessing the Role of Emotional Mediation in Explaining Crossmodal Correspondences Involving Musical Stimuli. Multisensory Research 33, 1 (July 2020), 1–29. https://doi.org/10.1163/22134808-20191469\nNavigate to\ncitation 1\ncitation 2\ncitation 3\ncitation 4\ncitation 5\nXiuwen Sun, Xiaoling Li, Lingyu Ji, Feng Han, Huifen Wang, Yang Liu, Yao Chen, Zhiyuan Lou, and Zhuoyun Li. 2018. An Extended Research of Crossmodal Correspondence between Color and Sound in Psychology and Cognitive Ergonomics. PeerJ 6 (March 2018), e4443. https://doi.org/10.7717/peerj.4443\nNavigate to\ncitation 1\nTawney Tsang and Karen B. Schloss. 2010. Associations between Color and Music Are Mediated by Emotion and Influenced by Tempo: (525772013-006). https://doi.org/10.1037/e525772013-006\nNavigate to\ncitation 1\ncitation 2\nJonna K. Vuoskoski and Tuomas Eerola. 2011. Measuring Music-Induced Emotion: A Comparison of Emotion Models, Personality Biases, and Intensity of Experiences. Musicae Scientiae 15, 2 (July 2011), 159–173. https://doi.org/10.1177/1029864911403367\nNavigate to\ncitation 1\ncitation 2\ncitation 3\ncitation 4\nTeija Waaramaa. 2017. Gender Differences in Identifying Emotions from Auditory and Visual Stimuli. Logopedics Phoniatrics Vocology 42, 4 (Oct. 2017), 160–166. https://doi.org/10.1080/14015439.2016.1243725\nNavigate to\ncitation 1\nJ Ward, B Huckstep, and E Tsakanikos. 2006. Sound-Colour Synaesthesia: To What Extent Does It Use Cross-Modal Mechanisms Common to Us All?Cortex 42, 2 (2006), 264–280. https://doi.org/10.1016/S0010-9452(08)70352-6\nNavigate to\ncitation 1\nKelly L. Whiteford, Karen B. Schloss, Nathaniel E. Helwig, and Stephen E. Palmer. 2018. Color, Music, and Emotion: Bach to the Blues. i-Perception 9, 6 (Nov. 2018), 204166951880853. https://doi.org/10.1177/2041669518808535\nNavigate to\ncitation 1\nLee H. Wurm, Gisela Labouvie-Vief, Joanna Aycock, Kristine A. Rebucal, and Heather E. Koch. 2004. Performance in Auditory and Visual Emotional Stroop Tasks: A Comparison of Older and Younger Adults.Psychology and Aging 19, 3 (2004), 523–535. https://doi.org/10.1037/0882-7974.19.3.523\nNavigate to\ncitation 1\nMarcel Zentner, Didier Grandjean, and Klaus R. Scherer. 2008. Emotions Evoked by the Sound of Music: Characterization, Classification, and Measurement.Emotion 8, 4 (2008), 494–521. https://doi.org/10.1037/1528-3542.8.4.494\nNavigate to\ncitation 1\ncitation 2\ncitation 3\nZhihong Zeng, M. Pantic, G.I. Roisman, and T.S. Huang. 2009. A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions. IEEE Transactions on Pattern Analysis and Machine Intelligence 31, 1 (Jan. 2009), 39–58. https://doi.org/10.1109/TPAMI.2008.52\nNavigate to\ncitation 1\nFOOTNOTE\n1 https://www.limesurvey.org\n\nThis work is licensed under a Creative Commons Attribution International 4.0 License.\nICMI '21 Companion, October 18–22, 2021, Montréal, QC, Canada\n© 2021 Copyright held by the owner/author(s).\nACM ISBN 978-1-4503-8471-1/21/10.\nDOI: https://doi.org/10.1145/3461615.3485419",
    "https://www.unprofesor.com/musica/tipos-de-cadencia-musical-3912.html?utm_source=chatgpt.com#anchor_1": "DESCUBRE\nBuscar\nunPROFESOR Música Lenguaje musical Los sonidos Tipos de cadencia musical\nLos sonidos\nTipos de cadencia musical\nValoración: 4 (4 votos) 4 comentarios\nPor Ana Sofía Rivera. 21 enero 2020\nPolifonia musical: características y...\nQué es el contrapunto en música\nUna de las cualidades innegables en el arte es la capacidad de crear un discurso, un a serie de ideas que están entrelazadas entre sí para poder expresar un sentimiento concreto. Gracias a la estructura y las formas del arte como método de comunicación, podemos establecer una forma elegante de hacer llegar nuestro mensaje de la manera mas sensible posible.\nLa música es uno de esos artes que nos permite hacer entender a nivel sentimental, esto en gran parte lo debemos a la forma tan refinada que tiene de llevarnos casi inconscientemente a la comprensión, de manera casi misteriosa por medio de la sucesión de sonidos. En esta lección de unPROFESOR hablaremos acerca de un componente de la música que nos permite lograr esto: los tipos de cadencia musical.\nTambién te puede interesar: Tipos de textura musical\nÍndice\nQué es una cadencia musical\nLos grados de la tonalidad en la música\nLos diferentes tipos de cadencia que existen\nQué es una cadencia musical\nLa música posee el elemento del tiempo, es decir que no es algo estático sino todo lo contrario, va cambiando mientras transcurre y es esto lo que le hace tener tanta vida. Otro factor crucial en la música es la dinámica que causa en cuanto a percepción de tensión y resolución, es decir que auditivamente tenemos la sensación de que cuando algo es muy denso, debe soltar esa tensión eventualmente. Es esta liberación lo que nos resulta tan placentero en la música y lo que hace que pueda transcurrir en el tiempo, tener ciclos, evolución y así crear un discurso.\nLa palabra “cadencia” viene del italiano y significa “caer”. Con esta palabra nos referimos a la necesidad de resolución antes mencionada. La estructura de una obra o canción musical se construye por una serie de acordes en un orden determinado. Si bien este orden queda a la merced del compositor, no es una sucesión aleatoria, ya que se rige por las reglas armónicas que le dan sentido a la música.\nToda progresión de acordes debe terminar eventualmente, desembocar en algún sitio y es precisamente a la acción de resolver a lo que le llamamos “cadencia”, es el momento y acorde en la música en la que un acorde “cae”.\nImagen: Pinterest\nLos grados de la tonalidad en la música\nAntes de conocer los tipos de cadencia debemos tener muy claro el concepto de los grados de la tonalidad y por supuesto, de tonalidad en sí.\nEn resumen, la tonalidad son las reglas que nos dictan el contexto armónico, las notas que podemos utilizar en una composición. Gracias a estas reglas logramos saber que tipos de acordes podemos utilizar y con esto, establecemos los grados de la tonalidad.\nLos grados de la tonalidad se encuentran en la escala base que estamos utilizando (ejemplo: escala mayor, escala menor, escala dórica... etc.) En una escala estándar tenemos 7 notas, y por lo tanto obtenemos 7 grados. Cada uno de los grados posee un nombre específico según su función, que esta definido por la cantidad de tensión o sensación de resolución. Estos son los nombres de los grados de tonalidad:\n(I) Primer grado: tónica (fundamental)\n(II) Segundo grado: supertónica\n(III) Tercer grado: mediante\n(IV) Cuarto grado: subdominante\n(V) Quinto grado: dominante\n(VI) Sexto grado: superdominante (o submediante)\n(VII) Séptimo grado: sensible\nEl grado de resolución por excelencia es el primer grado, tónica o fundamental, ya que nos provee la mayor sensación de conclusión gracias a su estabilidad armónica.\nImagen: Slideshare\nLos diferentes tipos de cadencia que existen\nExisten muchos tipos de cadencia ya que la música es un arte y depende de la creatividad. Sin embargo en el transcurso de la historia se han utilizado con frecuencia ciertas cadencias gracias a su conocida funcionalidad. Existen dos categorías para las cadencias: cadencias conclusivas y cadencias suspensivas.\nLas cadencias conclusivas son aquellas que alcanzan una resolución y por lo tanto una finalización clara. Por el contrario, una cadencia suspensiva es aquella que no brinda una resolución y que provoca la sensación de que el sonido debe continuar.\nPara ambos tipos de cadencia existen algunos tipos estándar.\nCadencias conclusivas\nCadencia perfecta: Va directamente de la dominante (V) a la tónica (I).\nCadencia imperfecta: Va de la dominantes (V) a la tónica (I) pero la organización de las notas se encuentra invertido o la voz más aguda no resuelve al primer grado.\nCadencia plagal: Va de la subdominante (IV) a la tónica (I).\nCadencia compuesta: Sucesión subdominante (IV) - dominante (V) - tónica (I).\nCadencia preclásica: Sucesión subdominante (IV) - dominante (V) – dominante octavada y tónica (I).\nCadencias suspensivas\nSemicadencia: Cuando se da un reposo en la subdominante (IV) o dominante (V), pero no resuelve aún.\nCadencia imperfecta: cuando en una cadencia perfecta el acorde de tónica se produce en un tiempo débil o si alguno de sus acordes se encuentra invertido (cuando la nota fundamental del acorde no es la más grave).\nCadencia rota o deceptiva: cuando una cadencia da la sensación de que resolverá a la tónica pero en cambio ese acorde es sustituido por un grado no resolutivo.\nCon estos conocimientos de la cadencias ya tienes un acercamiento para las bases de la música y de su composición. La música se compone de muchas relaciones que debemos ir estudiando poco a poco, para poder expresarnos en el arte con propiedad.\nImagen: Musicnet\nSi deseas leer más artículos parecidos a Tipos de cadencia musical, te recomendamos que entres en nuestra categoría de Lenguaje musical.\nPolifonia musical: características y...\nQué es el contrapunto en música\nCategorías relacionadas\nHistoria de la MúsicaInstrumentos musicales\nLo más visto\n1.\nCuáles son las notas musicales en el pentagrama\n2.\nEtapas de la música\n3.\nNombres de los instrumentos de cuerda\n4.\nPrincipales elementos de la música\n5.\nNotas musicales: símbolos y nombres\n6.\nFiguras musicales y su duración\n7.\nInstrumentos de viento madera\n8.\nQué son las claves musicales\nMás lecciones de Los sonidos\nLección 5 de 14\nQué es la forma musical y sus clasificaciones\nLección 6 de 14\nEstructura de la forma sonata\nLección 7 de 14\nTipos de melodías y sus características\nLección 8 de 14\nPolifonia musical: características y ejemplos\nLección 9 de 14\nTipos de cadencia musical\nLección 10 de 14\nQué es el contrapunto en música\nLección 11 de 14\nContrapunto musical: ejemplos\nLección 12 de 14\nCanon musical: definición y ejemplos\n¡Quiero ver más lecciones!\nPregunta al profesor sobre Tipos de cadencia musical\n\n\n\nTipos de cadencia musical\nRedes sociales\nAprender matemáticas\nAprender lengua española\nAprender ciencias sociales\nAprender ciencias naturales\nAprender música\nAprender física\nAprender química\nAprender inglés\nAprender tecnología e informática\nConsejos para estudiar\n© unprofesor.com 2025 Quiénes somos Contacta con nosotros Términos y Condiciones Política de privacidad Política de cookies\nCompartir en:",
    "https://eldiaadiariomusica.wordpress.com/2013/06/23/sentido-y-personalidad-de-las-tonalidades/": "Música que siento\nUn rincón literario\n— Artículos\n← Montserrat Figueras y el Cant de la Sibil·la\nTonalidad de Si bemol menor →\nSentido y personalidad de las Tonalidades\nPublicado el 23 de junio de 2013 por Manel Artero\nHubo un tiempo en el que se aceptaba otorgar una cierta personalidad a cada una de las tonalidades que pueden construirse con el sistema de afinación temperada, establecido desde el barroco y hasta la ruptura que supuso la nueva Armonía que estableció Arnold Schöenberg.\nEse concepto de «personalidad» debió tener su apogeo en el Romanticismo, decayendo a finales del siglo XIX y principios del XX.\nNo hay nadie, al menos yo no lo he encontrado, que haya demostrado en la práctica que esa convención es cierta. A pesar de ello sí que podemos encontrarnos con tonalidades que nos transmites distintas sensaciones. Pero también es cierto que eso puede atribuirse a la calidad de la melodía de los temas, a su orquestación, a su armonización e incluso al modo de utilizar la mezcla de timbres de los distintos instrumentos.\nA pesar de ello comparto con vosotros una tabla con lo que se petende que transmite cada tonalidad. A partir de ésta entrada intentaré publicar una obra que esté escrita en cada una de ellas para que podamos juzgarlo entre todos.\n Tonalidades Mayores\nTonalidad\nPersonalidad\nDo mayor\nAlegre, guerrero, completamente puro. Su carácter es de inocencia y de simplicidad.\nDo sostenido mayor\nMiradas lascivas. Pena y éxtasis. No puede reír, pero puede sonreír. No puede aullar, sólo puede hacer una mueca de su llanto. Caracteres y sentimientos inusuales.\nRe mayor\nFeliz y muy guerrero. El triunfo, Aleluyas, júbilo, victoria.\nMi bemol mayor\nCrueldad, dureza, amor, devoción, conversación íntima con Dios.\nMi mayor\nQuerellante, chillón, gritos ruidosos de alegría, placer al reírse.\nFa mayor\nFurioso y arrebatado.\nFa sostenido mayor\nTriunfo sobre la dificultad, libertad, alivio, superación de obstáculos, el eco de un alma que ferozmente ha lidiado y finalmente conquistó.\nSol mayor\nDulcemente jovial, idílico, lírico, calmado, pasión satisfecha, gratitud por la amistad verdadera y el amor esperanzado, emociones gentiles y pacíficas.\nLa bemol mayor\nGravedad, muerte y putrefacción.\nLa mayor\nAlegre, campestre, declaración de amor inocente, satisfacción, la esperanza de volver lo que le pertenece a uno de nuevo al regresar de una partida, juventud, aplausos y creencia en Dios.\nSi bemol mayor\nMagnífico, alegría, amor alegre, conciencia limpia, metas y deseos por un mundo mejor.\nSi mayor\nDuro, doliente, deslumbrante, fuertemente coloreado, anunciando pasiones salvajes, enfado, odios y resentimientos.\n Tonalidades menores\nTonalidad\nPersonalidad\nDo menor\nOscuro y triste. Declaración de amor y a la vez lamento de un amor no correspondido. Anhelos y suspiros.\nDo sostenido menor\nSentimientos de ansiedad, angustia y dolor profundo en el alma, desesperación, depresión, sentimientos sombríos, miedos, indecisiones, escalofríos. Si los fantasmas hablaran se aproximarían a esta tonalidad.\nRe menor\nGrave y devoto. Melancolía femenina. El rencor.\nMi bemol menor\nHorrible, espantoso.\nMi menor\nAfeminado, amoroso, melancólico.\nFa menor\nOscuro, doliente, depresivo, lamento funerario, gemidos de miseria, nostalgia solemne.\nFa sostenido menor\nPesimista, triste, sombrío, oscuro, terco a la pasión, resentimientos, descontentos.\nSol menor\nSerio, magnífico, descontento, preocupado por el rompimiento de los esquemas, mal templado, rechinamiento de dientes, disgusto.\nLa bemolmenor\nQuejándose todo el tiempo, incomplaciente, insatisfecho, corazón sofocado, lamentos, dificultades.\nLa menor\nTierno, lloroso, piedad femenina.\nSi bemol menor\nOscuro, terrible, criatura pintoresca y curiosa, ropa de noche, tosco, maleducado, burlesco, descortés, descontento con sí mismo, sonidos del suicidio.\nSi menor\nSolitario, melancólico, ermitaño, paciencia, fe y sumisión esperando el perdón divino.\nTu voto:\n60\n10\nRate This\nComparte esto:\nHaz clic para compartir en Twitter (Se abre en una ventana nueva)\nHaz clic para compartir en Facebook (Se abre en una ventana nueva)\nHaz clic para compartir en Tumblr (Se abre en una ventana nueva)\nHaz clic para compartir en Pinterest (Se abre en una ventana nueva)\nHaz clic para compartir en LinkedIn (Se abre en una ventana nueva)\nHaz clic para compartir en WhatsApp (Se abre en una ventana nueva)\nHaz clic para enviar un enlace por correo electrónico a un amigo (Se abre en una ventana nueva)\nHaz clic para imprimir (Se abre en una ventana nueva)\nCargando...\nRelacionado\nDe «OM» a «Amén», el sentido de la Música\n7 de agosto de 2019\nEn «La palabra entre el Arte»\nUna interpretación del preludio en Do M del Clave bien temperado\n18 de agosto de 2023\nEn «Bach»\nSentido de las tonalidades. Hoy: Do Mayor\n11 de octubre de 2014\nEn «Barroco»\nAcerca de Manel Artero\nManel Artero, nacido en Barcelona, en el barrio de Poble Sec, dedicó gran parte de su vida a la informática, compaginando con ella su amor por la lectura y por la música. De esta última cursó un grado de Historia. Más tarde haría los tres cursos de narrativa y novela de l’Escola d’escriptura de l’Ateneu barcelonès que le abriría las puertas al mundo de la escritura del que siempre formó parte sin saberlo. Desde entonces ganado diversos premios en concursos de relatos. El más sobresaliente, el de la Asociación “El coloquio de los perros” de Córdoba. Compagina su tiempo entre la escritura y diversos talleres y charlas sobre música, lectura y cultura de paz, que imparte en Cerdanyola del Vallès. El ladrón de rostros es su primera novela. Editada originalmente en 2017 por la editorial Maluma y6 reeditada por su hijo, Roger Artero, en 2023.\nVer todas las entradas por Manel Artero →\nEsta entrada fue publicada en Tonalidad y etiquetada afinación, personalidad, tonalidad. Guarda el enlace permanente.\n← Montserrat Figueras y el Cant de la Sibil·la\nTonalidad de Si bemol menor →\n26 respuestas a Sentido y personalidad de las Tonalidades\nPingback: Tonalidad de Si bemol menor |\nPingback: Sentido de las tonalidades. Hoy: Do Mayor | Música que siento\nRoberto Reffray dijo:\n30 de marzo de 2016 en 16:48\nHola. Estoy haciendo mi tesis de pregrado de la traducción del Dichterliebe de Schumann para ser interpretado. Ahora, me pareció muy interesante la relación que existe entre tonalidad y personalidad. ¿Puedes decirme la fuente, por favor? Gracias.\nResponder\nAgustí dijo:\n23 de octubre de 2023 en 15:04\nHola, existe un libro de un talChristian Friedrich Daniel Schubart que hizo un estudio sobre el tema allá por el año 1806 y que se llama «deen zu einer Ästhetik der Tonkunst», que viene a decir «Ideas para una estética del arte musical». Sólo lo he visto en catalán bajo el título «Idees per a una estètica de l’art musical» pero aún no he conseguido encontrarlo en las librerias. Si alguien tiene más suerte que yo ya lo comentareis.\nSaludos a tod@s.\nResponder\nManel Artero dijo:\n25 de octubre de 2023 en 5:44\nMuchísimas gracias por tu aportación.\nRecibe un cordial saludo.\nRoberto Reffray dijo:\n30 de marzo de 2016 en 16:59\nHola. Estoy haciendo una tesis de pregrado sobre la traducción del Dichterliebe de Schumann, para ser interpretada al español. Me parece muy interesante la relación que existe entre las tonalidades y la personalidad que cada armadura transmite, ¿podrías decirme la fuente, por favor? Gracias.\nResponder\nManel Artero dijo:\n30 de marzo de 2016 en 22:30\nHola Roberto,\nLo cierto es que la fuente no es demasiado rebuscada.\nCopié directamente la tabla que ofrece la Wikipedia:\nhttps://es.wikipedia.org/wiki/Tonalidad_(m%C3%BAsica)\nPero creo recordar que la comparé con algún otro lugar y eran muy parecidas.\nNo pensé en contrastarla con ninguno de los libros que tengo de música.\nNo sé si te habrá servido.\nUn saludo\nResponder\nRafael dijo:\n3 de octubre de 2019 en 0:52\nHola. Me identifico un poco con como las veo yo. No venía buscando algo científico u objetivo sino la experiencia de una persona. Me interesa conocer lo que se dice de las tonalidades para ver si coincidimos en sensaciones las personas que les buscamos un significado a las tonalidades.\nAñadiría,bajo mi punto de vista:\nRe Mayor: el cielo, visto como algo espiritual.\nMi bemol Mayor: calor maternal, una madre.\nMi Mayor: Humanidad. Unión humana.\nFa Mayor: Juventud, pasión aventurera. La persona joven que se lanza a hacer algo con mucho corazón pero poco conocimiento por no haber vivido suficiente. Amor romántico. El no importar qué pasará mañana.\nSol Mayor: el mar, profundidad, misterio, paz, inspiración, arte, romanticismo (en el sentido artístico)\nLa Mayor: aire, viento, algo ligero, claro (en sentido de color) y despreocupado, libertad.\nSi bemol Mayor: la tonalidad usual de los brindis. Celebración.\nSi Mayor: intranquilidad.\nRe menor: el destino. La fuerza natural de la realidad.\nMi menor: melancolía, nostalgia, el llanto que sana. Profundidad. Fuerza.\nSol menor: rabia, batalla.\nLa menor: delicadeza, canción de amor, lamento.\nResponder\nManel Artero dijo:\n3 de octubre de 2019 en 12:34\nHola Rafael,\nante todo gracias por tu comentario.\nEn mi caso, todo y que vivo para el sonido desde bastante joven, no tengo una percepción tan afinada como la tuya. Imagino que son niveles de sensibilidad o de interés.\nEn cambio me identifico con los timbres y las disonancias. Ellos sí me transportan a otro estado de percepción.\nTe felicito por tanta sensibilidad.\nRecibe un cordial saludo.\nResponder\nMaría Hornos Miller dijo:\n16 de abril de 2020 en 3:57\nEl Preludio de Rigoletto, de Verdi, está escrito en la tonalidad de do# menor y es tal como tú lo describes: tragedia, desesperación, certeza de que todo se hará pedazos. Gracias por tu artículo.\nResponder\nManel Artero dijo:\n16 de abril de 2020 en 12:43\nGracias a ti por pasar por el blog y leerlo.\nSaludos cordiales.\nResponder\nPingback: Download Mp3 COMO ESCRIBIR UNA CANCIÓN. PARTE 1. LOS MODOS Songs\ncarlos dijo:\n31 de octubre de 2020 en 15:05\nMuy interesante, llevo ya cuatro años trasteando con FL Studio (que no solo sirve para hacer trap) y bastantes sintetizadores virtuales y aprendiendo un poco sobre música, escalas y demás; sé que no alcanzaré nunca alguien que la ha estudiado de verdad pero componer aunque sea modestamente me llena de verdad. Ahora gracias a este Post estoy experimentando con los 24 modos entre mayor y menor de una manera consciente y tengo que decir que sí que suenan esos matices que se apuntan y sabiéndolo, trato de buscar esos adjetivos y el resultado es más potente.\nEstaría bien un post con otros modos tipo Frigio, eólico… e incluso de otras culturas como la india que tienen matices diferentes si se trasladan a la sonoridad occidental\nun saludo.\nResponder\ncarlos dijo:\n31 de octubre de 2020 en 15:09\nMuy interesante, llevo ya cuatro años trasteando con FL Studio y bastantes sintetizadores virtuales y aprendiendo un poco sobre música, escalas y demás; sé que no alcanzaré nunca alguien que la ha estudiado de verdad pero componer aunque sea modestamente me llena de verdad. Ahora gracias a este Post estoy experimentando con los 24 modos entre mayor y menor de una manera consciente y tengo que decir que sí que suenan esos matices que se apuntan y sabiéndolo, trato de buscar esos adjetivos y el resultado es más potente. Ahora gracias a este Post estoy experimentando con los 24 modos entre mayor y menor de una manera consciente y tengo que decir que sí que suenan esos matices que se apuntan y sabiéndolo, trato de buscar esos adjetivos y el resultado es más potente.\nEstaría bien un post con otros modos tipo Frigio, eólico… e incluso de otras culturas como la india que tienen matices diferentes si se trasladan a la sonoridad occidental\nun saludo.\nResponder\nManel Artero dijo:\n31 de octubre de 2020 en 17:29\nHola Carlos,\nAnte todo gracias por tu aportación.\nSí que me encantaría entrar en la complejidad de la música india. Su división de la octava es más compleja, sus ritmos también… Pero me falta tiempo. Me es imposible dedicarle al blog todo el tiempo que desearía.\nNo obstante tomo nota.\nEn cuanto a las escalas y modos griegos puedo asegurarte que en Youtube hay muchísmas cosas.\nUn saludo.\nResponder\nManel Artero dijo:\n31 de octubre de 2020 en 17:31\nOlvidé decirte que me alegra muchísimo que el Post te haya servido para experimentar con los modos.\nResponder\ncarlosgecheverria dijo:\n31 de octubre de 2020 en 19:30\nSí, muchas gracias. Aún sigo con ello.\nSaludos\nEstaba de paso dijo:\n31 de octubre de 2020 en 22:03\nNo tengo mucha idea de música, pero me parece una manera muy buena de aprender música con la ayuda de las emociones. Es importante darle un sentido al solfeo.Muy interesante gracias!\nResponder\nManel Artero dijo:\n31 de octubre de 2020 en 22:32\nGracias a ti por pasarte a leerme.\nUn saludo.\nResponder\nElide dijo:\n7 de junio de 2021 en 20:43\nTonalidad de Fa# M, totalmente queda con el Va Pensiero! de Nabbuco de Verdi 🙂\nResponder\nPaula dijo:\n22 de marzo de 2023 en 10:44\nBuenos días,\nMe preguntaba cuál es la »personalidad» de re bemol mayor. No aparece en tu tabla ni en wikipedia, me parece extraño.\nAtentamente,\nResponder\nManel Artero dijo:\n3 de abril de 2023 en 13:51\nHola Paula, encantado de encontrarte por aquí.\nPreguntas por una de las tonalidades que no pude encontrar en su momento.\nNo sé si puede servirte, pero buscando he encontrado esto:\nAntes de finales del siglo XIX, se afinaba con temperamentos distintos (o sea, cada tonalidad sonaba ligeramente distinta). Por lo tanto, cada tonalidad tenía asociada unas cualidades.\nRe bemol mayor, la que trajo tu comentario, era la «una tonalidad lasciva que degenera en pena y arrebato.»\nHector Berlioz llamó a esta tonalidad «majestuosa» en su Gran Tratado de Instrumentación5 mientas que a su tonalidad enarmónica, do sostenido mayor, la definió como «menos imprecisa que do♭ mayor y más elegante que esta».\nEspero que te sirva.\nRecibe un cordial saludo.\nResponder\nVanessa dijo:\n31 de marzo de 2023 en 18:01\nHola a todos, alguien sabe las emociones que emite Sol sostenido menor?\n🙂 Muchas gracias.\nResponder\nManel Artero dijo:\n1 de abril de 2023 en 23:40\nHola Vanessa,\nhe buscado pero no la encuentro.\nDe todos modos, su equivalente sonoro sería la de «La bemol menor» ya que Sol sostenido es la nota enarmónica (suena igual) que el La bemol.\nEspero que pueda servirte.\nRecibe un cordial saludo.\nResponder\nNoa Ayra Yishay dijo:\n17 de julio de 2024 en 9:53\nHola!\nTiene algun nombre en concreto esta teoria??\nGracias\nResponder\nManel Artero dijo:\n18 de julio de 2024 en 10:54\nHola, Noa,\nAnte todo decirte que las definiciones y asociaciones que se hacen con ellas son meramente subjetivas.\nDicho esto, se las conoce generalmente como «características afectivas de las tonalidades» o «afectos de las tonalidades». Aunque no tiene un nombre específico universalmente aceptado.\nRecibe un cordial saludo.\nResponder\nDeja un comentario\n════════════════════\nEjemplo de audio del libro\nReproductor de audio\n00:00\n00:00\nUtiliza las teclas de flecha arriba/abajo para aumentar o disminuir el volumen.\nIr a descargar\nY a escuchar todo el Podcast\n════════════════════\nSuscríbete al Blog\nEnter your email address to follow this blog and receive notifications of new posts by email.\nSuscripción\nÚnete a otros 269 suscriptores\n════════════════════\nAnécdotas Ars Antiqua Bach banda sonora Barroco Belleza canción concierto De Otros Grecia Historia de la música jazz Jordi Savall king Crimson mozart Novela Pasion perfeccion poesía Robert Fripp Rock Progresivo sencillez sensibilidad sensualidad Sinfonía Tonalidad tristeza trovadores Uncategorized wagner\n════════════════════\nArchivos\n   \n   Elegir el mes\n     enero 2025\n  septiembre 2024\n  junio 2024\n  noviembre 2023\n  octubre 2023\n  agosto 2023\n  julio 2023\n  May 2023\n  abril 2023\n  marzo 2022\n  noviembre 2021\n  marzo 2021\n  febrero 2021\n  octubre 2020\n  septiembre 2020\n  junio 2020\n  May 2020\n  abril 2020\n  noviembre 2019\n  agosto 2019\n  julio 2019\n  junio 2019\n  abril 2019\n  marzo 2019\n  enero 2019\n  diciembre 2018\n  noviembre 2018\n  agosto 2018\n  abril 2018\n  marzo 2018\n  febrero 2018\n  enero 2018\n  diciembre 2017\n  noviembre 2017\n  octubre 2017\n  septiembre 2017\n  agosto 2017\n  julio 2017\n  junio 2017\n  abril 2017\n  marzo 2017\n  diciembre 2016\n  noviembre 2016\n  octubre 2016\n  julio 2016\n  junio 2016\n  May 2016\n  abril 2016\n  marzo 2016\n  febrero 2016\n  enero 2016\n  diciembre 2015\n  noviembre 2015\n  octubre 2015\n  agosto 2015\n  junio 2015\n  May 2015\n  abril 2015\n  marzo 2015\n  febrero 2015\n  enero 2015\n  diciembre 2014\n  noviembre 2014\n  octubre 2014\n  agosto 2014\n  junio 2014\n  May 2014\n  abril 2014\n  marzo 2014\n  febrero 2014\n  diciembre 2013\n  noviembre 2013\n  octubre 2013\n  septiembre 2013\n  agosto 2013\n  julio 2013\n  junio 2013\n  May 2013\n  abril 2013\n  marzo 2013\n  enero 2013\n  diciembre 2012\n  noviembre 2012\n  octubre 2012\n  septiembre 2012\n  agosto 2012\n  julio 2012\n  junio 2012\n   ═══════════════════\nBlogroll\nAncha es mi casa\nAula de Música\nAzul, el principio fue azul…\nBach tras Bach\nBachiano\nBelleza sin palabras\nCamino de música\nConciertos en el Delibes\nEL cavaller del Cigne\nGenerador de Frecuencias sonoras\nHistoria de la Música\nLa mejor guitarra\nLa vuelta al mundo en 80 músicas\nMúsica antigua\nmúsica con nocturnidad y alevosía\nMúsica y literatura en clave personal\nMelomanía y otros estados sensoriales\nRadio Kras\nTarab Al Andalus\nTono menor, un blog de clásica\nVoces para la Paz\n════════════════════\n\nEl Día a Diario (Música) by Manel Artero Badenes is licensed under a Creative Commons Reconocimiento-SinObraDerivada 3.0 Unported License.\nPermissions beyond the scope of this license may be available at https://eldiaadiariomusica.wordpress.com.\n════════════════════\nMeta\nRegistro\nIniciar sesión\nFeed de entradas\nFeed de comentarios\nWordPress.com\n═══════════════════\nEnlaces RSS\n RSS - Comentarios\nComptador\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','//www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-50423519-1', 'wordpress.com'); ga('send', 'pageview');\nestá Dísticas?\n122.579 hits\ndocument.write(unescape(\"%3Cscript src=%27http://s10.histats.com/js15.js%27 type=%27text/javascript%27%3E%3C/script%3E\")); try {Histats.start(1,1967963,4,107,170,20,\"00001010\"); Histats.framed_page(); Histats.track_hits();} catch(err){};\nMúsica que siento\nWeb construida con WordPress.com.\nComentar\nRebloguear\nSuscribirse\nPrivacidad\nDiseña un sitio como este con WordPress.com\nComenzar"
}